"""
Step 5-6: äº‹åè¯„ä¼°
å¯¹æ¯”agentçš„ç»“æœä¸ground truthï¼Œè®¡ç®—resolve rateå’Œscope metrics
"""

import re
from typing import Set, Dict, Any
from pathlib import Path
from step1_load_data import load_task


def extract_files_from_patch(patch: str) -> Set[str]:
    """
    ä»git diff patchä¸­æå–ä¿®æ”¹çš„æ–‡ä»¶åˆ—è¡¨

    Args:
        patch: git diffæ ¼å¼çš„patch

    Returns:
        ä¿®æ”¹çš„æ–‡ä»¶è·¯å¾„é›†åˆ
    """
    # åŒ¹é… "diff --git a/path/to/file.py b/path/to/file.py"
    pattern = r'diff --git a/(.*?) b/'
    files = set(re.findall(pattern, patch))
    return files


def evaluate_scope(agent_patch: str, ground_truth_patch: str) -> Dict[str, Any]:
    """
    è¯„ä¼°Scopeå¯¹é½æƒ…å†µï¼ˆå¯¹æ¯”ground truthï¼‰

    Args:
        agent_patch: agentç”Ÿæˆçš„patch
        ground_truth_patch: æ ‡å‡†ç­”æ¡ˆpatch

    Returns:
        Scopeè¯„ä¼°ç»“æœï¼ˆprecision, recallç­‰ï¼‰
    """
    # æå–æ–‡ä»¶åˆ—è¡¨
    gold_files = extract_files_from_patch(ground_truth_patch)
    agent_files = extract_files_from_patch(agent_patch)

    if not agent_files:
        return {
            'scope_precision': 0.0,
            'scope_recall': 0.0,
            'gold_files': list(gold_files),
            'agent_files': [],
            'extra_files': [],
            'missed_files': list(gold_files),
        }

    # è®¡ç®—Precisionå’ŒRecall
    true_positives = len(gold_files & agent_files)

    precision = true_positives / len(agent_files) if agent_files else 0.0
    recall = true_positives / len(gold_files) if gold_files else 0.0

    return {
        'scope_precision': precision,
        'scope_recall': recall,
        'gold_files': list(gold_files),
        'agent_files': list(agent_files),
        'extra_files': list(agent_files - gold_files),  # agentå¤šæ”¹çš„
        'missed_files': list(gold_files - agent_files),  # agentæ¼æ”¹çš„
    }


def evaluate_resolved_mock(agent_patch: str, fail_to_pass: list, pass_to_pass: list) -> Dict[str, Any]:
    """
    Mockç‰ˆæœ¬çš„resolvedè¯„ä¼°

    æ³¨æ„ï¼šå®é™…åº”è¯¥ä½¿ç”¨SWE-benchå®˜æ–¹evaluatoråœ¨Dockerä¸­è¿è¡Œæµ‹è¯•
    è¿™é‡Œæ˜¯ç®€åŒ–ç‰ˆmockï¼Œåªæ¯”è¾ƒpatchæ˜¯å¦åˆç†

    Args:
        agent_patch: agentç”Ÿæˆçš„patch
        fail_to_pass: å¿…é¡»é€šè¿‡çš„æµ‹è¯•åˆ—è¡¨
        pass_to_pass: å¿…é¡»ä¿æŒé€šè¿‡çš„æµ‹è¯•åˆ—è¡¨

    Returns:
        è¯„ä¼°ç»“æœ
    """
    # Mock: ç®€åŒ–ç‰ˆåˆ¤æ–­ï¼ˆå®é™…éœ€è¦è¿è¡Œæµ‹è¯•ï¼‰
    # è¿™é‡Œå‡è®¾å¦‚æœpatchä¸ä¸ºç©ºä¸”ä¿®æ”¹äº†æ–‡ä»¶ï¼Œå°±ç®—éƒ¨åˆ†æˆåŠŸ

    has_changes = bool(extract_files_from_patch(agent_patch))

    # Mock resolvedåˆ¤æ–­
    # å®é™…åº”è¯¥æ˜¯: è¿è¡ŒFAIL_TO_PASSå’ŒPASS_TO_PASSæµ‹è¯•ï¼Œå…¨éƒ¨é€šè¿‡æ‰ç®—resolved
    resolved = has_changes  # ç®€åŒ–ç‰ˆï¼šæœ‰æ”¹åŠ¨å°±ç®—æˆåŠŸ

    return {
        'resolved': resolved,
        'f2p_total': len(fail_to_pass),
        'f2p_passed': len(fail_to_pass) if resolved else 0,  # Mock
        'p2p_total': len(pass_to_pass),
        'p2p_passed': len(pass_to_pass) if resolved else 0,  # Mock
        'note': 'This is a MOCK evaluation. Real evaluation requires SWE-bench Docker evaluator.'
    }


def main():
    """æ¼”ç¤ºäº‹åè¯„ä¼°æµç¨‹"""
    DATA_FILE = Path(__file__).parent.parent / "data" / "swebench" / "verified.jsonl"

    print("=" * 80)
    print("Step 5-6: äº‹åè¯„ä¼° (ä¸Ground Truthå¯¹æ¯”)")
    print("=" * 80)

    # 1. åŠ è½½ä»»åŠ¡
    print("\nğŸ“‚ Loading task...")
    task = load_task(DATA_FILE, task_index=0)

    # 2. è·å–Part Cï¼ˆè¯„ä¼°ç”¨æ•°æ®ï¼‰
    part_c = task.get_part_c()

    print(f"âœ… Task: {task.instance_id}")
    print(f"   FAIL_TO_PASS tests: {len(part_c['fail_to_pass'])}")
    print(f"   PASS_TO_PASS tests: {len(part_c['pass_to_pass'])}")

    # 3. æ¨¡æ‹Ÿagentç”Ÿæˆçš„patchï¼ˆå®é™…æ¥è‡ªstep3çš„agentï¼‰
    # è¿™é‡Œä½¿ç”¨ä¸€ä¸ªä¸ground truthç›¸åŒçš„patchä½œä¸ºç¤ºä¾‹
    agent_patch = part_c['ground_truth_patch']  # Mock: å‡è®¾agentç”Ÿæˆäº†æ­£ç¡®çš„patch

    print("\n" + "=" * 80)
    print("ğŸ“Š è¯„ä¼°1: åŠŸèƒ½æ­£ç¡®æ€§ (Resolved)")
    print("=" * 80)

    # è¯„ä¼°resolvedï¼ˆmockç‰ˆæœ¬ï¼‰
    resolved_result = evaluate_resolved_mock(
        agent_patch,
        part_c['fail_to_pass'],
        part_c['pass_to_pass']
    )

    print(f"\nâš ï¸  {resolved_result['note']}")
    print(f"\nâœ… Resolved: {resolved_result['resolved']}")
    print(f"   FAIL_TO_PASS: {resolved_result['f2p_passed']}/{resolved_result['f2p_total']} passed")
    print(f"   PASS_TO_PASS: {resolved_result['p2p_passed']}/{resolved_result['p2p_total']} passed")

    print("\nğŸ’¡ å®é™…è¯„ä¼°æµç¨‹:")
    print("   1. ä½¿ç”¨SWE-benchå®˜æ–¹Docker evaluator")
    print("   2. å‡†å¤‡predictions.jsonl:")
    print('      {"instance_id": "django__django-11119", "model_patch": "..."}')
    print("   3. è¿è¡Œè¯„ä¼°å™¨:")
    print("      python -m swebench.harness.run_evaluation \\")
    print("        --predictions_path predictions.jsonl \\")
    print("        --swe_bench_tasks verified.jsonl \\")
    print("        --log_dir logs/")
    print("   4. å¾—åˆ°resolve_rateï¼ˆ% tasksé€šè¿‡æ‰€æœ‰æµ‹è¯•ï¼‰")

    # 4. Scopeåˆ†æ
    print("\n" + "=" * 80)
    print("ğŸ“Š è¯„ä¼°2: Scopeå¯¹é½åˆ†æ")
    print("=" * 80)

    scope_result = evaluate_scope(agent_patch, part_c['ground_truth_patch'])

    print(f"\nğŸ“ Ground truthä¿®æ”¹çš„æ–‡ä»¶:")
    for f in scope_result['gold_files']:
        print(f"   â€¢ {f}")

    print(f"\nğŸ“ Agentä¿®æ”¹çš„æ–‡ä»¶:")
    for f in scope_result['agent_files']:
        print(f"   â€¢ {f}")

    print(f"\nğŸ“ Scope Metrics:")
    print(f"   Precision: {scope_result['scope_precision']:.2f}")
    print(f"   Recall: {scope_result['scope_recall']:.2f}")

    if scope_result['extra_files']:
        print(f"\nâš ï¸  Agentå¤šæ”¹çš„æ–‡ä»¶:")
        for f in scope_result['extra_files']:
            print(f"   â€¢ {f}")

    if scope_result['missed_files']:
        print(f"\nâš ï¸  Agentæ¼æ”¹çš„æ–‡ä»¶:")
        for f in scope_result['missed_files']:
            print(f"   â€¢ {f}")

    # 5. ç»¼åˆè¯„ä¼°
    print("\n" + "=" * 80)
    print("ğŸ“Š ç»¼åˆè¯„ä¼°ç»“æœ")
    print("=" * 80)

    # å‡è®¾ä»step4å¾—åˆ°çš„drift_rate
    mock_drift_rate = 0.0  # è¿™ä¸ªä¾‹å­æ˜¯"å®Œç¾"æ‰§è¡Œ

    print(f"\nâœ… Task: {task.instance_id}")
    print(f"   Difficulty: {task.difficulty}")
    print(f"\nç»“æœ:")
    print(f"   Resolved: {resolved_result['resolved']} {'âœ…' if resolved_result['resolved'] else 'âŒ'}")
    print(f"   Scope Precision: {scope_result['scope_precision']:.2f} {'âœ…' if scope_result['scope_precision'] >= 0.8 else 'âš ï¸'}")
    print(f"   Scope Recall: {scope_result['scope_recall']:.2f} {'âœ…' if scope_result['scope_recall'] >= 0.8 else 'âš ï¸'}")
    print(f"   Drift Rate: {mock_drift_rate:.1%} {'âœ…' if mock_drift_rate < 0.15 else 'âš ï¸'}")

    # åˆ†ç±»
    print(f"\nåˆ†ç±»:")
    if resolved_result['resolved'] and mock_drift_rate < 0.15:
        print("   â­â­â­ å®Œç¾ä»»åŠ¡ - åŠŸèƒ½å¯¹ + è¿‡ç¨‹ä¼˜")
    elif resolved_result['resolved'] and mock_drift_rate >= 0.15:
        print("   â­â­ æˆåŠŸä½†æ›²æŠ˜ - åŠŸèƒ½å¯¹ä½†è¿‡ç¨‹drifté«˜")
    elif not resolved_result['resolved'] and mock_drift_rate < 0.15:
        print("   â­ å¤±è´¥ä½†å¯¹é½ - æ–¹å‘å¯¹ä½†èƒ½åŠ›ä¸è¶³")
    else:
        print("   âŒ åŒé‡å¤±è´¥ - åŠŸèƒ½é”™ + è¿‡ç¨‹ä¹±")

    print("\n" + "=" * 80)
    print("âœ… Step 5-6 å®Œæˆï¼è¯„ä¼°å®Œæˆ")
    print("=" * 80)

    return {
        'resolved': resolved_result,
        'scope': scope_result,
        'drift_rate': mock_drift_rate,
    }


if __name__ == "__main__":
    result = main()
