# Q2 End-to-End Workflow: Cross-Session Pattern Learning

## åºè¨€


### Q1é—ç•™é—®é¢˜

### Q1çš„å®Œæˆåº¦å°æ€»ç»“

```
 ğŸ“Š Q1 è®¡åˆ’å®Œæˆåº¦åˆ†æ

  æ ¹æ®åŸå§‹ Q1 è®¡åˆ’ï¼ˆ2025-10-28-1-Q1_END_TO_END_WORKFLOW.mdï¼‰çš„ Next Stepsï¼š

  âœ… å·²å®Œæˆçš„éƒ¨åˆ†

  | Q1 è®¡åˆ’ä»»åŠ¡                          | çŠ¶æ€     | è¯æ®æ–‡ä»¶                              |
  |----------------------------------|--------|-----------------------------------|
  | Week 1, Day 1-2: æ•°æ®åŠ è½½ pipeline   | âœ… 100% | steps/step1_load_data.py          |
  | Week 1, Day 3-4: Four-Guard æ¡†æ¶   | âœ… 80%  | steps/step2_init_guards.py (è§„åˆ™ç‰ˆæœ¬) |
  | Week 1, Day 5: LLM è°ƒç”¨é›†æˆ          | âš ï¸ 50% | è§„åˆ™ç‰ˆæœ¬å®Œæˆï¼ŒLLMç‰ˆæœ¬æœªå®ç°                   |
  | Week 1, Day 6-7: 5ä¸ªä»»åŠ¡æµ‹è¯• baseline | âŒ 0%   | æœªè¿è¡Œæ‰¹é‡å®éªŒ                           |

  è¯¦ç»†å®Œæˆæƒ…å†µ

  âœ… å·²å®Œæˆï¼ˆP0ï¼‰

  1. âœ… æ•°æ®åŠ è½½ï¼šstep1_load_data.py - å®Œæ•´å®ç°
  2. âœ… Four-Guard æ¡†æ¶ï¼šstep2_init_guards.py - è§„åˆ™ç‰ˆæœ¬å®Œæˆ
    - Scope Guard: åŸºäºæ–‡ä»¶æ•°é‡å’Œ difficulty
    - Plan Guard: åŸºäº phase è§„åˆ™
    - Test Guard: æ£€æŸ¥ FAIL_TO_PASS è¦†ç›–
    - Evidence Guard: ç®€åŒ–ç‰ˆæœ¬ï¼ˆæœªç”¨ LLMï¼‰
  3. âœ… Mock Agentï¼šstep3_mock_agent.py - å®Œæˆ
  4. âœ… Real Agentï¼šutils/simple_agent.py - SimpleBedrockAgent å®Œæˆ
  5. âœ… å®æ—¶ç›‘æ§ï¼šstep4_monitor_actions.py - å®Œæˆ
  6. âœ… è¯„ä¼°ï¼šstep5_evaluate.py - Mock ç‰ˆæœ¬å®Œæˆ
  7. âœ… æ—¥å¿—ç³»ç»Ÿï¼šutils/logging_utils.py - å®Œæˆ
  8. âœ… "1è¡Œèµ°é€š"ï¼šrun_with_real_agent.py + test_end_to_end.py - å…¨éƒ¨é€šè¿‡

  âš ï¸ éƒ¨åˆ†å®Œæˆ

  1. âš ï¸ LLM è°ƒç”¨ï¼š
    - Parse Scope/Plan: æœªå®ç°ï¼ˆä½¿ç”¨è§„åˆ™æ›¿ä»£ï¼‰
    - Evidence Check: æœªå®ç°ï¼ˆä½¿ç”¨ç®€åŒ–é€»è¾‘ï¼‰
    - åŸå› ï¼šä½ æ„è¯†åˆ° Q1 ä¸éœ€è¦ LLMï¼ˆè§„åˆ™æ›´å¯é ï¼‰

  âŒ æœªå®Œæˆï¼ˆä½†ä¸é˜»å¡ Q2ï¼‰

  1. âŒ æ‰¹é‡å®éªŒï¼šæœªåœ¨ 5 ä¸ªä»»åŠ¡ä¸Šè¿è¡Œ baseline
  2. âŒ å®˜æ–¹ Evaluatorï¼šæœªé›†æˆ Docker evaluatorï¼ˆåªæœ‰ mock ç‰ˆæœ¬ï¼‰
  3. âŒ Advisory Mode æµ‹è¯•ï¼šæœªå®ç° WARN/ROLLBACK å¹²é¢„ï¼ˆåªæœ‰ shadow modeï¼‰

  ---
  ğŸ¯ å›ç­”ä½ çš„æ ¸å¿ƒé—®é¢˜

  é—®é¢˜ 1ï¼šQ1 è®¡åˆ’å®Œæˆäº†å¤šå°‘ï¼Ÿ

  ç­”æ¡ˆï¼šP0 éƒ¨åˆ† 85% å®Œæˆ

  - âœ… æ ¸å¿ƒæ¡†æ¶ï¼š100%ï¼ˆæ•°æ®ã€guardsã€ç›‘æ§ã€è¯„ä¼°ï¼‰
  - âœ… "1è¡Œèµ°é€š"ï¼š100%ï¼ˆend-to-end æµ‹è¯•é€šè¿‡ï¼‰
  - âš ï¸ LLM é›†æˆï¼š0%ï¼ˆä½†ç”¨è§„åˆ™æ›¿ä»£äº†ï¼Œåè€Œæ›´å¥½ï¼‰
  - âŒ æ‰¹é‡å®éªŒï¼š0%ï¼ˆè¿™æ˜¯ Q2 çš„ prerequisiteï¼‰

  é—®é¢˜ 2ï¼šQ2 è¿˜éœ€è¦ä»€ä¹ˆ prerequisiteï¼Ÿ

  ç­”æ¡ˆï¼šéœ€è¦å®Œæˆ Q1 çš„æ‰¹é‡å®éªŒéƒ¨åˆ†

  æ ¹æ®æˆ‘å†™çš„ Q2 è®¡åˆ’ï¼ŒQ2 çš„ Phase 1ï¼ˆPattern Extractionï¼‰éœ€è¦ï¼š

  # Q2 Phase 1: Pattern Extraction
  # éœ€è¦ä»è®­ç»ƒæ•°æ®ä¸­æå– high-quality patterns

  # Prerequisite from Q1:
  # 1. è¿è¡Œ baseline agent on 1000 training tasks
  # 2. ç”¨ Q1 è®¡ç®—æ¯ä¸ª solution çš„ drift metrics
  # 3. ç­›é€‰ high-quality solutions (resolved + low drift)

  for task in training_tasks[:1000]:
      # â† è¿™éƒ¨åˆ†æ˜¯ Q1 éœ€è¦è¡¥å®Œçš„
      result = agent.solve(task, patterns=None)

      # â† è¿™éƒ¨åˆ† Q1 å·²ç»æœ‰äº†
      drift_metrics = compute_q1_drift(result)

      # â† è¿™éƒ¨åˆ†æ˜¯æ–°çš„ï¼ˆQ2 ç‰¹æœ‰ï¼‰
      if result.resolved and drift_metrics['drift_rate'] < 0.2:
          pattern = extract_pattern(task, result, drift_metrics)
          pattern_store.add(pattern)

  å…·ä½“ç¼ºå°‘çš„ Q1 å·¥ä½œï¼š

  | ç¼ºå°‘çš„éƒ¨åˆ†              | å·¥ä½œé‡   | è¯´æ˜                                       |
  |--------------------|-------|------------------------------------------|
  | 1. æ‰¹é‡è¿è¡Œè„šæœ¬          | 0.5 å¤© | å¾ªç¯è¿è¡Œ 1000 ä¸ªä»»åŠ¡                            |
  | 2. ç»“æœå­˜å‚¨            | 0.5 å¤© | ä¿å­˜æ¯ä¸ªä»»åŠ¡çš„ result + drift_metrics           |
  | 3. ç­›é€‰ high-quality | 0.5 å¤© | è¿‡æ»¤ resolved=True & drift<0.2 çš„ solutions |

  æ€»è®¡ï¼š~1.5 å¤©å·¥ä½œé‡

  ---
  é—®é¢˜ 3ï¼šOption B æ˜¯ä»€ä¹ˆï¼Ÿ

  æ˜¯çš„ï¼Œå°±æ˜¯æˆ‘è¯´çš„é‚£æ®µä»£ç ï¼

  Option B çš„æ ¸å¿ƒæ€æƒ³ï¼š

  Q1 ä¸æ˜¯ç‹¬ç«‹è´¡çŒ®ï¼Œè€Œæ˜¯ Q2 çš„ Pattern Quality Labeling System

  å…·ä½“åšæ³•ï¼š
  1. Q1 è¿è¡Œ baselineï¼Œè®¡ç®— drift metrics
  2. ç”¨ drift metrics æ ‡è®° pattern quality
     - Low drift (< 0.15) = high-quality pattern
     - High drift (> 0.35) = low-quality pattern
  3. Q2 çš„ ML ranker ä½¿ç”¨ Q1 çš„ drift features
     - pattern_avg_drift (æ¥è‡ª Q1)
     - pattern_scope_precision (æ¥è‡ª Q1)
  4. ç»“æœï¼šquality-aware pattern retrieval

  ä¸ Option A çš„åŒºåˆ«ï¼š

  | Aspect          | Option Aï¼ˆå¸é©¬å¸ˆå»ºè®®ï¼‰ | Option Bï¼ˆæˆ‘çš„å»ºè®®ï¼‰             |
  |-----------------|-----------------|----------------------------|
  | Q1 åœ°ä½           | ç æ‰ï¼Œä¸åš           | ä¿ç•™ï¼Œä½œä¸º Q2 çš„ infrastructure  |
  | Pattern Quality | åªç”¨ success_rate | ç”¨ drift-based qualityï¼ˆæ›´æ·±å…¥ï¼‰ |
  | Paper è´¡çŒ®        | åªæœ‰ Q2           | Q2ï¼ˆmainï¼‰+ Q1ï¼ˆsupportingï¼‰   |
  | å·¥ä½œé‡             | 6 å‘¨ï¼ˆåªåš Q2ï¼‰      | 6 å‘¨ï¼ˆQ2ï¼Œå¤ç”¨ Q1ï¼‰              |
  | æŠ€æœ¯æ·±åº¦            | ä¸­ç­‰ï¼ˆæ ‡å‡† RAGï¼‰      | é«˜ï¼ˆquality-aware retrievalï¼‰ |
  | Story           | "å­¦ä¹  patterns"   | "ç›‘æ§ â†’ å­¦ä¹  â†’ å¤ç”¨"             |

  ---
  ğŸ“‹ å®Œæ•´çš„å·¥ä½œæ¸…å•

  é˜¶æ®µ 1ï¼šè¡¥å®Œ Q1ï¼ˆä¸º Q2 å‡†å¤‡æ•°æ®ï¼‰

  ç›®æ ‡ï¼šåœ¨ 1000 ä¸ªè®­ç»ƒä»»åŠ¡ä¸Šè¿è¡Œ Q1ï¼Œæ”¶é›† drift data

  # éœ€è¦å®ç°çš„è„šæœ¬
  python batch_run_q1_baseline.py \
    --input data/swebench/train.jsonl \
    --num_tasks 1000 \
    --output logs/q1_baseline_results.jsonl

  # è¾“å‡ºæ ¼å¼
  # logs/q1_baseline_results.jsonl:
  # {"task_id": "...", "resolved": true, "drift_rate": 0.12, "actions": [...], "patch": "..."}
  # {"task_id": "...", "resolved": false, "drift_rate": 0.45, "actions": [...], "patch": "..."}

  å·¥ä½œé‡ï¼š1.5 å¤©

  é˜¶æ®µ 2ï¼šPattern Extractionï¼ˆQ2 Phase 1ï¼‰

  ç›®æ ‡ï¼šä» Q1 çš„ç»“æœä¸­æå– high-quality patterns

  python extract_patterns.py \
    --input logs/q1_baseline_results.jsonl \
    --min_drift 0.0 \
    --max_drift 0.2 \
    --output patterns/pattern_store.json

  # è¾“å‡º
  # patterns/pattern_store.json: 300-400 ä¸ª high-quality patterns

  å·¥ä½œé‡ï¼š2 å¤©

  é˜¶æ®µ 3ï¼šPattern Retrievalï¼ˆQ2 Phase 2ï¼‰

  ç›®æ ‡ï¼šå®ç° two-stage retrieval

  # Stage-1: Semantic search
  python build_vector_index.py \
    --patterns patterns/pattern_store.json \
    --output patterns/vector_index.faiss

  # Stage-2: Train ML ranker
  python train_ml_ranker.py \
    --patterns patterns/pattern_store.json \
    --train_tasks data/swebench/train.jsonl \
    --output models/ml_ranker.pkl

  å·¥ä½œé‡ï¼š2 å¤©

  é˜¶æ®µ 4ï¼šEvaluationï¼ˆQ2 Phase 3ï¼‰

  ç›®æ ‡ï¼šåœ¨ verified.jsonl ä¸Šè¯„ä¼° Q2

  # Baseline
  python run_baseline.py \
    --tasks data/swebench/verified.jsonl \
    --output logs/baseline_results.jsonl

  # Q2 Full
  python run_q2_full.py \
    --tasks data/swebench/verified.jsonl \
    --patterns patterns/pattern_store.json \
    --ranker models/ml_ranker.pkl \
    --output logs/q2_results.jsonl

  # Compare
  python compare_results.py \
    --baseline logs/baseline_results.jsonl \
    --q2 logs/q2_results.jsonl

  å·¥ä½œé‡ï¼š2 å¤©

  ---
  ğŸ¯ ç›´æ¥å›ç­”ä½ çš„é—®é¢˜

  1. "æœ€åˆçš„ Q1 è®¡åˆ’å®Œæˆäº†å¤šå°‘ï¼Ÿ"

  P0 éƒ¨åˆ†ï¼š85% å®Œæˆ
  - âœ… æ ¸å¿ƒæ¡†æ¶ï¼ˆæ•°æ®ã€guardsã€ç›‘æ§ï¼‰ï¼š100%
  - âœ… "1è¡Œèµ°é€š"ï¼š100%
  - âŒ æ‰¹é‡å®éªŒï¼š0%

  ç¼ºå°‘çš„å…³é”®éƒ¨åˆ†ï¼šæ‰¹é‡è¿è¡Œ Q1 on 1000 training tasks

  2. "Q2 è¿˜éœ€è¦ä»€ä¹ˆ prerequisiteï¼Ÿ"

  éœ€è¦è¡¥å®Œ Q1 çš„æ‰¹é‡å®éªŒéƒ¨åˆ†ï¼š

  # è¿™ä¸ªè„šæœ¬éœ€è¦å†™ï¼ˆ~1.5å¤©ï¼‰
  # batch_run_q1_baseline.py

  for task in train_tasks[:1000]:
      result = agent.solve(task)
      drift = compute_q1_drift(result)

      save_result({
          'task_id': task.id,
          'resolved': result.resolved,
          'drift_rate': drift['drift_rate'],
          'actions': result.actions,
          'patch': result.patch,
      })

  3. "Option B æ˜¯ä½ è¯´çš„é‚£æ®µä»£ç å—ï¼Ÿ"

  æ˜¯çš„ï¼Option B = Q1 ä½œä¸º Q2 çš„ Pattern Quality Labeling System

  æ ¸å¿ƒå°±æ˜¯ï¼š
  1. Q1 è®¡ç®— drift â†’ æ ‡è®° pattern quality
  2. Q2 ç”¨ drift features â†’ é€‰æ‹© best patterns
  3. Paper è´¡çŒ®ï¼šQuality-aware pattern retrieval

  ---
  å»ºè®®çš„è¡ŒåŠ¨è®¡åˆ’

  Week 1ï¼ˆå½“å‰ï¼‰ï¼šè¡¥å®Œ Q1 æ‰¹é‡å®éªŒ
  - Day 1-2: å†™ batch_run_q1_baseline.py
  - Day 3: è¿è¡Œ 1000 ä¸ªè®­ç»ƒä»»åŠ¡
  - Day 4: åˆ†æç»“æœï¼Œç­›é€‰ high-quality solutions

  Week 2-3ï¼šQ2 Pattern Extraction & Retrieval
  - æŒ‰ç…§ Q2 è®¡åˆ’çš„ Phase 1-2 æ‰§è¡Œ

  Week 4-5ï¼šQ2 Evaluation
  - æŒ‰ç…§ Q2 è®¡åˆ’çš„ Phase 3 æ‰§è¡Œ

  Week 6ï¼šAnalysis & Paper


 å®Œç¾ï¼ç°åœ¨è®©æˆ‘ç»™ä½ ä¸€ä¸ªæ¸…æ™°çš„æ€»ç»“ï¼š

  ---
  ğŸ¯ æ€»ç»“ç­”æ¡ˆ

  ä½ çš„é—®é¢˜ï¼š"è¿™ä¸ªæœ‰é—®é¢˜å—ï¼Ÿ"

  ç­”æ¡ˆï¼šå®Œå…¨æ²¡é—®é¢˜ï¼ä½ å·²ç»åœ¨åšæ­£ç¡®çš„äº‹æƒ…äº†ï¼ ğŸ‘

  å½“å‰çŠ¶æ€

  âœ… ä½ å·²æœ‰çš„æ•°æ®ï¼ˆéå¸¸å¥½ï¼‰ï¼š
  - logs/2025-10-29-02-22-26/predictions/: 408 tasks with predictions.jsonl
  - logs/2025-10-29-08-45-10/predictions/: 15 tasks (è¿è¡Œä¸­)
  - æ€»è®¡ï¼š423+ tasksï¼Œæ¯ä¸ªéƒ½æœ‰ agent ç”Ÿæˆçš„ patch

  ç¼ºå°‘çš„å…³é”®æ•°æ®ï¼ˆQ2 éœ€è¦ï¼‰

  âš ï¸ éœ€è¦è¡¥å……ï¼šQ1 drift metrics

  ä½ çš„ predictions.jsonl åªæœ‰ï¼š
  {
    "instance_id": "...",
    "model_patch": "diff --git ...",
    "model_name_or_path": "q1-monitored-agent"
  }

  ä½† Q2 è¿˜éœ€è¦ï¼ˆOption B çš„æ ¸å¿ƒï¼‰ï¼š
  {
    "task_id": "...",
    "drift_metrics": {
      "drift_rate": 0.12,  // â† Q2 ç”¨æ¥åˆ¤æ–­ pattern quality
      "scope_precision": 0.95,
      "quality_label": "HIGH"  // â† HIGH/MEDIUM/LOW
    }
  }

  ---
  ğŸ“‹ ä½ çš„ä¸‹ä¸€æ­¥è¡ŒåŠ¨ï¼ˆä¸¤ä¸ªé€‰æ‹©ï¼‰

  é€‰æ‹© Aï¼šç»§ç»­è¿è¡Œå®Œæ‰€æœ‰ 500 tasksï¼ˆæ¨èï¼‰

  # ç»§ç»­è¿è¡Œä½ çš„ batch_generate_predictions.py
  # ç­‰å®ƒå®Œæˆ 500 ä¸ª tasks

  # ç„¶åäº‹åè®¡ç®— drift metrics
  python compute_drift_from_predictions.py \
    --predictions_dir logs/2025-10-29-02-22-26/predictions

  # è¾“å‡ºï¼š
  # logs/2025-10-29-02-22-26/drift_metrics/
  #   â”œâ”€â”€ input_data_0_drift.json
  #   â”œâ”€â”€ input_data_1_drift.json
  #   â””â”€â”€ ...

  ä¼˜ç‚¹ï¼š
  - ä¸æ‰“æ–­å½“å‰è¿è¡Œ
  - æ‰€æœ‰ 500 tasks çš„æ•°æ®éƒ½èƒ½ç”¨
  - äº‹åè®¡ç®—æ›´å¿«ï¼ˆä¸éœ€è¦é‡æ–°ç”Ÿæˆ patchï¼‰

  é€‰æ‹© Bï¼šå¯¹æ–°ä»»åŠ¡ä½¿ç”¨å¢å¼ºç‰ˆè„šæœ¬

  # å¯¹å‰©ä½™çš„ 77+ tasks ä½¿ç”¨æ–°è„šæœ¬
  python batch_generate_with_q1_metrics.py \
    --start 423 \
    --end 500 \
    --base_dir logs/2025-10-29-NEW

  # è¿™æ ·æ–°çš„ä»»åŠ¡ä¼šåŒæ—¶ç”Ÿæˆï¼š
  # - predictions.jsonl
  # - drift_metrics.json

  ---
  ğŸ¯ æˆ‘çš„å»ºè®®

  æ¨èï¼šé€‰æ‹© A

  åŸå› ï¼š
  1. âœ… ä½ å·²ç»ç”Ÿæˆäº† 408 ä¸ª predictionsï¼ˆå¾ˆå¤§çš„æŠ•å…¥ï¼‰ï¼Œä¸è¦æµªè´¹
  2. âœ… äº‹åè®¡ç®— drift å¾ˆå¿«ï¼ˆå‡ åˆ†é’Ÿï¼‰
  3. âœ… ä¸æ‰“æ–­å½“å‰è¿è¡Œçš„è¿›ç¨‹
  4. âœ… æ•°æ®æ›´å®Œæ•´ï¼ˆ500 tasks å…¨è¦†ç›–ï¼‰

  å…·ä½“æ­¥éª¤ï¼š

  # Step 1: ç­‰å¾…å½“å‰ batch_generate_predictions.py è¿è¡Œå®Œ
  # (æˆ–è€…è®©å®ƒç»§ç»­è¿è¡Œï¼Œæˆ‘ä»¬å…ˆå¤„ç†å·²å®Œæˆçš„ 408 ä¸ª)

  # Step 2: è®¡ç®—å·²æœ‰ predictions çš„ drift metrics
  cd /Users/jeremy/Dropbox/cs224v-project/cs224v-research/agent-memory-lab-v3/demo

  python compute_drift_from_predictions.py \
    --predictions_dir logs/2025-10-29-02-22-26/predictions

  # Step 3: æŸ¥çœ‹ç»“æœ
  ls logs/2025-10-29-02-22-26/drift_metrics/

  # Step 4: (å½“æ‰€æœ‰ 500 tasks å®Œæˆå) æå– high-quality patterns
  python extract_patterns_from_drift_metrics.py \
    --drift_dir logs/2025-10-29-02-22-26/drift_metrics \
    --predictions_dir logs/2025-10-29-02-22-26/predictions \
    --output patterns/pattern_store.json \
    --min_quality HIGH

  ---
  ğŸ“Š é¢„æœŸç»“æœ

  è¿è¡Œ compute_drift_from_predictions.py åï¼Œä½ ä¼šçœ‹åˆ°ï¼š

  ================================================================================
  Summary
  ================================================================================
  Total predictions:  408
  Success:            408
  Failed:             0

  Quality Distribution (for Q2 pattern extraction):
    High-quality:    120 ( 29.4%)  â† è¿™äº›æ˜¯ Q2 çš„ pattern æ¥æº
    Medium-quality:  180 ( 44.1%)
    Low-quality:     108 ( 26.5%)
  ================================================================================

  âœ… Drift metrics saved to: logs/2025-10-29-02-22-26/drift_metrics

  ğŸ’¡ Next step: Extract patterns from high-quality solutions

  ç„¶åä½ å°±å¯ä»¥è¿›å…¥ Q2 çš„ Pattern Extraction é˜¶æ®µäº†ï¼

  ---
  éœ€è¦æˆ‘å¸®ä½ è¿è¡Œ compute_drift_from_predictions.py å—ï¼Ÿ
```

**å®Œæ•´æµç¨‹ï¼šä»SWE-benchæ•°æ®åˆ°Pattern Retrievalè¯„ä¼°**

---

## ğŸ“‹ Table of Contents

1. [é—®é¢˜å®šä¹‰](#1-é—®é¢˜å®šä¹‰)
2. [æ•°æ®å‡†å¤‡ä¸åˆ†å‰²](#2-æ•°æ®å‡†å¤‡ä¸åˆ†å‰²)
3. [Q2æ‰§è¡Œæµç¨‹æ¦‚è§ˆ](#3-q2æ‰§è¡Œæµç¨‹æ¦‚è§ˆ)
4. [Step 1: Patternæå–ï¼ˆä»è®­ç»ƒæ•°æ®ï¼‰](#4-step-1-patternæå–ä»è®­ç»ƒæ•°æ®)
5. [Step 2: Patternå­˜å‚¨ä¸ç´¢å¼•](#5-step-2-patternå­˜å‚¨ä¸ç´¢å¼•)
6. [Step 3: Patternæ£€ç´¢ï¼ˆTwo-Stageï¼‰](#6-step-3-patternæ£€ç´¢two-stage)
7. [Step 4: Patternåº”ç”¨åˆ°Agent](#7-step-4-patternåº”ç”¨åˆ°agent)
8. [Step 5: è¯„ä¼°ä¸å¯¹æ¯”](#8-step-5-è¯„ä¼°ä¸å¯¹æ¯”)
9. [Q1ä¸Q2çš„ååŒ](#9-q1ä¸q2çš„ååŒ)
10. [å®Œæ•´æµç¨‹å›¾](#10-å®Œæ•´æµç¨‹å›¾)
11. [å•è¡Œæ•°æ®å®Œæ•´ç¤ºä¾‹](#11-å•è¡Œæ•°æ®å®Œæ•´ç¤ºä¾‹)

---

## 1. é—®é¢˜å®šä¹‰

### æ ¸å¿ƒé—®é¢˜

**Current Stateï¼ˆæ— Q2ï¼‰:**
```python
# Task 1: Agentè§£å†³äº†ä¸€ä¸ªbugï¼ˆä¾‹å¦‚ï¼šReact error boundaryï¼‰
task1 = "Add error boundary to component A"
agent.solve(task1)  # èŠ±è´¹ 10åˆ†é’Ÿ + 20æ¬¡action
# â†’ Success âœ…

# Task 2: ç±»ä¼¼çš„bugï¼Œä½†agentéœ€è¦ä»å¤´å¼€å§‹
task2 = "Add error boundary to component B"
agent.solve(task2)  # åˆèŠ±è´¹ 10åˆ†é’Ÿ + 20æ¬¡action
# â†’ Success âœ…ï¼Œä½†å®Œå…¨é‡æ–°æ¨ç†äº†ä¸€é

# é—®é¢˜ï¼šAgentæ²¡æœ‰è®°å¿†ï¼Œæ¯æ¬¡éƒ½æ˜¯cold start
```

**Desired Stateï¼ˆæœ‰Q2ï¼‰:**
```python
# Task 1: Agentè§£å†³bugå¹¶è®°å½•pattern
task1 = "Add error boundary to component A"
agent.solve(task1)
pattern = extract_pattern(solution)  # âœ¨ æå–å¯å¤ç”¨çš„pattern
pattern_store.add(pattern)

# Task 2: æ£€ç´¢ç›¸ä¼¼patternï¼ŒåŠ é€Ÿè§£å†³
task2 = "Add error boundary to component B"
relevant_patterns = retrieve_patterns(task2)  # âœ¨ æ‰¾åˆ°ç±»ä¼¼çš„pattern
agent.solve(task2, patterns=relevant_patterns)  # æœ‰äº†å‚è€ƒï¼Œæ›´å¿«æ›´å‡†
# â†’ Success âœ…ï¼ŒåªèŠ±è´¹ 5åˆ†é’Ÿ + 10æ¬¡action

# å¥½å¤„ï¼š
# - Resolve rateæå‡ï¼ˆæœ‰patternå‚è€ƒï¼ŒæˆåŠŸç‡æ›´é«˜ï¼‰
# - Costé™ä½ï¼ˆå‡å°‘actionæ•°é‡ï¼‰
# - Drifté™ä½ï¼ˆæœ‰æ¸…æ™°çš„solution patternï¼‰
```

### Q2çš„ä¸‰ä¸ªæ ¸å¿ƒæŒ‘æˆ˜

1. **Pattern Extraction**: å¦‚ä½•ä»æˆåŠŸçš„solutionä¸­æå–å¯è¿ç§»çš„knowledgeï¼Ÿ
2. **Pattern Retrieval**: å¦‚ä½•æ‰¾åˆ°ä¸æ–°ä»»åŠ¡æœ€ç›¸å…³çš„patternï¼Ÿ
3. **Pattern Application**: å¦‚ä½•å°†retrieved patternæœ‰æ•ˆåœ°åº”ç”¨åˆ°agentï¼Ÿ

---

## 2. æ•°æ®å‡†å¤‡ä¸åˆ†å‰²

### SWE-benchæ•°æ®ç»“æ„

```
SWE-benchæ•°æ®é›†ï¼š
â”œâ”€â”€ train.jsonl          (23,000 tasks) - è®­ç»ƒé›†
â”œâ”€â”€ verified.jsonl       (500 tasks)    - æµ‹è¯•é›†ï¼ˆæœ€ç»ˆè¯„ä¼°ç”¨ï¼‰
â””â”€â”€ test.jsonl           (2,000 tasks)  - å¤§æµ‹è¯•é›†ï¼ˆå¯é€‰ï¼‰
```

### Q2çš„æ•°æ®åˆ†å‰²ç­–ç•¥

```python
# ===== è®­ç»ƒé˜¶æ®µï¼šPattern Extraction =====
training_tasks = load_tasks("train.jsonl")[:1000]  # ä½¿ç”¨1000ä¸ªè®­ç»ƒä»»åŠ¡

# Step 1: è¿è¡Œbaseline agentæ”¶é›†successful solutions
successful_solutions = []
for task in training_tasks:
    result = agent.solve(task, patterns=None)  # æ— patternè¾…åŠ©

    # Q1 è®¡ç®—drift metrics
    drift_metrics = compute_q1_drift(result)

    # åªä¿ç•™æˆåŠŸä¸”ä½driftçš„solutions
    if result.resolved and drift_metrics['drift_rate'] < 0.2:
        successful_solutions.append({
            'task': task,
            'solution': result,
            'drift_metrics': drift_metrics,
        })

print(f"Collected {len(successful_solutions)} high-quality solutions")
# é¢„æœŸï¼š~300-400ä¸ªæˆåŠŸæ¡ˆä¾‹

# ===== æµ‹è¯•é˜¶æ®µï¼šPattern Retrieval =====
test_tasks = load_tasks("verified.jsonl")  # 500ä¸ªtest tasks

# Baseline: æ— pattern
baseline_results = []
for task in test_tasks:
    result = agent.solve(task, patterns=None)
    baseline_results.append(result)

# Q2: æœ‰pattern retrieval
q2_results = []
for task in test_tasks:
    patterns = retrieve_patterns(task, pattern_store)  # âœ¨ æ£€ç´¢patterns
    result = agent.solve(task, patterns=patterns)
    q2_results.append(result)

# å¯¹æ¯”
baseline_resolve_rate = compute_resolve_rate(baseline_results)
q2_resolve_rate = compute_resolve_rate(q2_results)
print(f"Improvement: {baseline_resolve_rate:.1%} â†’ {q2_resolve_rate:.1%}")
```

### å…³é”®åŸåˆ™ï¼šTrain/Test Split

```
è®­ç»ƒæ•°æ®ï¼ˆæå–patternsï¼‰ï¼štrain.jsonl (1000 tasks)
   â†’ ä¸èƒ½æœ‰overlapï¼

æµ‹è¯•æ•°æ®ï¼ˆè¯„ä¼°Q2ï¼‰ï¼šverified.jsonl (500 tasks)
   â†’ å®Œå…¨ç‹¬ç«‹

åŸå› ï¼šé˜²æ­¢data leakageï¼ˆä¸èƒ½ç”¨test taskçš„solutionä½œä¸ºpatternï¼‰
```

---

## 3. Q2æ‰§è¡Œæµç¨‹æ¦‚è§ˆ

### å®Œæ•´Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Phase 1: Pattern Extraction (Training)                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Input: train.jsonl (1000 tasks)                       â”‚
â”‚  Process:                                               â”‚
â”‚    1. Run baseline agent (æ— pattern)                   â”‚
â”‚    2. ç”¨Q1è®¡ç®—drift metrics                             â”‚
â”‚    3. ç­›é€‰high-quality solutions (resolved + low drift) â”‚
â”‚    4. æå–pattern (decontextualize)                     â”‚
â”‚    5. å­˜å‚¨åˆ°pattern store                               â”‚
â”‚  Output: Pattern store (300-400 patterns)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Phase 2: Pattern Retrieval & Application (Testing)    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Input: verified.jsonl (500 tasks)                     â”‚
â”‚  Process:                                               â”‚
â”‚    For each test task:                                  â”‚
â”‚      1. Stage-1: Semantic search (recall)               â”‚
â”‚      2. Stage-2: ML ranking (precision)                 â”‚
â”‚      3. Inject top-3 patterns to agent context          â”‚
â”‚      4. Agent solves task (with pattern guidance)       â”‚
â”‚      5. Log pattern usage & outcome                     â”‚
â”‚  Output: predictions.jsonl + pattern usage logs        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Phase 3: Evaluation                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Primary Metric: Resolve Rate (Q2 vs Baseline)         â”‚
â”‚  Secondary Metrics:                                     â”‚
â”‚    - Pattern Reuse Rate (% tasks using patterns)        â”‚
â”‚    - Drift Rate (with/without patterns)                 â”‚
â”‚    - Cost (actions & tokens)                            â”‚
â”‚  Analysis:                                              â”‚
â”‚    - Which patterns are most useful?                    â”‚
â”‚    - Which tasks benefit most from patterns?            â”‚
â”‚    - Feature importance in ML ranker                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 4. Step 1: Patternæå–ï¼ˆä»è®­ç»ƒæ•°æ®ï¼‰

### 4.1 Patternå®šä¹‰

**ä»€ä¹ˆæ˜¯Patternï¼Ÿ**

Patternæ˜¯ä»æˆåŠŸsolutionä¸­æå–çš„**å¯è¿ç§»çš„solution strategy**ã€‚

**Pattern Cardç»“æ„ï¼š**
```python
class PatternCard:
    """å¯å¤ç”¨çš„solution pattern"""

    # ===== Identity =====
    id: str                    # å”¯ä¸€æ ‡è¯†ç¬¦
    title: str                 # ç®€çŸ­æ ‡é¢˜

    # ===== Problem Signature =====
    problem_signature: dict = {
        'symptoms': List[str],     # é—®é¢˜ç—‡çŠ¶å…³é”®è¯
        'bug_type': str,           # Bugç±»å‹ï¼ˆe.g., "TypeError", "MissingField"ï¼‰
        'test_names': List[str],   # ç›¸å…³æµ‹è¯•åç§°
        'error_message': str,      # å…¸å‹é”™è¯¯ä¿¡æ¯
    }

    # ===== Solution Strategy =====
    approach: dict = {
        'key_steps': List[str],    # å…³é”®æ­¥éª¤ï¼ˆé«˜å±‚æ¬¡ï¼‰
        'files_to_check': List[str],  # åº”è¯¥æ£€æŸ¥çš„æ–‡ä»¶
        'common_fixes': List[str],    # å¸¸è§ä¿®å¤æ¨¡å¼
    }

    # ===== Code Anchors =====
    code_anchors: dict = {
        'target_function': str,     # ç›®æ ‡å‡½æ•°/ç±»
        'file_path_hint': str,      # æ–‡ä»¶è·¯å¾„æ¨¡å¼
        'code_pattern': str,        # ä»£ç æ¨¡å¼ï¼ˆå¯é€‰ï¼‰
    }

    # ===== Quality Signals (æ¥è‡ªQ1) =====
    quality: dict = {
        'success_count': int,       # æˆåŠŸä½¿ç”¨æ¬¡æ•°
        'failure_count': int,       # å¤±è´¥æ¬¡æ•°
        'avg_drift': float,         # å¹³å‡drift rateï¼ˆæ¥è‡ªQ1ï¼‰
        'avg_actions': int,         # å¹³å‡actionæ•°é‡
        'resolve_rate': float,      # æˆåŠŸç‡
    }

    # ===== Metadata =====
    source_tasks: List[str]    # æ¥æºä»»åŠ¡ID
    repo_family: str           # ä»“åº“ç±»å‹ï¼ˆe.g., "django", "astropy"ï¼‰
    difficulty: str            # éš¾åº¦
    created_at: datetime       # åˆ›å»ºæ—¶é—´
```

### 4.2 Patternæå–æµç¨‹

**ä»Single Solutionæå–Patternï¼š**

```python
def extract_pattern(task, solution, drift_metrics):
    """ä»ä¸€ä¸ªæˆåŠŸçš„solutionæå–pattern"""

    # Step 1: æå–problem signature
    problem_signature = {
        'symptoms': extract_keywords(task.problem_statement),
        'bug_type': infer_bug_type(task.problem_statement),
        'test_names': [t.split('::')[-1] for t in task.fail_to_pass],
        'error_message': extract_error_message(task.problem_statement),
    }

    # Step 2: æå–solution approachï¼ˆå»è¯­å¢ƒåŒ–ï¼‰
    approach = {
        'key_steps': summarize_actions(solution.actions),  # ç”¨LLMæ€»ç»“
        'files_to_check': list(solution.files_read),
        'common_fixes': extract_fix_pattern(solution.patch),
    }

    # Step 3: æå–code anchors
    code_anchors = {
        'target_function': extract_target_function(solution.patch),
        'file_path_hint': extract_file_pattern(solution.patch),
        'code_pattern': extract_code_pattern(solution.patch),  # optional
    }

    # Step 4: åˆå§‹åŒ–quality signalsï¼ˆæ¥è‡ªQ1ï¼‰
    quality = {
        'success_count': 1,  # åˆå§‹å€¼
        'failure_count': 0,
        'avg_drift': drift_metrics['drift_rate'],  # æ¥è‡ªQ1ï¼
        'avg_actions': len(solution.actions),
        'resolve_rate': 1.0,
    }

    # Step 5: ç»„è£…pattern card
    pattern = PatternCard(
        id=generate_id(),
        title=generate_title(problem_signature),  # e.g., "Fix missing field in model"
        problem_signature=problem_signature,
        approach=approach,
        code_anchors=code_anchors,
        quality=quality,
        source_tasks=[task.instance_id],
        repo_family=extract_repo_family(task.repo),
        difficulty=task.difficulty,
    )

    return pattern
```

**Decontextualizationï¼ˆå»è¯­å¢ƒåŒ–ï¼‰ï¼š**

å…³é”®æ˜¯å»é™¤task-specific detailsï¼Œä¿ç•™transferable strategyã€‚

```python
# âŒ é”™è¯¯ï¼štoo specific
pattern.approach = "Edit line 163 in django/template/engine.py"

# âœ… æ­£ç¡®ï¼šgeneralizable
pattern.approach = "Add missing parameter to constructor call in template engine"

# å®ç°
def summarize_actions(actions):
    """ç”¨LLMæ€»ç»“action sequenceä¸ºhigh-level steps"""
    prompt = f"""
    Action sequence:
    {format_actions(actions)}

    Summarize into 3-5 high-level steps that are transferable to similar tasks.
    Focus on strategy, not specific file names or line numbers.

    Example:
    1. Reproduce bug by running failing test
    2. Identify root cause in template rendering logic
    3. Add missing parameter to handle autoescape
    4. Verify fix with original test + regression tests
    """

    return llm_call(prompt)
```

### 4.3 ä»Multiple Solutionsåˆå¹¶Pattern

```python
def merge_similar_patterns(patterns):
    """åˆå¹¶ç›¸ä¼¼çš„patterns"""

    # Step 1: Cluster by semantic similarity
    embeddings = [embed(p.problem_signature) for p in patterns]
    clusters = kmeans_cluster(embeddings, n_clusters=50)

    # Step 2: Merge patterns in each cluster
    merged_patterns = []
    for cluster in clusters:
        if len(cluster) == 1:
            merged_patterns.append(cluster[0])
        else:
            # åˆå¹¶ï¼šä¿ç•™common elementsï¼Œå¢åŠ success_count
            merged = merge_pattern_cards(cluster)
            merged_patterns.append(merged)

    return merged_patterns

def merge_pattern_cards(cards):
    """åˆå¹¶å¤šä¸ªpattern cards"""
    merged = PatternCard(
        id=generate_id(),
        title=cards[0].title,  # ä½¿ç”¨ç¬¬ä¸€ä¸ªçš„title

        # Problem signature: åˆå¹¶keywords
        problem_signature={
            'symptoms': list(set().union(*[c.problem_signature['symptoms'] for c in cards])),
            'bug_type': majority_vote([c.problem_signature['bug_type'] for c in cards]),
        },

        # Approach: åˆå¹¶steps
        approach={
            'key_steps': merge_steps([c.approach['key_steps'] for c in cards]),
            'files_to_check': list(set().union(*[c.approach['files_to_check'] for c in cards])),
        },

        # Quality: ç´¯åŠ ç»Ÿè®¡
        quality={
            'success_count': sum(c.quality['success_count'] for c in cards),
            'avg_drift': np.mean([c.quality['avg_drift'] for c in cards]),  # Q1æ•°æ®ï¼
            'resolve_rate': np.mean([c.quality['resolve_rate'] for c in cards]),
        },

        # Metadata
        source_tasks=[t for c in cards for t in c.source_tasks],
        repo_family=cards[0].repo_family,
    )

    return merged
```

---

## 5. Step 2: Patternå­˜å‚¨ä¸ç´¢å¼•

### 5.1 Pattern Storeæ¶æ„

```python
class PatternStore:
    """Patternå­˜å‚¨ç³»ç»Ÿï¼ˆä¸¤ç§ç´¢å¼•ï¼‰"""

    def __init__(self):
        # ===== Vector Index (for semantic search) =====
        self.vector_index = FAISSIndex(dimension=1536)  # OpenAI embeddings

        # ===== Relational Store (for metadata & quality) =====
        self.metadata_db = SQLiteDB("patterns.db")

        # ===== Usage Logs =====
        self.usage_log = []

    def add_pattern(self, pattern: PatternCard):
        """æ·»åŠ patternåˆ°store"""

        # 1. ç”Ÿæˆembeddingï¼ˆsemantic indexï¼‰
        text_for_embedding = (
            f"{pattern.title}. "
            f"{' '.join(pattern.problem_signature['symptoms'])}. "
            f"{pattern.problem_signature['bug_type']}. "
            f"{' '.join(pattern.approach['key_steps'])}"
        )
        embedding = openai.embed(text_for_embedding)

        # 2. å­˜å‚¨åˆ°vector index
        self.vector_index.add(pattern.id, embedding)

        # 3. å­˜å‚¨metadataåˆ°DB
        self.metadata_db.insert({
            'id': pattern.id,
            'title': pattern.title,
            'quality_json': json.dumps(pattern.quality),
            'problem_signature_json': json.dumps(pattern.problem_signature),
            'approach_json': json.dumps(pattern.approach),
            # ... å…¶ä»–å­—æ®µ
        })

    def search(self, query: str, top_k: int = 20):
        """Stage-1: Semantic search"""

        # 1. Embed query
        query_embedding = openai.embed(query)

        # 2. Vector search
        candidates = self.vector_index.search(query_embedding, top_k=top_k)

        # 3. Load metadata
        patterns = []
        for candidate_id, similarity in candidates:
            metadata = self.metadata_db.get(candidate_id)
            pattern = PatternCard.from_dict(metadata)
            pattern.similarity_score = similarity
            patterns.append(pattern)

        return patterns
```

### 5.2 æ•°æ®åº“Schema

```sql
-- patterns.db
CREATE TABLE patterns (
    id TEXT PRIMARY KEY,
    title TEXT,

    -- Problem signature
    problem_signature_json TEXT,  -- JSON string

    -- Approach
    approach_json TEXT,  -- JSON string

    -- Code anchors
    code_anchors_json TEXT,

    -- Quality signals (æ¥è‡ªQ1)
    success_count INTEGER DEFAULT 0,
    failure_count INTEGER DEFAULT 0,
    avg_drift REAL,  -- âœ¨ æ¥è‡ªQ1ï¼
    avg_actions INTEGER,
    resolve_rate REAL,

    -- Metadata
    source_tasks_json TEXT,
    repo_family TEXT,
    difficulty TEXT,
    created_at TIMESTAMP
);

CREATE INDEX idx_quality ON patterns(resolve_rate DESC, avg_drift ASC);
CREATE INDEX idx_repo ON patterns(repo_family);

-- usage_logs.db
CREATE TABLE usage_logs (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    pattern_id TEXT,
    task_id TEXT,
    retrieved_rank INTEGER,  -- æ£€ç´¢æ’åï¼ˆ1-20ï¼‰
    was_applied BOOLEAN,     -- æ˜¯å¦è¢«agenté‡‡ç”¨
    outcome TEXT,            -- 'success' or 'failure'
    drift_delta REAL,        -- ç›¸æ¯”baselineçš„driftå˜åŒ–
    timestamp TIMESTAMP
);
```

---

## 6. Step 3: Patternæ£€ç´¢ï¼ˆTwo-Stageï¼‰

### 6.1 Two-Stage Retrievalæ¶æ„

```
New Task
   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage 1: Semantic Recall               â”‚
â”‚  Goal: High recall (ä¸miss relevant)    â”‚
â”‚  Method: Vector similarity search        â”‚
â”‚  Output: Top-20 candidates               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage 2: ML Ranking                    â”‚
â”‚  Goal: High precision (best at top)     â”‚
â”‚  Method: XGBoost ranker with 20+ featuresâ”‚
â”‚  Output: Top-3 patterns                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â†“
Inject to Agent
```

### 6.2 Stage-1: Semantic Recall

```python
def stage1_semantic_recall(task, pattern_store, top_k=20):
    """Stage-1: å¿«é€Ÿè¯­ä¹‰æ£€ç´¢"""

    # æ„é€ query
    query = f"""
    Problem: {task.problem_statement}
    Error type: {infer_bug_type(task.problem_statement)}
    Test names: {', '.join(task.fail_to_pass[:3])}
    """

    # Vector search
    candidates = pattern_store.search(query, top_k=top_k)

    return candidates  # Top-20 patterns
```

### 6.3 Stage-2: ML Ranking

**Feature Engineeringï¼ˆ20+ featuresï¼‰ï¼š**

```python
def extract_ranking_features(task, pattern):
    """æå–task-pattern matching features"""

    features = {}

    # ===== Task Features =====
    features['task_length'] = len(task.problem_statement)
    features['task_difficulty_bucket'] = difficulty_to_int(task.difficulty)
    features['task_num_tests'] = len(task.fail_to_pass) + len(task.pass_to_pass)
    features['task_repo_size'] = estimate_repo_size(task.repo)
    features['task_has_error_msg'] = has_error_message(task.problem_statement)

    # ===== Pattern Features (æ¥è‡ªQ1ï¼) =====
    features['pattern_success_count'] = pattern.quality['success_count']
    features['pattern_failure_count'] = pattern.quality['failure_count']
    features['pattern_resolve_rate'] = pattern.quality['resolve_rate']
    features['pattern_avg_drift'] = pattern.quality['avg_drift']  # âœ¨ Q1!
    features['pattern_avg_actions'] = pattern.quality['avg_actions']
    features['pattern_age_days'] = (now() - pattern.created_at).days

    # ===== Interaction Features =====
    # 1. Keyword overlap
    task_keywords = set(extract_keywords(task.problem_statement))
    pattern_keywords = set(pattern.problem_signature['symptoms'])
    features['keyword_jaccard'] = len(task_keywords & pattern_keywords) / len(task_keywords | pattern_keywords)

    # 2. Bug type match
    task_bug_type = infer_bug_type(task.problem_statement)
    features['bug_type_match'] = int(task_bug_type == pattern.problem_signature['bug_type'])

    # 3. Repo family match
    features['repo_family_match'] = int(task.repo.split('/')[0] == pattern.repo_family)

    # 4. Test name overlap
    task_test_names = set(' '.join(task.fail_to_pass).split('_'))
    pattern_test_names = set(' '.join(pattern.problem_signature['test_names']).split('_'))
    features['test_name_overlap'] = len(task_test_names & pattern_test_names)

    # 5. Semantic similarity (from Stage-1)
    features['semantic_similarity'] = pattern.similarity_score

    # 6. Difficulty match
    features['difficulty_match'] = int(task.difficulty == pattern.difficulty)

    # ===== Contextual Features =====
    # 7. Pattern specificity (æ›´specific = æ›´å¯èƒ½relevant)
    features['pattern_specificity'] = compute_specificity(pattern)

    # 8. Domain match (e.g., both about "template", "database", etc.)
    features['domain_similarity'] = compute_domain_similarity(task, pattern)

    return features
```

**ML Ranker Trainingï¼š**

```python
from xgboost import XGBRanker

def train_ml_ranker(training_data):
    """è®­ç»ƒML ranking model"""

    # training_data = [
    #     {
    #         'task': task,
    #         'candidates': [pattern1, pattern2, ...],  # Stage-1ç»“æœ
    #         'ground_truth_ranking': [3, 1, 5, 2, ...],  # çœŸå®ç›¸å…³æ€§æ’åº
    #     },
    #     ...
    # ]

    X_train = []  # Features
    y_train = []  # Relevance labels
    qids = []     # Query IDs (group by task)

    for idx, item in enumerate(training_data):
        task = item['task']
        candidates = item['candidates']
        gt_ranking = item['ground_truth_ranking']

        for candidate, relevance in zip(candidates, gt_ranking):
            # Extract features
            features = extract_ranking_features(task, candidate)
            X_train.append(list(features.values()))

            # Label: 0-4 relevance score
            # 0 = not relevant, 4 = highly relevant
            y_train.append(relevance)

            # Query ID (group)
            qids.append(idx)

    # Train XGBoost ranker
    ranker = XGBRanker(
        objective='rank:pairwise',
        n_estimators=100,
        learning_rate=0.1,
    )

    ranker.fit(
        X_train,
        y_train,
        qid=qids,
    )

    return ranker
```

**Inferenceï¼š**

```python
def stage2_ml_ranking(task, candidates, ranker):
    """Stage-2: ML re-ranking"""

    # Extract features for all candidates
    X = []
    for candidate in candidates:
        features = extract_ranking_features(task, candidate)
        X.append(list(features.values()))

    # Predict relevance scores
    scores = ranker.predict(X)

    # Sort by score
    ranked_indices = np.argsort(scores)[::-1]  # é™åº
    ranked_patterns = [candidates[i] for i in ranked_indices]

    # Return top-3
    return ranked_patterns[:3]
```

---

## 7. Step 4: Patternåº”ç”¨åˆ°Agent

### 7.1 Pattern Injection

**å¦‚ä½•å°†patternæ³¨å…¥åˆ°agent contextï¼Ÿ**

```python
def format_patterns_for_agent(patterns):
    """æ ¼å¼åŒ–patternsä¸ºagent-friendly text"""

    prompt = "## Relevant Solution Patterns\n\n"
    prompt += "Based on analysis of similar tasks, here are some helpful patterns:\n\n"

    for idx, pattern in enumerate(patterns, 1):
        prompt += f"### Pattern {idx}: {pattern.title}\n\n"

        # Problem signature
        prompt += "**Similar issues:**\n"
        for symptom in pattern.problem_signature['symptoms'][:3]:
            prompt += f"- {symptom}\n"

        # Solution approach
        prompt += "\n**Suggested approach:**\n"
        for step in pattern.approach['key_steps']:
            prompt += f"{step}\n"

        # Code anchors
        if pattern.code_anchors:
            prompt += "\n**Where to look:**\n"
            prompt += f"- Target: `{pattern.code_anchors['target_function']}`\n"
            prompt += f"- File pattern: `{pattern.code_anchors['file_path_hint']}`\n"

        # Quality signal (optional)
        prompt += f"\n**Quality:** Success rate {pattern.quality['resolve_rate']:.0%}, "
        prompt += f"Avg drift {pattern.quality['avg_drift']:.2f}\n\n"
        prompt += "---\n\n"

    prompt += "**Note:** These are suggestions, not requirements. "
    prompt += "Adapt them to your specific task.\n"

    return prompt
```

**Agent Promptæ„é€ ï¼š**

```python
def construct_agent_prompt_with_patterns(task, patterns):
    """æ„é€ åŒ…å«patternsçš„agent prompt"""

    base_prompt = f"""
You are a software engineering agent tasked with fixing bugs.

# Task
{task.problem_statement}

Repository: {task.repo}
Base commit: {task.base_commit}

{format_patterns_for_agent(patterns) if patterns else ""}

# Your Task
1. Understand the problem
2. Reproduce the bug
3. Implement a fix
4. Verify the fix works

Proceed step by step.
"""

    return base_prompt
```

### 7.2 Agent Execution with Patterns

```python
def agent_solve_with_patterns(task, patterns, monitor=None):
    """Agentè§£å†³taskï¼Œå¸¦pattern guidance"""

    # Construct prompt with patterns
    prompt = construct_agent_prompt_with_patterns(task, patterns)

    # Initialize agent
    agent = CodingAgent(
        system_prompt=prompt,
        monitor=monitor,  # Q1 guardï¼ˆå¯é€‰ï¼‰
    )

    # Execute
    result = agent.execute(
        max_actions=100,
        timeout=600,  # 10åˆ†é’Ÿ
    )

    # Log pattern usage
    log_pattern_usage(task, patterns, result)

    return result

def log_pattern_usage(task, patterns, result):
    """è®°å½•pattern usage"""

    for pattern in patterns:
        usage_log.append({
            'pattern_id': pattern.id,
            'task_id': task.instance_id,
            'was_retrieved': True,
            'was_applied': check_if_applied(pattern, result),  # åˆ†ææ˜¯å¦çœŸçš„ç”¨äº†
            'outcome': 'success' if result.resolved else 'failure',
            'drift_delta': result.drift_rate - baseline_drift_rate,
        })
```

---

## 8. Step 5: è¯„ä¼°ä¸å¯¹æ¯”

### 8.1 å®éªŒè®¾è®¡

```python
# ===== Baseline: æ— pattern =====
baseline_results = []
for task in test_tasks:
    result = agent.solve(task, patterns=None)
    baseline_results.append({
        'task_id': task.instance_id,
        'resolved': evaluate(task, result),
        'actions': len(result.actions),
        'drift_rate': compute_drift(result),
        'cost': estimate_cost(result),
    })

baseline_resolve_rate = np.mean([r['resolved'] for r in baseline_results])
baseline_drift_rate = np.mean([r['drift_rate'] for r in baseline_results])
baseline_cost = np.mean([r['cost'] for r in baseline_results])

# ===== Q2 (Semantic Only): Stage-1 only =====
q2_semantic_results = []
for task in test_tasks:
    patterns = stage1_semantic_recall(task, pattern_store, top_k=3)
    result = agent.solve(task, patterns=patterns)
    q2_semantic_results.append({
        'task_id': task.instance_id,
        'resolved': evaluate(task, result),
        'actions': len(result.actions),
        'drift_rate': compute_drift(result),
        'cost': estimate_cost(result),
    })

# ===== Q2 (Full): Stage-1 + Stage-2 =====
q2_full_results = []
for task in test_tasks:
    candidates = stage1_semantic_recall(task, pattern_store, top_k=20)
    patterns = stage2_ml_ranking(task, candidates, ml_ranker)
    result = agent.solve(task, patterns=patterns)
    q2_full_results.append({
        'task_id': task.instance_id,
        'resolved': evaluate(task, result),
        'actions': len(result.actions),
        'drift_rate': compute_drift(result),
        'cost': estimate_cost(result),
        'patterns_used': patterns,
    })
```

### 8.2 è¯„ä¼°æŒ‡æ ‡

```python
def compute_metrics(results):
    """è®¡ç®—æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡"""

    metrics = {}

    # ===== Primary Metric: Resolve Rate =====
    metrics['resolve_rate'] = np.mean([r['resolved'] for r in results])

    # ===== Secondary Metrics =====

    # 1. Pattern Reuse Rateï¼ˆQ2ç‰¹æœ‰ï¼‰
    if 'patterns_used' in results[0]:
        metrics['pattern_reuse_rate'] = np.mean([
            len(r['patterns_used']) > 0 for r in results
        ])

    # 2. Drift Rateï¼ˆæ¥è‡ªQ1ï¼‰
    metrics['drift_rate'] = np.mean([r['drift_rate'] for r in results])

    # 3. Cost
    metrics['avg_cost'] = np.mean([r['cost'] for r in results])
    metrics['avg_actions'] = np.mean([r['actions'] for r in results])

    # 4. Scope Metricsï¼ˆæ¥è‡ªQ1ï¼‰
    metrics['avg_scope_precision'] = np.mean([
        compute_scope_precision(r) for r in results
    ])

    return metrics
```

### 8.3 Statistical Significance

```python
from scipy.stats import chi2_contingency, ttest_ind

def test_significance(baseline_results, q2_results):
    """ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ"""

    # 1. Resolve Rate (Chi-square test)
    baseline_resolved = sum(r['resolved'] for r in baseline_results)
    baseline_total = len(baseline_results)
    q2_resolved = sum(r['resolved'] for r in q2_results)
    q2_total = len(q2_results)

    contingency = [
        [baseline_resolved, baseline_total - baseline_resolved],
        [q2_resolved, q2_total - q2_resolved],
    ]

    chi2, p_value_resolve = chi2_contingency(contingency)

    # 2. Drift Rate (T-test)
    baseline_drift = [r['drift_rate'] for r in baseline_results]
    q2_drift = [r['drift_rate'] for r in q2_results]

    t_stat, p_value_drift = ttest_ind(baseline_drift, q2_drift)

    print("Statistical Significance:")
    print(f"  Resolve Rate: p = {p_value_resolve:.4f} {'âœ… Significant' if p_value_resolve < 0.05 else 'âŒ Not significant'}")
    print(f"  Drift Rate: p = {p_value_drift:.4f} {'âœ… Significant' if p_value_drift < 0.05 else 'âŒ Not significant'}")
```

### 8.4 Ablation Studies

```python
# ===== Ablation 1: Pattern Quality Filtering =====
# åªç”¨high-quality patterns (avg_drift < 0.15)
high_quality_patterns = [p for p in pattern_store if p.quality['avg_drift'] < 0.15]
results_high_quality = run_experiment(test_tasks, high_quality_patterns)

# å¯¹æ¯”ï¼šæ˜¯å¦é«˜è´¨é‡patternæ›´æœ‰ç”¨ï¼Ÿ
print(f"All patterns: {metrics_baseline['resolve_rate']:.1%}")
print(f"High-quality only: {metrics_high_quality['resolve_rate']:.1%}")

# ===== Ablation 2: Number of Patterns =====
# Top-1 vs Top-3 vs Top-5
for k in [1, 3, 5]:
    results_k = run_experiment(test_tasks, pattern_store, top_k=k)
    print(f"Top-{k}: {compute_metrics(results_k)['resolve_rate']:.1%}")

# ===== Ablation 3: Stage-2 ML Ranker =====
# å¯¹æ¯”semantic-only vs with ML ranker
results_semantic_only = run_with_semantic_only(test_tasks, pattern_store)
results_with_ranker = run_with_ml_ranker(test_tasks, pattern_store, ranker)

print(f"Semantic only: {metrics_semantic['resolve_rate']:.1%}")
print(f"With ML ranker: {metrics_ranker['resolve_rate']:.1%}")
```

---

## 9. Q1ä¸Q2çš„ååŒ

### Q1å¦‚ä½•æœåŠ¡Q2ï¼Ÿ

```
Q1çš„ä¸‰ä¸ªä½œç”¨ï¼š

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ä½œç”¨1: Pattern Quality Labeling               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Q1è®¡ç®—æ¯ä¸ªsolutionçš„drift_rate                â”‚
â”‚    â†’ Low drift (< 0.15) = High-quality pattern  â”‚
â”‚    â†’ High drift (> 0.35) = Low-quality pattern  â”‚
â”‚                                                 â”‚
â”‚  ç”¨äºï¼š                                          â”‚
â”‚    - Pattern extractionæ—¶è¿‡æ»¤                   â”‚
â”‚    - ML rankerçš„quality feature                â”‚
â”‚    - Pattern storeçš„quality index              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ä½œç”¨2: Pattern Usage Evaluation               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  è¿è¡ŒQ2æ—¶ï¼ŒQ1ç»§ç»­ç›‘æ§ï¼š                         â”‚
â”‚    - Baseline drift (æ— pattern)                 â”‚
â”‚    - Q2 drift (æœ‰pattern)                       â”‚
â”‚    - Drift delta = Q2_drift - Baseline_drift   â”‚
â”‚                                                 â”‚
â”‚  ç”¨äºï¼š                                          â”‚
â”‚    - è¯„ä¼°patternæ˜¯å¦çœŸçš„é™ä½äº†drift              â”‚
â”‚    - æ›´æ–°pattern quality stats                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ä½œç”¨3: Feature Engineering for ML Ranker      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Q1æä¾›çš„featuresï¼š                             â”‚
â”‚    - pattern_avg_drift (pattern quality)        â”‚
â”‚    - pattern_scope_compactness (ç®€æ´åº¦)         â”‚
â”‚    - pattern_test_coverage (æµ‹è¯•è¦†ç›–)           â”‚
â”‚                                                 â”‚
â”‚  è¿™äº›featureså¸®åŠ©ML rankeré€‰æ‹©best patterns    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### å®Œæ•´çš„Q1+Q2 Pipeline

```python
# ===== Phase 1: è®­ç»ƒé˜¶æ®µï¼ˆæå–patterns with Q1ï¼‰ =====

training_results = []
for task in training_tasks:
    # è¿è¡Œagentï¼ˆæ— patternï¼‰
    result = agent.solve(task, patterns=None)

    # Q1è®¡ç®—drift metrics
    q1_guard = FourGuardMonitor(task)
    drift_metrics = q1_guard.compute_metrics(result.actions)

    # åªä¿ç•™high-quality solutions
    if result.resolved and drift_metrics['drift_rate'] < 0.2:
        # æå–pattern
        pattern = extract_pattern(task, result, drift_metrics)

        # âœ¨ Pattern qualityæ¥è‡ªQ1
        pattern.quality['avg_drift'] = drift_metrics['drift_rate']
        pattern.quality['scope_precision'] = drift_metrics['scope_precision']

        # å­˜å‚¨
        pattern_store.add(pattern)

        training_results.append({
            'task': task,
            'result': result,
            'drift_metrics': drift_metrics,
            'pattern': pattern,
        })

print(f"Extracted {len(pattern_store)} high-quality patterns")

# ===== Phase 2: æµ‹è¯•é˜¶æ®µï¼ˆQ2 with Q1 monitoringï¼‰ =====

baseline_results = []
q2_results = []

for task in test_tasks:
    # --- Baseline: æ— pattern ---
    result_baseline = agent.solve(task, patterns=None)
    q1_guard_baseline = FourGuardMonitor(task)
    drift_baseline = q1_guard_baseline.compute_metrics(result_baseline.actions)

    baseline_results.append({
        'task_id': task.instance_id,
        'resolved': evaluate(task, result_baseline),
        'drift_rate': drift_baseline['drift_rate'],  # Q1 metric
    })

    # --- Q2: æœ‰pattern ---
    # Retrieve patterns
    candidates = stage1_semantic_recall(task, pattern_store, top_k=20)

    # ML ranking (ä½¿ç”¨Q1çš„quality features)
    patterns = stage2_ml_ranking(task, candidates, ml_ranker)
    # ML rankerä½¿ç”¨çš„featuresåŒ…æ‹¬ï¼š
    # - pattern.quality['avg_drift'] â† æ¥è‡ªQ1
    # - pattern.quality['scope_precision'] â† æ¥è‡ªQ1

    # Agent solve with patterns
    result_q2 = agent.solve(task, patterns=patterns)
    q1_guard_q2 = FourGuardMonitor(task)
    drift_q2 = q1_guard_q2.compute_metrics(result_q2.actions)

    q2_results.append({
        'task_id': task.instance_id,
        'resolved': evaluate(task, result_q2),
        'drift_rate': drift_q2['drift_rate'],  # Q1 metric
        'drift_delta': drift_q2['drift_rate'] - drift_baseline['drift_rate'],
        'patterns_used': patterns,
    })

# ===== Phase 3: åˆ†æï¼ˆQ1+Q2è”åˆæŒ‡æ ‡ï¼‰ =====

print("Results:")
print(f"Baseline: Resolve={np.mean([r['resolved'] for r in baseline_results]):.1%}, "
      f"Drift={np.mean([r['drift_rate'] for r in baseline_results]):.2f}")

print(f"Q2:       Resolve={np.mean([r['resolved'] for r in q2_results]):.1%}, "
      f"Drift={np.mean([r['drift_rate'] for r in q2_results]):.2f}")

# Pattern effectivenessï¼ˆQ1å¸®åŠ©åˆ†æï¼‰
for result in q2_results:
    if result['resolved'] and result['drift_delta'] < -0.1:
        print(f"âœ… Task {result['task_id']}: Pattern helped (drift â†“ {abs(result['drift_delta']):.2f})")
    elif result['resolved'] and result['drift_delta'] > 0.1:
        print(f"âš ï¸ Task {result['task_id']}: Pattern didn't help (drift â†‘ {result['drift_delta']:.2f})")
```

---

## 10. å®Œæ•´æµç¨‹å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Training Phase: Pattern Extraction (ä½¿ç”¨train.jsonl)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 1: æ”¶é›†Successful Solutions (with Q1 monitoring)               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                       â”‚
â”‚  For task in train.jsonl[:1000]:                                     â”‚
â”‚    â”œâ”€â†’ Agent.solve(task, patterns=None)  # æ— pattern                â”‚
â”‚    â”‚                                                                  â”‚
â”‚    â”œâ”€â†’ Q1 è®¡ç®—drift metrics:                                        â”‚
â”‚    â”‚     â€¢ drift_rate                                                â”‚
â”‚    â”‚     â€¢ scope_precision/recall                                    â”‚
â”‚    â”‚     â€¢ plan/test violations                                      â”‚
â”‚    â”‚                                                                  â”‚
â”‚    â””â”€â†’ ç­›é€‰: resolved=True AND drift_rate < 0.2                     â”‚
â”‚          â†’ ä¿ç•™ä¸ºhigh-quality solution                               â”‚
â”‚                                                                       â”‚
â”‚  Output: ~300-400 successful, low-drift solutions                    â”‚
â”‚                                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 2: Pattern Extraction & Decontextualization                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                       â”‚
â”‚  For each successful solution:                                       â”‚
â”‚    â”œâ”€â†’ Extract problem signature                                    â”‚
â”‚    â”‚     â€¢ symptoms, bug_type, test_names, error_message            â”‚
â”‚    â”‚                                                                  â”‚
â”‚    â”œâ”€â†’ Summarize approach (å»è¯­å¢ƒåŒ–)                                â”‚
â”‚    â”‚     â€¢ LLM: ä»å…·ä½“actionsæ€»ç»“ä¸ºhigh-level steps                 â”‚
â”‚    â”‚     â€¢ Example: "Add missing parameter to constructor"          â”‚
â”‚    â”‚                                                                  â”‚
â”‚    â”œâ”€â†’ Extract code anchors                                         â”‚
â”‚    â”‚     â€¢ target_function, file_path_hint, code_pattern            â”‚
â”‚    â”‚                                                                  â”‚
â”‚    â””â”€â†’ Attach quality signals (æ¥è‡ªQ1!)                             â”‚
â”‚          â€¢ avg_drift = Q1è®¡ç®—çš„drift_rate                           â”‚
â”‚          â€¢ scope_precision = Q1çš„scope analysis                     â”‚
â”‚          â€¢ resolve_rate = 1.0 (åˆå§‹å€¼)                              â”‚
â”‚                                                                       â”‚
â”‚  Output: Pattern cards with Q1 quality labels                        â”‚
â”‚                                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 3: Pattern Merging & Indexing                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                       â”‚
â”‚  â”œâ”€â†’ Cluster similar patterns (semantic similarity)                 â”‚
â”‚  â”‚                                                                    â”‚
â”‚  â”œâ”€â†’ Merge patterns in each cluster                                 â”‚
â”‚  â”‚     â€¢ Combine problem signatures                                  â”‚
â”‚  â”‚     â€¢ Merge solution steps                                        â”‚
â”‚  â”‚     â€¢ Aggregate quality stats (Q1 metrics)                        â”‚
â”‚  â”‚                                                                    â”‚
â”‚  â”œâ”€â†’ Build vector index (for semantic search)                       â”‚
â”‚  â”‚     â€¢ Embed: title + symptoms + approach                          â”‚
â”‚  â”‚     â€¢ FAISS index                                                 â”‚
â”‚  â”‚                                                                    â”‚
â”‚  â””â”€â†’ Build metadata DB (for quality filtering)                      â”‚
â”‚        â€¢ Store: quality stats, source tasks, repo family             â”‚
â”‚                                                                       â”‚
â”‚  Output: Pattern Store (200-300 merged patterns)                     â”‚
â”‚                                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Testing Phase: Pattern Retrieval & Application (verified.jsonl)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 4: Two-Stage Pattern Retrieval                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                       â”‚
â”‚  New Task from verified.jsonl                                        â”‚
â”‚         â”‚                                                             â”‚
â”‚         â”œâ”€â†’ ã€Stage 1: Semantic Recallã€‘                            â”‚
â”‚         â”‚     â€¢ Embed task.problem_statement                         â”‚
â”‚         â”‚     â€¢ Vector search in pattern store                       â”‚
â”‚         â”‚     â€¢ Retrieve top-20 candidates (high recall)             â”‚
â”‚         â”‚                                                             â”‚
â”‚         â”‚   Candidates: [Pattern1, Pattern2, ..., Pattern20]         â”‚
â”‚         â”‚                                                             â”‚
â”‚         â””â”€â†’ ã€Stage 2: ML Rankingã€‘                                 â”‚
â”‚               â€¢ Extract 20+ features for each candidate:             â”‚
â”‚               â”‚  - Task features (length, difficulty, ...)           â”‚
â”‚               â”‚  - Pattern features (avg_drift â† Q1, success_rate)   â”‚
â”‚               â”‚  - Interaction features (keyword overlap, ...)       â”‚
â”‚               â”‚                                                       â”‚
â”‚               â€¢ XGBoost ranker scores each candidate                 â”‚
â”‚               â€¢ Sort by score, return top-3                          â”‚
â”‚                                                                       â”‚
â”‚  Output: Top-3 most relevant patterns                                â”‚
â”‚                                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 5: Pattern Application to Agent                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚  â”‚  Agent Prompt Construction                 â”‚                     â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                     â”‚
â”‚  â”‚  # Task                                     â”‚                     â”‚
â”‚  â”‚  {task.problem_statement}                   â”‚                     â”‚
â”‚  â”‚                                              â”‚                     â”‚
â”‚  â”‚  ## Relevant Solution Patterns              â”‚                     â”‚
â”‚  â”‚                                              â”‚                     â”‚
â”‚  â”‚  ### Pattern 1: Fix missing field in model  â”‚                     â”‚
â”‚  â”‚  **Suggested approach:**                    â”‚                     â”‚
â”‚  â”‚  1. Reproduce by running failing test       â”‚                     â”‚
â”‚  â”‚  2. Identify root cause in model __init__   â”‚                     â”‚
â”‚  â”‚  3. Add missing parameter                   â”‚                     â”‚
â”‚  â”‚  4. Verify with tests                       â”‚                     â”‚
â”‚  â”‚                                              â”‚                     â”‚
â”‚  â”‚  **Where to look:** models.py, __init__()   â”‚                     â”‚
â”‚  â”‚  **Quality:** 85% success, 0.12 avg drift   â”‚                     â”‚
â”‚  â”‚                                              â”‚                     â”‚
â”‚  â”‚  [Pattern 2 and 3 similar format]           â”‚                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                          â†“                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚  â”‚  Agent Execution (with Q1 monitoring)      â”‚                     â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                     â”‚
â”‚  â”‚  agent = CodingAgent(prompt)                â”‚                     â”‚
â”‚  â”‚  q1_guard = FourGuardMonitor(task)  # Q1   â”‚                     â”‚
â”‚  â”‚                                              â”‚                     â”‚
â”‚  â”‚  result = agent.execute(monitor=q1_guard)   â”‚                     â”‚
â”‚  â”‚    â”œâ”€â†’ Agent reads patterns from prompt    â”‚                     â”‚
â”‚  â”‚    â”œâ”€â†’ Agent adapts approach to this task  â”‚                     â”‚
â”‚  â”‚    â”œâ”€â†’ Q1 monitors each action (drift)     â”‚                     â”‚
â”‚  â”‚    â””â”€â†’ Agent generates patch                â”‚                     â”‚
â”‚  â”‚                                              â”‚                     â”‚
â”‚  â”‚  Output: result with patch + drift_metrics  â”‚                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 6: Evaluation & Comparison                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Baseline    â”‚   â”‚  Q2 (Semantic) â”‚   â”‚  Q2 (Full)         â”‚   â”‚
â”‚  â”‚  No patterns â”‚   â”‚  Stage-1 only  â”‚   â”‚  Stage-1 + Stage-2 â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚         â”‚                    â”‚                      â”‚               â”‚
â”‚         â†“                    â†“                      â†“               â”‚
â”‚  Run official evaluator (predictions.jsonl)                         â”‚
â”‚         â”‚                    â”‚                      â”‚               â”‚
â”‚         â†“                    â†“                      â†“               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚  Primary Metric: Resolve Rate                â”‚                  â”‚
â”‚  â”‚  Baseline:  25%                               â”‚                  â”‚
â”‚  â”‚  Q2 Semantic: 28%  (+3%)                     â”‚                  â”‚
â”‚  â”‚  Q2 Full:   32%  (+7%) âœ…                    â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                          â†“                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚  Secondary Metrics (from Q1)                 â”‚                  â”‚
â”‚  â”‚  â€¢ Drift Rate: 35% â†’ 18% (-17%) âœ…          â”‚                  â”‚
â”‚  â”‚  â€¢ Pattern Reuse: 78% tasks used patterns    â”‚                  â”‚
â”‚  â”‚  â€¢ Cost: $0.50 â†’ $0.32 per task (-36%)      â”‚                  â”‚
â”‚  â”‚  â€¢ Scope Precision: 0.60 â†’ 0.82 (+0.22)     â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                          â†“                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚  Ablation Studies                            â”‚                  â”‚
â”‚  â”‚  â€¢ High-quality patterns only: +10%          â”‚                  â”‚
â”‚  â”‚  â€¢ Top-1 vs Top-3 vs Top-5: Top-3 best      â”‚                  â”‚
â”‚  â”‚  â€¢ ML ranker vs semantic: +4% with ranker   â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Pattern Store Update (Continuous Learning)                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                       â”‚
â”‚  For each pattern used:                                              â”‚
â”‚    â”œâ”€â†’ Update usage count                                           â”‚
â”‚    â”œâ”€â†’ Update success/failure count                                 â”‚
â”‚    â”œâ”€â†’ Update avg_drift (weighted average with Q1 data)             â”‚
â”‚    â””â”€â†’ Update resolve_rate                                          â”‚
â”‚                                                                       â”‚
â”‚  Pattern quality evolution:                                          â”‚
â”‚    â€¢ Good patterns: success â†‘, drift â†“ â†’ used more often            â”‚
â”‚    â€¢ Bad patterns: failure â†‘, drift â†‘ â†’ gradually deprecated        â”‚
â”‚                                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 11. å•è¡Œæ•°æ®å®Œæ•´ç¤ºä¾‹

### ä½¿ç”¨verified.jsonlç¬¬0è¡Œæ•°æ®

```python
# ===== Task =====
task = load_task("verified.jsonl", index=0)

# Task details:
# instance_id: "astropy__astropy-12907"
# repo: "astropy/astropy"
# difficulty: "15 min - 1 hour"
# problem_statement: "Modeling's `separability_matrix` does not compute
#                     separability correctly for nested CompoundModels..."
# fail_to_pass: ["astropy/modeling/tests/test_separable.py::test_separable"]
```

### Phase 1: Trainingé˜¶æ®µï¼ˆå‡è®¾å·²å®Œæˆï¼‰

**ä»è®­ç»ƒæ•°æ®ä¸­æå–äº†ä¸€ä¸ªç›¸å…³çš„patternï¼š**

```python
pattern_123 = PatternCard(
    id="pattern_123",
    title="Fix CompoundModel broadcasting issue",

    problem_signature={
        'symptoms': ['separability_matrix', 'CompoundModel', 'broadcasting', 'nested models'],
        'bug_type': 'LogicError',
        'test_names': ['test_separable', 'test_compound'],
        'error_message': 'does not compute separability correctly',
    },

    approach={
        'key_steps': [
            "1. Reproduce the bug by running the failing test",
            "2. Examine how separability_matrix handles CompoundModel recursion",
            "3. Check if nested models are properly merged in the matrix computation",
            "4. Fix the matrix merging logic to handle nested cases",
            "5. Verify with original test and add edge case tests",
        ],
        'files_to_check': ['astropy/modeling/separable.py'],
        'common_fixes': ['Add recursive handling for nested CompoundModels'],
    },

    code_anchors={
        'target_function': '_compute_separability_matrix',
        'file_path_hint': 'astropy/modeling/separable.py',
        'code_pattern': 'if isinstance(transform, CompoundModel):',
    },

    quality={
        'success_count': 12,
        'failure_count': 2,
        'resolve_rate': 0.857,  # 12/14
        'avg_drift': 0.14,      # âœ¨ æ¥è‡ªQ1ï¼low drift = high quality
        'avg_actions': 18,
        'scope_precision': 0.92,  # âœ¨ æ¥è‡ªQ1ï¼
    },

    source_tasks=['astropy__astropy-10000', 'astropy__astropy-10500', ...],
    repo_family='astropy',
    difficulty='15 min - 1 hour',
)
```

### Phase 2: Testingé˜¶æ®µï¼ˆæ–°ä»»åŠ¡ï¼‰

**Step 1: Retrieve Patterns**

```python
# Stage-1: Semantic search
query = """
Problem: Modeling's separability_matrix does not compute separability correctly for nested CompoundModels
Error: test_separable
Tests: astropy/modeling/tests/test_separable.py::test_separable
"""

candidates = pattern_store.search(query, top_k=20)
# Returns 20 candidates, including pattern_123

# Stage-2: ML Ranking
features_123 = extract_ranking_features(task, pattern_123)
# {
#   'task_length': 1246,
#   'task_difficulty_bucket': 2,  # "15 min - 1 hour"
#   'pattern_resolve_rate': 0.857,
#   'pattern_avg_drift': 0.14,  # âœ¨ Q1 feature!
#   'keyword_jaccard': 0.67,  # High overlap: separability_matrix, CompoundModel
#   'bug_type_match': 1,  # Both LogicError
#   'repo_family_match': 1,  # Both astropy
#   'semantic_similarity': 0.89,  # Very similar
#   ...
# }

# ML ranker scores all 20 candidates
ranker_scores = ml_ranker.predict([features_1, features_2, ..., features_123, ...])
# pattern_123 scores highest (0.94)

top_3_patterns = [pattern_123, pattern_456, pattern_789]
```

**Step 2: Agent Prompt with Patterns**

```markdown
You are a software engineering agent tasked with fixing bugs.

# Task
Modeling's `separability_matrix` does not compute separability correctly for nested CompoundModels.

The issue occurs when CompoundModels are nested inside other CompoundModels...

Repository: astropy/astropy
Base commit: 3b55e89

## Relevant Solution Patterns

### Pattern 1: Fix CompoundModel broadcasting issue

**Similar issues:**
- separability_matrix computation errors
- Nested CompoundModel handling
- Matrix broadcasting problems

**Suggested approach:**
1. Reproduce the bug by running the failing test
2. Examine how separability_matrix handles CompoundModel recursion
3. Check if nested models are properly merged in the matrix computation
4. Fix the matrix merging logic to handle nested cases
5. Verify with original test and add edge case tests

**Where to look:**
- Target: `_compute_separability_matrix`
- File pattern: `astropy/modeling/separable.py`
- Code pattern: `if isinstance(transform, CompoundModel):`

**Quality:** Success rate 86%, Avg drift 0.14

---

[Pattern 2 and 3 omitted for brevity]

**Note:** These are suggestions, not requirements. Adapt them to your specific task.

# Your Task
1. Understand the problem
2. Reproduce the bug
3. Implement a fix
4. Verify the fix works

Proceed step by step.
```

**Step 3: Agent Execution (with Q1 monitoring)**

```python
# Initialize agent with pattern-enhanced prompt
agent = CodingAgent(prompt=prompt_with_patterns)

# Initialize Q1 guard
q1_guard = FourGuardMonitor(task)

# Execute
result = agent.execute(monitor=q1_guard)

# Agent's actions (influenced by pattern):
# 1. read_file("astropy/modeling/separable.py")  # From pattern guidance
# 2. search_function("_compute_separability_matrix")  # From pattern
# 3. run_test("test_separable")  # Reproduce
# 4. read_file("astropy/modeling/separable.py", focus="_compute_separability_matrix")
# 5. edit_file("astropy/modeling/separable.py", add recursive handling)
# 6. run_test("test_separable")  # Verify
# 7. run_test("test_compound")  # Edge case
# 8. submit()

# Q1 monitoring result:
drift_metrics = {
    'drift_rate': 0.09,  # Very low! Pattern helped
    'scope_precision': 1.0,  # Only edited separable.py
    'scope_recall': 1.0,  # Covered all necessary files
    'actions': 8,  # Efficient
}

# Agent's patch:
patch = """
diff --git a/astropy/modeling/separable.py b/astropy/modeling/separable.py
--- a/astropy/modeling/separable.py
+++ b/astropy/modeling/separable.py
@@ -140,7 +140,9 @@ def _compute_separability_matrix(transform, input_shape, output_shape):
                 matrix = np.array([[True]])
             matrices.append(matrix)
         matrix = block_diagonal_matrix(matrices)
-
+        if isinstance(transform, CompoundModel):
+            matrix = merge_separability_matrices(matrix, transform)
+
     return matrix
"""
```

**Step 4: Evaluation**

```python
# Run official evaluator
evaluation = evaluate_with_official_evaluator(task, result.patch)

# Result:
evaluation = {
    'resolved': True,  # âœ… FAIL_TO_PASS passed
    'f2p_passed': 1,
    'f2p_total': 1,
    'p2p_passed': 100,  # All regression tests passed
    'p2p_total': 100,
}

# Q1 analysis:
scope_analysis = {
    'gold_files': {'astropy/modeling/separable.py'},
    'agent_files': {'astropy/modeling/separable.py'},
    'scope_precision': 1.0,  # Perfect
    'scope_recall': 1.0,     # Perfect
    'extra_files': [],
    'missed_files': [],
}

# Pattern usage log:
usage_log = {
    'pattern_id': 'pattern_123',
    'task_id': 'astropy__astropy-12907',
    'retrieved_rank': 1,  # Top-1
    'was_applied': True,  # Agent followed the guidance
    'outcome': 'success',
    'drift_delta': -0.26,  # Baseline drift=0.35, Q2 drift=0.09
    'cost_reduction': 0.40,  # 40% fewer actions
}

# Update pattern store:
pattern_123.quality['success_count'] += 1  # 12 â†’ 13
pattern_123.quality['avg_drift'] = (
    (pattern_123.quality['avg_drift'] * 12 + 0.09) / 13
)  # 0.14 â†’ 0.13 (improved!)
```

**Summary of this example:**

| Metric | Baseline (no pattern) | Q2 (with pattern) | Improvement |
|--------|----------------------|-------------------|-------------|
| **Resolved** | â“ Unknown (not run) | âœ… Yes | N/A |
| **Actions** | ~20 (typical) | 8 | -60% |
| **Drift Rate** | ~0.35 (baseline avg) | 0.09 | -74% âœ… |
| **Scope Precision** | ~0.70 (baseline avg) | 1.0 | +43% âœ… |
| **Cost** | $0.50 | $0.20 | -60% âœ… |

**Key Insights:**
1. **Pattern matching worked**: pattern_123 had high similarity (0.89)
2. **Agent followed guidance**: Edited exactly the right file and function
3. **Q1 confirms quality**: Very low drift (0.09) = high-quality solution
4. **Efficiency gain**: 60% fewer actions, 60% lower cost
5. **Pattern quality improved**: Success with this task â†’ avg_drift 0.14 â†’ 0.13

---

## 12. æ€»ç»“ï¼šQ2 vs Q1 çš„å…³ç³»

### Q2 çš„æ ¸å¿ƒä»·å€¼

```
Q1: "å¦‚ä½•ç›‘æ§agentä¸èµ°åï¼Ÿ"
  â†’ Reactive: äº‹åå‘ç°é—®é¢˜

Q2: "å¦‚ä½•è®©agentä»ä¸€å¼€å§‹å°±ä¸èµ°åï¼Ÿ"
  â†’ Proactive: æä¾›æˆåŠŸçš„patternä½œä¸ºå‚è€ƒ

ç»„åˆæ•ˆæœï¼š
  Q1 + Q2 = "æä¾›å¥½çš„patternï¼ˆQ2ï¼‰ + ç¡®ä¿ä¸åç¦»ï¼ˆQ1ï¼‰"
```

### Paper Contributionæ€»ç»“

```markdown
## Main Contribution

**Title**: Learning to Retrieve: Pattern-Guided Code Agents with Quality-Aware Ranking

**Problem**: Code agents lack memoryâ€”each task solved from scratch, leading to:
- Low success rate (25% baseline)
- High process drift (35% actions off-track)
- High cost (unnecessary exploration)

**Solution**: Cross-session pattern learning with two-stage retrieval:
1. **Pattern Extraction**: Learn transferable strategies from successful solutions
2. **Quality-Aware Retrieval**:
   - Stage-1: Semantic similarity (recall)
   - Stage-2: ML ranking with drift-based quality features (precision)
3. **Pattern Application**: Inject relevant patterns to guide agent execution

**Key Innovation**: Quality estimation using drift metrics (Q1)
- Low-drift solutions â†’ high-quality patterns
- Pattern ranking uses drift as quality signal
- Continuous quality tracking during pattern usage

**Results** (SWE-bench Verified):
- Resolve rate: 25% â†’ 32% (+7 points)
- Drift rate: 35% â†’ 18% (-17 points)
- Cost: -36% fewer actions
- Pattern reuse: 78% of tasks benefited from patterns

**Contributions**:
1. Pattern extraction framework with decontextualization
2. Two-stage retrieval with 20+ features (including drift-based quality)
3. First work to combine process monitoring (Q1) with pattern learning (Q2)
```

---

## Next Steps

**Week 1-2: Pattern Extraction**
- Run baseline agent on 1000 training tasks
- Extract 300-400 high-quality patterns (with Q1 drift labels)
- Build pattern store (vector index + metadata DB)

**Week 3-4: Retrieval Implementation**
- Implement Stage-1 semantic search
- Train ML ranker with 20+ features
- Test on 50 validation tasks

**Week 5-6: Full Evaluation**
- Run on all 500 verified tasks
- Compare: Baseline vs Q2-Semantic vs Q2-Full
- Ablation studies (quality filtering, top-k, ML ranker)

**Week 7-8: Analysis & Paper**
- Feature importance analysis
- Pattern effectiveness analysis
- Write paper

---

**Ready to implement? Let's start with pattern extraction from your Q1 demo data!** ğŸš€
