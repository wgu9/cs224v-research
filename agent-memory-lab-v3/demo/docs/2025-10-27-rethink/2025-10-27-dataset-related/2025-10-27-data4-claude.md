# SWE-bench: A comprehensive guide to the leading software engineering benchmark

SWE-bench is the premier benchmark for evaluating AI systems on real-world software engineering tasks, consisting of **2,294 GitHub issue-pull request pairs** from 12 Python repositories. Models must generate code patches that resolve actual bugs while passing both new tests and regression checks. Since its October 2023 release at ICLR 2024, the benchmark has become the de facto standard for autonomous software engineering evaluation, with resolution rates improving from 1.96% (Claude 2) to 65% (optimized agents) on validated subsets. The dataset's real-world complexity, Docker-based reproducibility, and rigorous two-part test validation make it uniquely challenging—and uniquely valuable for advancing AI-powered program repair.

## The data structure powers reproducible evaluation

SWE-bench stores each instance as a JSON object with 12 fields that enable complete reproduction of the software engineering task. Every instance includes the repository information, commit hashes for exact version control, problem descriptions, gold solution patches, and critically, two test categories: FAIL_TO_PASS tests that must transition from failing to passing, and PASS_TO_PASS tests that must remain passing to prevent regressions. The dataset is distributed via HuggingFace in Parquet format and can be loaded directly through the datasets library.

### Complete schema specification

The schema contains **core identification fields** (instance_id formatted as "repo_owner__repo_name-PR-number", and repo identifier), **code repository fields** (base_commit SHA-1 hash representing pre-fix state, environment_setup_commit for dependency installation, and version number for evaluation), **problem description fields** (problem_statement with issue details, hints_text containing issue comments, and created_at timestamp), **solution fields** (patch with the gold solution in unified diff format, and test_patch with test file changes), and **test validation fields** (FAIL_TO_PASS as JSON list of tests that must start failing and end passing, and PASS_TO_PASS as JSON list of tests that must remain passing throughout).

The field constraints are precise: commit hashes are exactly 40 characters (SHA-1), instance_ids range from 20-34 characters, patches range from 278 to 50,600 characters, problem statements from 52 to 24,400 characters, and test specifications are stored as JSON strings ranging from 2 to 62,900 characters. This structure ensures every instance is completely specified for automated evaluation.

### Real-world example 1: Feature addition with quiet mode

The **sqlfluff__sqlfluff-4764** instance demonstrates a feature request for quiet mode in the SQLFluff CLI tool. The problem statement describes users wanting less verbose output when running `sqlfluff fix` in pre-commit hooks, similar to how the black formatter operates. The base commit is a820c139ccbe6d1865d73c4a459945cd69899f8f, with environment setup at commit d19de0ecd16d298f9e3bfb91da122734c40c01e5, targeting version 1.4 and created on April 16, 2023.

The gold patch spans over 50,000 characters, modifying multiple files across the CLI commands module. Key changes include adding verbosity checks before printing status messages, introducing a --quiet flag that conflicts with --verbose, and updating the output formatter to respect the verbosity level. The solution demonstrates careful interface design where the formatter's verbosity property gates console output: `if formatter and formatter.verbosity >= 0: click.echo("Persisting Changes...")`.

The test validation requires two specific tests to pass: `test__cli__fix_multiple_errors_quiet_force` and `test__cli__fix_multiple_errors_quiet_no_force`. These FAIL_TO_PASS tests verify the quiet mode works correctly both with and without the force flag. The PASS_TO_PASS suite includes over 200 existing CLI tests that must remain passing, ensuring the new feature doesn't break existing functionality. The hints_text field is empty, indicating the issue description alone provided sufficient specification.

### Real-world example 2: Complex bug fix with template handling

Instance **sqlfluff__sqlfluff-2862** tackles a subtle bug where the fixer incorrectly adds newlines inside Jinja template blocks rather than at file end. The problem statement includes reproduction steps: create a file with `{% if true %} SELECT 1 + 1 {%- endif %}`, run `sqlfluff fix`, and observe the newline appearing before `{%- endif %}` instead of after it. Running the fix repeatedly continues adding newlines in the wrong location, demonstrating the bug's persistence.

The issue discussion in hints_text reveals deeper technical context: "There's a bug in JinjaTracer -- if a Jinja block is the final slice in the file (i.e. there's no final newline), that slice is missing from the output." This architectural insight guided the solution approach. The base commit is 447ecf862a4d2b977d0add9f444655357b9c4f1f, environment setup at 3d52e8270d82aeccf4c516d059a80a6947919aea, for SQLFluff version 0.10, created March 14, 2022.

The solution required extensive refactoring across multiple modules including template processing, rule analysis, and Jinja handling. The patch modifies how the JinjaTracer processes file slices, ensuring template blocks are properly included when determining file structure. This enables the L009 rule (files must end with trailing newline) to correctly identify the actual end of file rather than the end of the SQL within templates.

Three FAIL_TO_PASS tests validate the fix: `test__api__lint_string`, `test__templater_jinja_slice_file[{%`, and `test__templater_jinja_slice_file[{%-`. These tests specifically check template boundary handling. The PASS_TO_PASS suite again includes 200+ regression tests, ensuring the template processing changes don't impact other linting scenarios. The test_patch also updates expected error codes in existing tests, reflecting the corrected file boundary detection.

### Real-world example 3: Reference resolution in UPDATE statements

The **sqlfluff__sqlfluff-2336** instance addresses a false positive where rule L026 incorrectly flags valid column references in UPDATE statement subqueries. The problem statement provides a minimal reproduction case: an UPDATE statement with a correlated subquery where `another_table.id = my_table.id` references the UPDATE target table. SQLFluff 0.9.0 incorrectly reports "Reference 'my_table.id' refers to table/view not found in the FROM clause," even though this reference pattern is valid SQL syntax.

The base commit is 37a993f7ad841ab3035d1db5ce6525f2e5584fd5, environment setup at a5c4eae4e3e419fe95460c9afd9cf39a35a470c4, targeting version 0.8, created January 17, 2022. Unlike the previous examples, this instance has an empty hints_text field, indicating the issue description alone was sufficiently clear for resolution. The problem demonstrates a gap in the reference resolution logic's understanding of UPDATE statement scope rules.

The solution modifies the reference analysis system in `src/sqlfluff/core/rules/analysis/select.py` and related files, implementing proper scope handling for UPDATE statements. The changes enable the analyzer to recognize that subqueries within UPDATE statements can legitimately reference the UPDATE target table, even though that table doesn't appear in the subquery's FROM clause. The patch includes adding pragma: no cover comments to defensive code paths and restructuring the reference matching logic.

The test validation is particularly interesting here: the FAIL_TO_PASS suite includes 14 variations of `test_object_ref_matches_table` with different `possible_references` and `targets` combinations, testing various scenarios of table reference matching including schema qualification, partial matches, and false positives. Notably, the PASS_TO_PASS field is an empty list, indicating this instance created new tests rather than relying on existing regression checks. This pattern appears in instances that add entirely new test files to the codebase.

## Academic research leverages SWE-bench across diverse methodologies

Research papers employ SWE-bench primarily for **automated program repair evaluation**, testing whether systems can autonomously fix real bugs by generating patches that pass all tests. The benchmark has evolved into the definitive measure of AI systems' software engineering capabilities, appearing in over 100 published works since early 2024. Researchers investigate questions ranging from agent architecture effectiveness to the role of specialized tooling, with experimental setups typically involving either agentic approaches (autonomous multi-step systems) or procedural pipelines (fixed-phase workflows).

### Dominant research applications and scenarios

The most common use case is evaluating **agentic systems** like SWE-agent, which achieved 12.5% resolution on full SWE-bench using custom tools for file editing, bash execution, and codebase navigation. These agents make iterative decisions, often following ReAct (Reasoning + Acting) patterns where the model alternates between analyzing the problem and taking actions. Systems like Devin (13.86% on a 25% subset) and OpenHands demonstrate this architecture's practical application, typically allowing 10-45 minutes per task.

Conversely, **procedural approaches** like Agentless achieve competitive results (27.3% on Lite, 40.7% with Claude 3.5 Sonnet) through fixed workflows: hierarchical localization (repository → file → function → line), multiple patch generation with candidate ranking, and explicit repair phases. At just $0.34 per issue, this demonstrates that simpler pipelines can outperform complex agents while reducing costs by orders of magnitude. The architectural choice between agent autonomy and structured pipelines has become a central research question.

**Fault localization research** treats SWE-bench as ground truth for identifying buggy code. AutoCodeRover combines AST-based search with spectrum-based fault localization (SBFL), achieving 19-22% on Lite by progressively narrowing from project structure to specific functions. Typical localization metrics include file-level accuracy (17-26% for leading systems) and function-level accuracy (11-19%), revealing that pinpointing bugs remains a fundamental challenge even when solutions are achievable.

### Evaluation dimensions and reported metrics

Papers universally report **resolution rate** (% Resolved) as the primary metric—the percentage of instances where generated patches pass both FAIL_TO_PASS tests (proving the bug is fixed) and PASS_TO_PASS tests (proving no regressions). This binary success criterion is typically reported as **pass@1** (single attempt) or **pass@k** (best of k attempts), though pass@1 dominates recent publications as it better reflects user experience.

Secondary metrics provide deeper insight into system behavior. **Plausible patches** count patches that pass tests but may be incorrect (potentially exploiting weak tests), while **correct patches** require manual inspection to verify semantic equivalence with developer solutions. The ratio of correct to plausible patches reveals overfitting rates—SWE-bench+ found that 32.67% of successful SWE-Agent patches actually contained solution leakage, and 31.08% exploited weak test cases rather than truly solving the problem.

**Cost and efficiency metrics** have become critical as systems scale. Papers report average cost per issue (ranging from $0.34 for Agentless to several dollars for complex agents), total token consumption, wall-clock time per task, and API call counts. The SWE-Effi study specifically compared AutoCodeRover, OpenHands, and SWE-Agent under token budget constraints, revealing substantial efficiency variations. Time efficiency is particularly relevant for practical deployment, where even correct solutions that require 45+ minutes per issue may be economically unviable.

### Standard experimental protocols and controls

The **canonical evaluation protocol** provides models with only the issue description (problem_statement field) and repository at base_commit, explicitly withholding the solution patch and test cases. Models generate a patch in unified diff format, which the evaluation harness applies to the codebase before running the test suites in isolated Docker containers. Both FAIL_TO_PASS and PASS_TO_PASS test suites must pass completely for the instance to count as resolved—partial success doesn't count.

**Retrieval configurations** significantly impact results and must be reported. The **oracle retrieval** setting provides models with the exact set of files that need modification, representing an upper bound on performance that isolates patch generation quality from localization accuracy. The **unassisted setting** uses BM25 retrieval with various context windows (13K, 27K, 40K tokens) to retrieve potentially relevant files, testing end-to-end capability. Papers should report both settings when possible, as the gap between them reveals localization effectiveness.

**Baseline comparisons** typically include direct LLM prompting (zero-shot and few-shot), RAG baselines, and previous state-of-the-art systems. The original SWE-bench paper established that Claude 2 achieved only 1.96% with RAG retrieval, setting a low initial bar. Human performance data exists but is complex—the average developer fix time is 2.77 days, though this includes understanding, planning, and testing rather than pure coding time. Recent papers compare against the rapidly evolving state-of-the-art, which jumped from ~12% (mid-2024) to 30-40% (late 2024) on various subsets.

### Systematic use of dataset variants

Papers increasingly use **SWE-bench Verified** (500 human-validated instances) as the primary benchmark, as it supersedes the original test set according to OpenAI and Princeton authors. The validation process—93 experienced Python developers reviewing 1,699 samples—filtered out 68.3% of instances with underspecified problems (38.3%) or unfair unit tests (61.1%). Performance on Verified runs approximately 2x higher than on the full benchmark (GPT-4o: 33.2% vs. ~16%), but the quality assurance makes results more reliable.

**SWE-bench Lite** (300 instances) serves cost-constrained evaluation during development, with selection criteria filtering for self-contained bugs without images, external references, or multi-file edits. Papers use Lite for rapid iteration and ablation studies, then report final results on Verified. The temporal pattern is clear: early 2024 papers reported on Full, mid-2024 added Lite, and late 2024+ emphasize Verified as the gold standard. Papers should explain which variant is used and why.

**Emerging variants** address specific research questions. SWE-bench Multimodal (617 JavaScript instances with required visual elements) tests cross-language and visual understanding, with top systems achieving only 12% resolution. SWE-bench Multilingual (300 instances across 9 languages) and Multi-SWE-bench (1,632 instances across 7 languages) evaluate language-agnostic approaches. SWE-bench Live (1,319+ continuously updated instances) prevents contamination concerns by using post-October 2024 issues—top systems achieve just 19.25% on Live vs. 60%+ on Verified, suggesting significant repository-specific overfitting.

### Notable papers and their distinct contributions

The **foundational SWE-bench paper** (Jimenez et al., ICLR 2024) established the benchmark, evaluated Claude 2 (1.96%) and GPT-4, and introduced retrieval settings. It demonstrated that even state-of-the-art LLMs struggled with repository-level tasks, motivating the agentic research wave that followed.

**SWE-agent** (Yang et al., NeurIPS 2024) achieved 12.5% on full SWE-bench by introducing the Agent-Computer Interface (ACI) concept—custom tools designed specifically for code editing rather than generic bash commands. The key insight was that interface design matters as much as model capability, spawning research on optimal tool design for coding agents.

**Agentless** (Xia et al., ACM SIGSOFT 2024) challenged the agent paradigm by achieving 27.3% on Lite (later 40.7% with Claude 3.5 Sonnet) through a simple two-phase pipeline: hierarchical localization then repair with multiple candidate generation. The work demonstrated that engineering matters more than architectural complexity, achieving superior cost-performance trade-offs ($0.34/issue vs. several dollars for agents) and faster execution.

**AutoCodeRover** (Zhang et al., ISSTA 2024) integrated program analysis techniques, using AST-based code search and spectrum-based fault localization (SBFL) to achieve 19-22% on Lite. The system demonstrates how classic software engineering techniques enhance LLM-based repair, performing iterative context retrieval to progressively refine understanding.

**SWE-bench+** (Aleithan et al., 2024) provided critical benchmark analysis, manually screening patches to find that 32.67% contained solution leakage and 31.08% exploited weak tests. The work created a contamination-resistant post-cutoff dataset, revealing that SWE-Agent+GPT-4 performance dropped from 12.47% to 3.97% under rigorous conditions. This exposed systematic issues with training data contamination (94%+ of instances created before LLM training cutoffs) and motivated improved variants.

The **SWE-bench Verified release** (OpenAI, August 2024) responded to quality concerns through systematic human validation, with 93 professional developers screening samples. The resulting 500-instance subset became the recommended evaluation standard, with difficulty annotations enabling stratified analysis: 196 easy tasks (under 15 minutes), 45 hard tasks (over 1 hour), and a median in between.

## Evaluation methodology combines rigorous testing with practical tooling

SWE-bench evaluation centers on a **binary resolution metric**: an instance is solved when all FAIL_TO_PASS tests transition from failing to passing (proving the bug is fixed) AND all PASS_TO_PASS tests remain passing (proving no regressions introduced). This two-part validation, inspired by real-world pull request review, makes the benchmark significantly harder than single-criterion benchmarks. The evaluation infrastructure uses a three-layer Docker architecture to ensure complete reproducibility across different systems, with base images (~5GB), environment images (~100GB total), and instance-specific images (~2TB for full benchmark).

### The resolution criterion and its implications

The **FAIL_TO_PASS test suite** consists of tests that fail on the base commit (pre-fix state) but should pass after applying the correct patch. These tests are extracted from the actual pull request that fixed the issue, typically from new test files or modified test methods. For instance, the sqlfluff-4764 example requires two specific tests to pass: `test__cli__fix_multiple_errors_quiet_force` and `test__cli__fix_multiple_errors_quiet_no_force`, both of which verify the new quiet mode functionality works correctly.

The **PASS_TO_PASS test suite** includes all tests that passed before the fix and must continue passing afterward, typically numbering in the hundreds per instance. These regression tests come from existing test files that weren't modified by the fix PR. A patch that fixes the bug but breaks even one regression test fails the instance completely. This strict criterion reflects real-world development where breaking existing functionality is unacceptable, even if the primary bug is fixed.

The dual requirement creates interesting failure modes. Models can generate "plausible patches" that pass FAIL_TO_PASS tests (appearing to fix the bug) but fail PASS_TO_PASS tests (introducing regressions). Alternatively, overly conservative patches might maintain all regressions but fail to actually fix the bug. The SWE-bench+ analysis revealed that 31.08% of apparently successful patches exploited weak tests—they passed the test suite but didn't correctly implement the intended solution, highlighting evaluation challenges even with two-part validation.

### Docker-based evaluation architecture

The evaluation harness uses **layered Docker images** to balance reproducibility and efficiency. The **base image** (~5GB) contains common system packages and Python installations used across all evaluations. **Environment images** (~60 images, ~100GB total) include repository-specific dependencies for different versions, shared across instances from the same repository and version. **Instance images** (one per task, ~2TB total for full benchmark) contain the repository at the exact base_commit with instance-specific setup.

The **cache level parameter** controls storage-speed tradeoffs. Setting `--cache_level none` rebuilds everything for each evaluation (slowest, minimal storage), `base` caches only the base image (~5GB), `env` caches base and environment images (~100GB, recommended for most use cases), and `instance` caches all layers (~2TB, fastest but storage-intensive). Most researchers use env caching, which provides good performance while keeping storage requirements manageable.

Each evaluation runs in an **isolated Docker container** that clones the repository at base_commit, installs dependencies, applies the generated patch, and runs both test suites. The container captures all output including test results, error messages, and execution logs. Results are stored in JSON format with detailed per-instance breakdowns including whether the patch applied cleanly, which specific tests passed or failed, and any errors encountered during execution.

### Practical evaluation workflow and tooling

Running evaluations requires the **official harness** from the princeton-nlp/SWE-bench repository. After installing Docker and the swebench package, users create predictions in JSONL format with three required fields: instance_id (matching the dataset), model_name_or_path (tracking the system), and model_patch (the generated solution in git diff format). Each line is a separate JSON object representing one prediction.

A **basic evaluation command** for SWE-bench Lite looks like: `python -m swebench.harness.run_evaluation --dataset_name princeton-nlp/SWE-bench_Lite --predictions_path predictions.jsonl --max_workers 8 --run_id my_evaluation`. The max_workers parameter controls parallelism, with the recommended limit being `min(0.75 * CPU_cores, 24)` to avoid resource contention. Evaluation of Lite takes approximately 30-50 minutes on an 8-core machine with env caching, while the full 2,294-instance benchmark requires several hours.

**System requirements** include x86_64 architecture (arm64/M-series support is experimental), 120GB+ free disk space with env caching, 16GB+ RAM, and 8+ cores recommended. Storage can become the limiting factor—researchers should monitor disk usage with `docker system df` and regularly prune unused images with `docker system prune`. Docker networking must be properly configured, as containers need internet access to install dependencies during the build phase.

Results are written to `evaluation_results/<run_id>/` with three key files. **results.json** contains overall metrics including total instances, instances submitted, instances completed, instances resolved, and resolution rate. **instance_results.jsonl** provides per-instance details showing patch application status, resolved boolean, and breakdowns of which FAIL_TO_PASS and PASS_TO_PASS tests succeeded or failed. **run_logs/** contains individual container logs for debugging failures.

### Available frameworks and integration tools

**SWE-agent** is the official agentic framework, providing an agent architecture with custom tools for code editing and bash execution. The system introduces the Agent-Computer Interface (ACI) with specialized commands like `str_replace` for exact string replacement, `insert` for adding code, and `undo` for reverting changes. Usage involves running inference to generate patches (`python run.py --model_name gpt4 --per_instance_cost_limit 2.00`), which creates predictions that are then evaluated using the official harness. SWE-agent achieved 12.47% on full SWE-bench, demonstrating that interface design significantly impacts performance.

**mini-SWE-agent** distills the approach into ~100 lines of Python, achieving 65% on SWE-bench Verified despite minimal complexity. The system provides just two tools—bash execution and file editing—with Claude 3.5 Sonnet handling all decision-making. The key design principle is giving maximum control to the model rather than constraining it with complex scaffolding. This minimalist implementation has become a reference point for understanding core requirements versus architectural complexity.

**Anthropic's Claude implementation** (49% on Verified) emphasizes detailed tool descriptions and error-proof interfaces. The bash tool provides a persistent shell session across the interaction, while the edit tool uses exact string matching to avoid ambiguity errors. Tools return detailed error messages when operations fail, enabling the model to self-correct. The implementation demonstrates that careful tool design with clear semantics can achieve strong results without extensive scaffolding.

**Cloud evaluation services** reduce infrastructure burden. **Modal** provides serverless evaluation—install the modal package, authenticate with `modal setup`, then run evaluations with `--modal true` flag for automatic parallelization across cloud resources. **sb-cli** (SWE-bench CLI tool) enables submitting predictions to managed evaluation infrastructure, particularly useful for the private test sets of Multimodal and other variants. These services handle Docker image management and result aggregation, though at the cost of reduced visibility into the evaluation process.

### Standard protocols and best practices

The **iterative development workflow** starts with single-instance testing before scaling. Researchers first validate the evaluation setup works by running gold patches: `python -m swebench.harness.run_evaluation --predictions_path gold --instance_ids sympy__sympy-20590 --max_workers 1`. This confirms Docker configuration and harness installation are correct. Then test on 1-5 instances with the actual model to verify patch format and basic functionality.

Once confident, evaluate on **SWE-bench Lite** for rapid iteration during development. The 300-instance subset enables testing architectural changes and ablations within hours rather than days. When performance plateaus on Lite, move to **SWE-bench Verified** for final evaluation, as it represents the community standard and ensures results are comparable with published work. Report results on both Lite and Verified when possible, as the comparison reveals how well the approach handles the full difficulty distribution.

**Patch format requirements** are strict: use standard git diff unified format output, ensure patches are self-contained with sufficient context lines, and validate that patches apply cleanly with `git apply --check`. Common failures include incorrect file paths (use repository-relative paths), missing context lines (use `git diff -U3` or higher for more context), and malformed hunks (ensure line numbers and offsets are correct). The evaluation harness attempts to apply patches and marks instances as unresolved if application fails.

**Avoiding common pitfalls** requires awareness of several issues. Hidden tests mean agents can't see the actual test suite, requiring thorough self-testing and verification before submission. Environment setup can fail if Docker isn't properly configured—check build logs in `logs/build_images/` if containers fail. Disk space exhaustion is common with instance caching—monitor continuously and use env caching for most workflows. Overly specific tests in some instances can reject valid solutions, which is partially addressed in Verified but remains an issue researchers should acknowledge when analyzing failures.

### Leaderboard submission and community standards

Submitting to the **official leaderboard** requires forking the SWE-bench/experiments repository, creating a dated folder (YYYYMMDD_modelname), and including all predictions, results, metadata, logs, and optionally reasoning trajectories. The metadata.yaml file specifies model name, authors, date, description, and contact information. Submissions undergo review to verify evaluation was performed correctly using the official harness with standard settings.

The leaderboard tracks several categories: full SWE-bench, Lite, and Verified splits, with separate entries for different foundation models and scaffolding approaches. The community distinguishes between scaffold performance (how well a specific agent implementation works) and model performance (inherent model capabilities), recognizing that results don't transfer directly between different scaffolds. Leading systems as of early 2025 include mini-SWE-agent (65% on Verified), various commercial agents (30-50% range), and research prototypes, with rapid progress continuing.

## Dataset variants enable specialized evaluation scenarios

SWE-bench has evolved into a family of benchmarks addressing different evaluation needs beyond the original 2,294 Python instances. **SWE-bench Verified** (500 human-validated instances) has become the gold standard for reliable evaluation, filtering out the 68.3% of instances with problematic specifications or tests through review by 93 professional developers. **SWE-bench Lite** (300 instances) provides cost-effective evaluation for iteration, selecting simpler self-contained bugs without multi-file changes or external dependencies. **SWE-bench Multimodal** (617 JavaScript instances) tests visual understanding with required images in every instance, achieving only 12% top resolution vs. 30-40% on Python benchmarks.

Contamination-resistant variants address training data leakage concerns. **SWE-bench Live** continuously updates monthly with 50 new verified instances from post-October 2024 issues, currently containing 1,565 instances across 164 repositories. Top systems achieve just 19.25% on Live compared to 60%+ on Verified, revealing significant overfitting to the static benchmark repositories. **SWE-bench+** uses only post-cutoff instances to eliminate the 94%+ contamination rate in the original benchmark, with SWE-Agent+GPT-4 performance dropping from 12.47% to 3.97% under these stricter conditions.

Multilingual variants expand beyond Python: **SWE-bench Multilingual** (300 instances across 9 languages in 42 repositories) enables quick cross-language evaluation, while **Multi-SWE-bench** (1,632 instances across 7 languages) provides comprehensive multilingual coverage with 68 expert annotators ensuring quality. **SWE-bench Pro** (1,865 instances) includes 276 instances from private commercial codebases and uses GPL-licensed repositories as contamination deterrents, representing the first benchmark with truly proprietary code.

### Choosing the right variant for your research

For **publication benchmarks**, use SWE-bench Verified as the primary evaluation since it supersedes the original test set according to the benchmark authors. Supplement with Lite results to demonstrate cost-effectiveness, and add Multimodal or Multilingual if making claims about those capabilities. The community consensus has converged on Verified as the standard, making results maximally comparable with other work.

For **model development workflows**, iterate on Lite during prototyping due to its 10x faster evaluation time and lower cost (~$102 vs. ~$1,020 for full benchmark at typical API prices). Move to Verified once the approach stabilizes, as it provides higher-quality assessment with better-specified problems. Consider Live or SWE-bench+ when contamination is a concern, particularly for models trained on post-2023 data that likely saw the original benchmark instances.

**Multimodal and multilingual research** requires the specialized variants. Multimodal tests visual understanding genuinely—every instance contains images essential to solving the problem. The 12% top resolution rate (vs. 30-40% on Python) demonstrates that visual software engineering remains significantly harder. For language generalization, Multilingual provides quick validation (300 instances) while Multi-SWE-bench offers comprehensive evaluation (1,632 instances). The performance gap between Python and other languages reveals language-specific optimizations in current systems.

The benchmark ecosystem demonstrates rapid evolution in response to quality concerns, contamination issues, and domain expansion needs. Researchers should track new variants and consider which best matches their evaluation goals, reporting variant selection rationale clearly in papers to ensure readers can assess result validity and comparability.