llmè¿™äº›åœ°æ–¹æˆ‘è¿˜è¦ç”¨åˆ°çš„ã€‚éœ€è¦llm èªæ˜çš„ä¸€äº›æ¨¡å‹ æœ€èµ·ç  gpt4.1ä»¥ä¸Šï¼Œæ€ä¹ˆä¹°æœ€ä¾¿å®œ å»å“ªé‡Œä¹°ï¼Ÿ


# LLMé‡‡è´­ç­–ç•¥ - æœ€ä¼˜æ€§ä»·æ¯”æ–¹æ¡ˆ

---

## ä¸€ã€ä½ çš„LLMéœ€æ±‚åˆ†æ

### **Criticaléœ€è¦Smart Modelsçš„åœ°æ–¹** (éœ€è¦GPT-4çº§åˆ«)

| ç”¨é€” | ä¸ºä»€ä¹ˆéœ€è¦Smart | é¢‘ç‡ | é¢„ä¼°æˆæœ¬/ä»»åŠ¡ |
|------|----------------|------|--------------|
| **Goal Parsing** | ä»å¤æ‚issueæå–ç»“æ„åŒ–goalï¼Œå®¹é”™ç‡ä½ | 1x/task | $0.02 |
| **Pattern Extraction** | Decontextualizeéœ€è¦æŠ½è±¡èƒ½åŠ› | 0.3x/task (åªæˆåŠŸæ—¶) | $0.05 |
| **Code Generation** | ç”Ÿæˆæ­£ç¡®patchï¼Œquality critical | 2-3x/task | $0.15 |
| **Drift Judging (å¤æ‚cases)** | åˆ¤æ–­subtle alignment issues | 3-5x/task | $0.05 |

**æ€»éœ€æ±‚**: ~300 tasks Ã— $0.27 = **~$80-100 for evaluation**

### **å¯ä»¥ç”¨ä¾¿å®œModelsçš„åœ°æ–¹** (GPT-4o-miniè¶³å¤Ÿ)

| ç”¨é€” | ä¸ºä»€ä¹ˆå¯ä»¥ä¾¿å®œ | é¢‘ç‡ | é¢„ä¼°æˆæœ¬/ä»»åŠ¡ |
|------|---------------|------|--------------|
| **Simple Drift Check** | Rule-basedä¸ºä¸»ï¼ŒLLMè¾…åŠ© | 10x/task | $0.03 |
| **Embedding Generation** | ä¸éœ€è¦reasoning | 5x/task | $0.001 |
| **Log Summarization** | æ ¼å¼åŒ–è¾“å‡º | 1x/task | $0.01 |

---

## äºŒã€æœ€ä¾¿å®œé‡‡è´­æ–¹æ¡ˆ (æŒ‰ä¼˜å…ˆçº§)

### **ğŸ¥‡ æ–¹æ¡ˆ1: OpenAI API Direct - æœ€æ¨è**

#### **ä¸ºä»€ä¹ˆæœ€ä¼˜**
- âœ… å®˜æ–¹ç¨³å®šæ€§æœ€å¥½
- âœ… GPT-4oç°åœ¨å¾ˆä¾¿å®œ ($2.50/1M input tokens)
- âœ… ä¸éœ€è¦é¢å¤–ä¸­é—´å•†
- âœ… å­¦ç”Ÿ/ç ”ç©¶æœ‰æ—¶æœ‰credits

#### **ä»·æ ¼å¯¹æ¯”** (2025å¹´1æœˆæœ€æ–°)

| Model | Input | Output | é€‚åˆä½ çš„ç”¨é€” |
|-------|-------|--------|-------------|
| **GPT-4o** | $2.50/1M | $10/1M | âœ… Goal parsing, Pattern extraction |
| **GPT-4o-mini** | $0.15/1M | $0.60/1M | âœ… Drift checking, Summarization |
| GPT-4 Turbo | $10/1M | $30/1M | âŒ å¤ªè´µï¼Œç”¨4oå°±å¤Ÿ |
| o1-preview | $15/1M | $60/1M | âŒ ä½ ä¸éœ€è¦è¿™ä¹ˆå¼ºæ¨ç† |

#### **å¦‚ä½•è´­ä¹°**
```
1. ç›´æ¥å®˜ç½‘æ³¨å†Œ: https://platform.openai.com/
2. Add credit: $100 (å¯ä»¥ç”¨å®Œä¸ºæ­¢ï¼Œä¸expire)
3. Get API key
4. ä½¿ç”¨ç¯å¢ƒå˜é‡: export OPENAI_API_KEY="sk-..."
```

#### **æˆæœ¬ä¼°ç®—**
```python
# ä½ çš„workload (300 tasks full evaluation)

Goal Parsing (GPT-4o):
- 300 tasks Ã— 1000 tokens input Ã— $2.50/1M = $0.75
- 300 tasks Ã— 500 tokens output Ã— $10/1M = $1.50
Subtotal: $2.25

Pattern Extraction (GPT-4o):
- 90 successes Ã— 2000 tokens input Ã— $2.50/1M = $0.45
- 90 successes Ã— 800 tokens output Ã— $10/1M = $0.72
Subtotal: $1.17

Code Generation (GPT-4o):
- 300 tasks Ã— 3 attempts Ã— 1500 tokens input = $3.37
- 900 generations Ã— 1000 tokens output = $9.00
Subtotal: $12.37

Drift Checking (GPT-4o-mini):
- 300 tasks Ã— 10 checks Ã— 300 tokens = $0.13
Subtotal: $0.13

Total: ~$16 for 300 tasks
```

**ä½ çš„å®é™…æˆæœ¬ä¼šæ›´é«˜å› ä¸º**:
- Development/debugging iterations: 3x
- Failed attempts: 2x
- **Realistic budget: $50-80**

---

### **ğŸ¥ˆ æ–¹æ¡ˆ2: Anthropic Claude API - å¤‡é€‰**

#### **ä»€ä¹ˆæ—¶å€™ç”¨Claude**
- âœ… Code generationå¯èƒ½æ¯”GPT-4oæ›´å¥½
- âœ… æ›´é•¿context window (200K)
- âš ï¸ ç¨è´µä¸€ç‚¹

#### **ä»·æ ¼** (Claude 3.5 Sonnet)

| Model | Input | Output |
|-------|-------|--------|
| Claude 3.5 Sonnet | $3/1M | $15/1M |
| Claude 3.5 Haiku | $0.25/1M | $1.25/1M |

**æˆæœ¬**: æ¯”GPT-4oè´µ~20%ï¼Œçº¦$20 for 300 tasks

#### **å¦‚ä½•è´­ä¹°**
```
1. https://console.anthropic.com/
2. Add $100 prepaid
3. Get API key
```

---

### **ğŸ¥‰ æ–¹æ¡ˆ3: OpenRouter - APIèšåˆå¹³å°**

#### **ä»€ä¹ˆæ˜¯OpenRouter**
- ä¸€ä¸ªå¹³å°è®¿é—®å¤šä¸ªLLM providers
- ç»Ÿä¸€API interface
- æœ‰æ—¶æœ‰ä¿ƒé”€ä»·æ ¼

#### **ä¼˜ç‚¹**
- âœ… ä¸€ä¸ªAPI keyè®¿é—®GPT-4, Claude, Geminiç­‰
- âœ… æ–¹ä¾¿åˆ‡æ¢modelsæµ‹è¯•
- âœ… æœ‰æ—¶æ¯”å®˜æ–¹ä¾¿å®œ5-10%

#### **ç¼ºç‚¹**
- âš ï¸ ç¨³å®šæ€§ç¨å·®äºå®˜æ–¹
- âš ï¸ æœ‰æ—¶æœ‰rate limits

#### **ä»·æ ¼**
- GPT-4o: ~$2.60/1M input (vs $2.50 official)
- Claude 3.5: ~$3.10/1M input (vs $3.00 official)

**æˆæœ¬**: çº¦$17-20 for 300 tasks

#### **å¦‚ä½•ä½¿ç”¨**
```
1. https://openrouter.ai/
2. Add credits
3. Use OpenAI-compatible API
```

---

### **âŒ ä¸æ¨èçš„æ–¹æ¡ˆ**

| æ–¹æ¡ˆ | ä¸ºä»€ä¹ˆä¸æ¨è |
|------|-------------|
| **Together AI / Replicate** | Open-source models (Llama, Qwen)è´¨é‡ä¸å¤Ÿ |
| **Azure OpenAI** | éœ€è¦ä¼ä¸šè´¦å·ï¼Œæ¯”ç›´æ¥APIè´µ |
| **å›½å†…API (æ™ºè°±/æœˆä¹‹æš—é¢)** | è‹±æ–‡ä»£ç èƒ½åŠ›ä¸å¦‚GPT-4o |
| **Local models** | RTX 4090ä¹Ÿè·‘ä¸åŠ¨GPT-4çº§åˆ«ï¼Œä¸”å¤ªæ…¢ |

---

## ä¸‰ã€çœé’±ç­–ç•¥ (é‡è¦!)

### **Strategy 1: Hybrid Modelä½¿ç”¨**

```python
# ç”¨ä¸åŒmodelsåšä¸åŒä»»åŠ¡
class CostOptimizedLLM:
    def __init__(self):
        self.smart_model = "gpt-4o"          # $2.50/1M
        self.cheap_model = "gpt-4o-mini"     # $0.15/1M
        self.code_model = "claude-3.5-sonnet" # Sometimes better
    
    def select_model(self, task_type):
        if task_type == "goal_parsing":
            return self.smart_model  # Critical, need accuracy
        
        elif task_type == "pattern_extraction":
            return self.smart_model  # Critical, need abstraction
        
        elif task_type == "code_generation":
            return self.code_model  # Claude may be better
        
        elif task_type == "drift_check":
            # Simple checks: use cheap
            # Complex checks: use smart
            return self.cheap_model if is_simple else self.smart_model
        
        elif task_type == "embedding":
            return "text-embedding-3-small"  # $0.02/1M
        
        else:
            return self.cheap_model
```

**Savings**: ~40% cost reduction

---

### **Strategy 2: Caching & Batching**

```python
# Cache LLM responses
import diskcache

cache = diskcache.Cache('./llm_cache')

def cached_llm_call(prompt, model="gpt-4o"):
    cache_key = hash(prompt + model)
    
    if cache_key in cache:
        return cache[cache_key]  # Free!
    
    response = llm.generate(prompt, model=model)
    cache[cache_key] = response
    return response
```

**Savings**: 
- Developmentæ—¶é‡å¤callsä¸èŠ±é’±
- å¯çœ~30-50% during development

---

### **Strategy 3: Prompt Optimization**

```python
# Bad (expensive)
prompt = f"""
Here is the entire codebase (50,000 tokens):
{entire_codebase}

And here is the issue:
{issue}

What's wrong?
"""

# Good (cheap)
prompt = f"""
Issue: {issue}

Relevant code section (500 tokens):
{relevant_section_only}

Extract goal as JSON.
"""
```

**Savings**: 10x token reduction = 10x cheaper

---

### **Strategy 4: Progressive Enhancement**

```python
# Start with cheap model, upgrade if needed
def smart_generate(prompt):
    # Try cheap first
    response = gpt4o_mini(prompt)
    
    # Check if response is good enough
    if is_high_quality(response):
        return response  # Save money!
    
    # If not, use expensive model
    return gpt4o(prompt)
```

**Savings**: ~20-30% by avoiding expensive calls

---

### **Strategy 5: Development vs Production**

```python
# During development (Week 1-4)
if os.getenv("ENVIRONMENT") == "dev":
    # Use smaller test set
    test_set = swebench[:10]  # Only 10 tasks
    # Use cheaper models
    model = "gpt-4o-mini"

# During evaluation (Week 5)
else:
    test_set = swebench[:300]  # Full set
    model = "gpt-4o"  # Best quality
```

**Savings**: DevelopmentåªèŠ±~$5-10ï¼Œevaluationæ‰èŠ±$50-80

---

## å››ã€å…·ä½“è´­ä¹°å»ºè®®

### **ä½ çš„Action Plan**

#### **Week 0 (ç°åœ¨): Setup**

```
1. OpenAI Account
   - å» https://platform.openai.com/signup
   - Add $50 credit (å¤Ÿdevelopmentç”¨)
   - Get API key
   - è®¾ç½®billing alerts ($25, $40)

2. Anthropic Account (Optional backup)
   - å» https://console.anthropic.com/
   - Add $20 credit (backup)
   - Get API key

3. Environment Setup
   export OPENAI_API_KEY="sk-proj-..."
   export ANTHROPIC_API_KEY="sk-ant-..."
```

#### **Week 1-3: Development ($10-20)**
- ç”¨small test set (10-20 tasks)
- å¤§é‡ç”¨cache
- ä¸»è¦coståœ¨debugging prompts

#### **Week 4: Validation ($10-15)**
- Test on 50 tasks
- Refine prompts
- ç¡®ä¿quality

#### **Week 5: Full Evaluation ($50-80)**
- 200-300 tasks
- Multiple runs for statistical significance
- Final results

**Total Budget: $80-120**

---

### **å¦‚æœé¢„ç®—ç´§å¼  (<$50)**

#### **Fallback Plan**

```
Option A: å‡å°‘test set
- åªè·‘100 tasks instead of 300
- Still statistically valid
- Cost: ~$30

Option B: ç”¨æ›´å¤šcheap models
- GPT-4o-mini for most tasks
- GPT-4o only for pattern extraction
- Cost: ~$25
- Trade-off: ç¨ä½quality

Option C: ç”³è¯·Research Credits
- OpenAIæœ‰researcher program
- Anthropicæœ‰academic program
- å¯èƒ½å…è´¹æˆ–æŠ˜æ‰£
```

---

### **ç”³è¯·å…è´¹Creditsçš„é€”å¾„**

#### **1. OpenAI Researcher Access**
```
URL: https://openai.com/form/researcher-access-program
Requirements:
- .edu email
- Research proposal
- Faculty endorsement (Yucheng?)

Credits: Up to $1000
Success rate: Medium (worth trying)
```

#### **2. Anthropic Academic Program**
```
URL: https://www.anthropic.com/research
Contact: research@anthropic.com

Requirements:
- Academic institution
- Research description

Credits: Case by case
```

#### **3. Google Cloud Credits (for Gemini)**
```
URL: https://cloud.google.com/edu
Credits: $300 for students

å¯ä»¥ç”¨æ¥è·‘Gemini (ç±»ä¼¼GPT-4è´¨é‡)
```

#### **4. Microsoft Azure for Students**
```
URL: https://azure.microsoft.com/en-us/free/students/
Credits: $100

å¯ä»¥access Azure OpenAI (éœ€è¦approvalä½†æ›´ä¾¿å®œ)
```

---

## äº”ã€Cost Tracking & Monitoring

### **å®æ—¶Monitor**

```python
import openai
from collections import defaultdict

class CostTracker:
    def __init__(self):
        self.costs = defaultdict(float)
        self.token_usage = defaultdict(int)
    
    def log_call(self, model, prompt_tokens, completion_tokens):
        # Calculate cost
        if model == "gpt-4o":
            input_cost = prompt_tokens * 2.50 / 1_000_000
            output_cost = completion_tokens * 10 / 1_000_000
        elif model == "gpt-4o-mini":
            input_cost = prompt_tokens * 0.15 / 1_000_000
            output_cost = completion_tokens * 0.60 / 1_000_000
        
        total_cost = input_cost + output_cost
        
        self.costs[model] += total_cost
        self.token_usage[model] += prompt_tokens + completion_tokens
    
    def report(self):
        print("Cost Summary:")
        total = 0
        for model, cost in self.costs.items():
            print(f"  {model}: ${cost:.2f} ({self.token_usage[model]:,} tokens)")
            total += cost
        print(f"Total: ${total:.2f}")
        
        if total > 50:
            print("âš ï¸ Warning: Approaching budget limit!")

# Use it
tracker = CostTracker()

# After each LLM call
response = openai.chat.completions.create(...)
tracker.log_call(
    model="gpt-4o",
    prompt_tokens=response.usage.prompt_tokens,
    completion_tokens=response.usage.completion_tokens
)

# Check periodically
tracker.report()
```

---

## å…­ã€æ€»ç»“ï¼šæ¨èé…ç½®

### **æœ€ä½³é…ç½® (Total ~$100)**

```
Primary: OpenAI Direct
â”œâ”€ GPT-4o: Goal parsing, Pattern extraction, Code gen
â”œâ”€ GPT-4o-mini: Drift checking, Summarization
â””â”€ text-embedding-3-small: Embeddings

Backup: Anthropic
â””â”€ Claude 3.5 Sonnet: å¦‚æœcode generationéœ€è¦æ›´å¥½

Budget Allocation:
- Development (Week 1-4): $20
- Validation (Week 4): $10
- Full Evaluation (Week 5): $60
- Buffer: $10
```

### **çœé’±é…ç½® (Total ~$50)**

```
Primary: OpenAI Direct
â”œâ”€ GPT-4o-mini: Everything except critical tasks
â””â”€ GPT-4o: Only pattern extraction

Strategies:
- Reduce test set to 100 tasks
- Aggressive caching
- Prompt optimization
- Progressive enhancement

Budget Allocation:
- Development: $10
- Validation: $5
- Evaluation: $30
- Buffer: $5
```

### **å­¦ç”Ÿé…ç½® (Total ~$20-30 + Free Credits)**

```
1. Apply for free credits:
   - OpenAI researcher program
   - Google Cloud $300
   - Azure student $100

2. Use smallest viable test set (50-100 tasks)

3. Hybrid approach:
   - Free credits for development
   - Paid API for final evaluation
```

---

## Quick Action Steps

### **ä»Šå¤©å°±åš**
1. âœ… æ³¨å†ŒOpenAIè´¦å· (https://platform.openai.com)
2. âœ… Add $50 credit (å¤Ÿä½ development)
3. âœ… Get API keyï¼Œå­˜åˆ°ç¯å¢ƒå˜é‡
4. âœ… æµ‹è¯•ä¸€ä¸ªç®€å•callç¡®ä¿å·¥ä½œ

### **This week**
1. â­ ç”³è¯·research credits (OpenAI + Google Cloud)
2. â­ Setup cost tracking in code
3. â­ Test model quality on 5 tasks

### **Week 1-2**
1. ğŸ”§ ç”¨10 tasksåå¤æµ‹è¯•ï¼Œoptimize prompts
2. ğŸ”§ Heavy caching
3. ğŸ”§ Budget: <$10

---

**Bottom line**: 
- **$100 budget is safe** for full project
- **$50 is doable** with optimizations
- **Free creditså¯ä»¥ç”³è¯·** (worth trying)
- **OpenAI GPT-4oæ˜¯æœ€ä¼˜é€‰æ‹©** (quality + price)

éœ€è¦å¸®ä½ å†™ç”³è¯·research creditsçš„emailå—ï¼Ÿæˆ–è€…setup cost tracking codeï¼Ÿ