好的 关于文章问题和架构，计划优先级，phase和里程碑以及metrics 定义和eval 还有如何用Llm（比如review judge 或者summary）和公开数据swe bench，重要的数据结构和枚举。随后可否写一个表格总结 或者bulets。简洁一些 依旧清晰和直接。不用给具体代码



# Adaptive Memory System for Coding Agents - Complete Overview

---

## 一、核心Research Questions

| Question | Focus | Novel Contribution | Success Criteria |
|----------|-------|-------------------|------------------|
| **Q1: Goal Alignment** | 防止agent在multi-step execution中drift | Real-time alignment checking + drift blocking | Drift rate <15% (vs baseline 28%) |
| **Q2: Pattern Learning** | 从past tasks提取可复用patterns | Decontextualization + cross-session reuse | Reuse rate ≥30%, time savings ≥30% |
| **Q3: Dynamic Abstraction** | 根据context调整detail level | Context-aware multi-level selection | Dynamic > fixed in efficiency |

**Core Insight**: Dynamic abstraction (Yucheng's "nobody is doing") - 在Q3实现，但context从"user expertise"调整为"task/agent context"

---

## 二、System Architecture

### **Component Dependencies**

```
User Input (GitHub Issue)
    ↓
[Goal Parser] ────────────→ Structured Goal (Q1)
    ↓
[Pattern Retriever] ──────→ Relevant Patterns (Q2)
    ↓                              ↓
[Abstraction Selector] ───→ Choose Level 1/2/3 (Q3)
    ↓
[Agent Execution Loop]
    ├─→ [Action Monitor] ───→ Check alignment (Q1)
    ├─→ Execute Action
    ├─→ Observe Results
    └─→ [Drift Detector] ────→ Log & Block if drift (Q1)
    ↓
Test Results (Pass/Fail)
    ↓
[Pattern Extractor] ──────→ New Pattern if success (Q2)
    ↓
[Pattern Library] ─────────→ Store for future use
```

**Critical Path**: Goal Parsing → Pattern Retrieval → Abstraction Selection → Monitored Execution → Pattern Learning

---

## 三、Implementation Priority & Phases

### **Phase 1: Foundation (Weeks 1-2)**

| Priority | Component | Dependencies | Deliverable |
|----------|-----------|--------------|-------------|
| P0 | Goal Parser | LLM API | Parse issue → structured goal |
| P0 | Action Logger | File I/O | Log all agent actions |
| P0 | Basic Drift Detector | Goal Parser | File scope + forbidden action checks |
| P1 | Pattern Extraction | LLM API | Extract pattern from successful session |
| P1 | Pattern Storage | Vector DB (ChromaDB) | Store patterns with embeddings |

**Week 2 Milestone**: 5-10 tasks with goal tracking + basic pattern extraction

---

### **Phase 2: Integration (Week 3 - CHECKPOINT)**

| Priority | Component | Dependencies | Deliverable |
|----------|-----------|--------------|-------------|
| P0 | Pattern Retrieval | Pattern Storage | Semantic search for relevant patterns |
| P0 | Pattern Application | Pattern Retrieval | Use patterns to solve new tasks |
| P0 | Evaluation Harness | SWE-bench | Run on 50 tasks, compute metrics |
| P1 | Multi-level Storage | Pattern Extraction | Store hint/explanation/code |

**Week 3 Checkpoint**: 
- ✅ Pattern reuse rate ≥20% on 50 tasks
- ✅ Drift detection working
- ⚠️ If not achieved → Debug Q1/Q2, may skip Q3

---

### **Phase 3: Advanced Features (Week 4)**

| Priority | Component | Dependencies | Deliverable |
|----------|-----------|--------------|-------------|
| P1 | Abstraction Selector | Multi-level Storage | Choose level based on context |
| P1 | Learning Mechanism | Usage History | Track which levels work best |
| P2 | Drift Recovery | Drift Detector | Checkpoint/rollback |

**Week 4 Milestone**: Dynamic abstraction implemented (if time permits)

---

### **Phase 4: Evaluation (Week 5)**

| Priority | Task | Deliverable |
|----------|------|-------------|
| P0 | Full evaluation (200 tasks) | All metrics computed |
| P0 | Statistical tests | T-tests, p-values, effect sizes |
| P0 | Ablation studies | Component contribution proven |
| P1 | Error analysis | Failure categorization |
| P1 | Learning curves | Show improvement over time |

**Week 5 Milestone**: Complete results for paper

---

### **Phase 5: Polish (Week 6)**

| Priority | Task | Deliverable |
|----------|------|-------------|
| P0 | Demo video | 5-min showing all components |
| P0 | Paper draft | 4-6 pages |
| P1 | Code cleanup | GitHub repo publishable |
| P2 | Visualizations | Publication-quality figures |

---

## 四、Data Structures & Enumerations

### **Core Data Types**

```
Task {
  instance_id: str
  problem_statement: str (GitHub issue text)
  repo: str
  base_commit: str (Git hash)
  patch: str (Gold solution - don't look during solving!)
  test_patch: str (Test code)
  FAIL_TO_PASS: List[str] (Tests that must pass)
}

Goal {
  objective: str (One-sentence goal)
  allowed_files: List[str] (Scope of changes)
  forbidden_actions: List[str] (refactor, optimize, etc)
  success_criteria: str (When is it done?)
}

Action {
  type: ActionType (read_file | edit_file | run_test | etc)
  file: str
  description: str
  aligned: bool (Does it match goal?)
  timestamp: datetime
}

Pattern {
  id: str
  name: str (e.g., "null_check_before_access")
  problem_signature: str (What problem it solves)
  solution_approach: str (High-level how)
  
  # Multi-level abstractions
  level_1_hint: str (10 words)
  level_2_explanation: str (50 words)
  level_3_code: str (Full code example)
  
  # Metadata
  times_used: int
  success_rate: float
  complexity: PatternComplexity
  applicable_domains: List[str]
  embedding: Vector (For semantic search)
}

Session {
  task_id: str
  goal: Goal
  actions: List[Action]
  drift_analysis: DriftAnalysis
  pattern_used: Optional[Pattern]
  outcome: SessionOutcome
}
```

### **Enumerations**

```
ActionType:
  - READ_FILE
  - EDIT_FILE
  - CREATE_FILE
  - DELETE_FILE
  - RUN_COMMAND
  - RUN_TESTS
  - SEARCH_CODE

DriftType:
  - WRONG_FILE (Modified file not in scope)
  - FORBIDDEN_ACTION (Refactor when should fix)
  - SCOPE_EXPANSION (Changed more than needed)
  - EXCESSIVE_EXPLORATION (Read too many irrelevant files)

AbstractionLevel:
  - LEVEL_1_HINT (Minimal, 1 sentence)
  - LEVEL_2_EXPLANATION (Medium, conceptual)
  - LEVEL_3_CODE (Detailed, full implementation)

PatternComplexity:
  - SIMPLE (Single concept, one file)
  - MEDIUM (Multiple concepts, few files)
  - COMPLEX (Multiple modules, architectural)

SessionOutcome:
  - SUCCESS (Tests passed)
  - PARTIAL (Some tests passed)
  - FAILURE (No tests passed)
  - TIMEOUT (Exceeded time limit)
```

---

## 五、Metrics Definition

### **Q1: Goal Alignment Metrics**

| Metric | Definition | Computation | Target |
|--------|------------|-------------|--------|
| **Drift Rate** | % of actions that are off-goal | drift_actions / total_actions | <15% |
| **Drift Frequency** | Average # drift actions per task | Σ drift_actions / # tasks | <1.5 |
| **Wasted Files** | # files modified but not in gold patch | len(agent_files - gold_files) | <2 |
| **First Drift Turn** | When does drift start? | Turn # of first drift action | Later is better |

### **Q2: Pattern Learning Metrics**

| Metric | Definition | Computation | Target |
|--------|------------|-------------|--------|
| **Pattern Reuse Rate** | % tasks where pattern applied | tasks_with_pattern / total_tasks | ≥30% |
| **Time Savings** | % faster with pattern vs without | (time_without - time_with) / time_without | ≥30% |
| **Success with Pattern** | Success rate when pattern used | successes_with_pattern / tasks_with_pattern | ≥35% |
| **Pattern Coverage** | % test tasks with relevant pattern | tasks_with_available_pattern / total_tasks | ≥40% |
| **Learning Slope** | Improvement rate over time | Linear regression coefficient | Positive |

### **Q3: Dynamic Abstraction Metrics**

| Metric | Definition | Computation | Target |
|--------|------------|-------------|--------|
| **Efficiency Gain** | Success/time for dynamic vs fixed | (success/time)_dynamic / (success/time)_fixed | >1.1 |
| **Selection Accuracy** | % times optimal level chosen | correct_selections / total_selections | ≥60% |
| **Level Distribution** | How often each level used | Count per level | Balanced |
| **Convergence Speed** | How fast learns optimal level | # tasks until stable | <50 tasks |

### **Overall System Metrics**

| Metric | Definition | Target |
|--------|------------|--------|
| **Resolve Rate** | % tasks fully solved (tests pass) | ≥30% |
| **Avg Time per Task** | Efficiency | <180s |
| **API Cost per Task** | LLM token usage | <$0.50 |
| **Actions per Task** | Efficiency | <15 |

---

## 六、Evaluation Framework

### **Dataset Usage**

```
SWE-bench Lite (300 tasks total)
├─ Training Set (0-50): Extract patterns, tune thresholds
├─ Validation Set (50-100): Tune system, select hyperparameters
└─ Test Set (100-300): Final evaluation, NEVER look during development

Test Set Split:
├─ Easy (33%): Simple bugs, 1-2 file changes
├─ Medium (50%): Moderate complexity, 3-5 file changes
└─ Hard (17%): Complex issues, 5+ file changes
```

### **Baseline Systems**

| Baseline | Type | Purpose | Expected Performance |
|----------|------|---------|---------------------|
| **Vanilla GPT-4** | Weak | Show improvement | ~8% resolve rate |
| **Static RAG** | Medium | Compare with standard retrieval | ~15% resolve rate |
| **AutoCodeRover** | Strong (Published) | Compare with SOTA | ~20% resolve rate |
| **Component Ablations** | Variants | Prove each component helps | Varies |

### **Statistical Tests Required**

```
For Each Metric:
1. T-test (Your System vs Baseline)
   - Null hypothesis: No difference
   - Alternative: Your system better
   - Significance: p < 0.05

2. Effect Size (Cohen's d)
   - Small: d > 0.2
   - Medium: d > 0.5
   - Large: d > 0.8

3. Confidence Intervals (Bootstrap)
   - 95% CI for main metrics
   - Report [lower, upper] bounds

4. Paired Comparisons
   - Same tasks for all systems
   - Control for task difficulty
```

### **Ablation Study Design**

```
Systems to Evaluate:
1. Full System (Q1 + Q2 + Q3)
2. Q1 + Q2 only
3. Q1 + Q3 only
4. Q2 + Q3 only
5. Q1 only
6. Q2 only
7. Q3 only
8. Baseline (None)

Analysis:
- Each component's contribution
- Interaction effects
- Diminishing returns?
```

---

## 七、LLM Usage in System

### **Where LLMs Are Used**

| Component | LLM Task | Input | Output | Model Choice |
|-----------|----------|-------|--------|--------------|
| **Goal Parser** | Extract structured goal | Problem statement | Goal JSON | GPT-4 (accuracy critical) |
| **Drift Checker** | Judge action alignment | Goal + Action | Yes/No + Reason | GPT-4o-mini (speed) |
| **Pattern Extractor** | Summarize session | Session trace + Solution | Pattern JSON | GPT-4 (quality critical) |
| **Pattern Retrieval** | Embedding generation | Pattern text | Vector | text-embedding-3-small |
| **Abstraction Selector** | Context analysis | Task + Pattern + History | Level 1/2/3 | GPT-4o-mini |
| **Code Generation** | Apply pattern to task | Task + Pattern + Level | Code solution | GPT-4 |

### **LLM as Judge/Evaluator**

```
Use Cases:
1. Goal Extraction Validation
   - Input: Problem statement + Extracted goal
   - Task: "Is this goal reasonable? Yes/No"
   - Validates goal parsing accuracy

2. Drift Detection (When rules insufficient)
   - Input: Goal + Action description
   - Task: "Is this action aligned? Yes/No + Reason"
   - Handles nuanced cases

3. Pattern Quality Rating
   - Input: Pattern + Example applications
   - Task: "Rate pattern quality 1-5"
   - Filter low-quality patterns

4. Solution Similarity (For pattern matching)
   - Input: Two solutions
   - Task: "Are these solving same problem? Yes/No"
   - Validate pattern applicability
```

### **Cost Management**

| Operation | Frequency | Model | Est. Cost/Task | Priority |
|-----------|-----------|-------|----------------|----------|
| Goal parsing | 1x per task | GPT-4 | $0.02 | P0 |
| Drift checking | 5-20x per task | GPT-4o-mini | $0.10 | P0 |
| Pattern extraction | 1x per success | GPT-4 | $0.05 | P0 |
| Code generation | 1-3x per task | GPT-4 | $0.20 | P0 |
| **Total** | - | - | **~$0.40/task** | - |

**Budget**: 300 tasks × $0.40 = ~$120 for full evaluation

---

## 八、SWE-bench Integration

### **How to Use SWE-bench**

```
Stage 1: Load Dataset
- Use Hugging Face datasets library
- Get: problem_statement, base_commit, gold patch
- Don't look at gold patch during solving!

Stage 2: Setup Environment
- Clone repo at base_commit
- Setup Python environment
- Install dependencies

Stage 3: Agent Execution
- Parse goal from problem_statement
- Agent takes multi-step actions
- Log all actions for drift detection
- Generate final patch

Stage 4: Evaluation
- Apply agent's patch
- Run test suite (FAIL_TO_PASS tests)
- Binary outcome: Pass or Fail

Stage 5: Learning
- If success: Extract pattern, store in library
- If failure: Analyze why, categorize error
```

### **Ground Truth Usage**

| Ground Truth | When to Use | When NOT to Use |
|--------------|-------------|-----------------|
| **problem_statement** | ✅ Always - agent input | - |
| **base_commit** | ✅ Always - starting state | - |
| **gold patch (solution)** | ✅ ONLY for drift detection (which files?) | ❌ Never for generation |
| **gold patch (solution)** | ✅ ONLY for evaluation comparison | ❌ Never show to agent |
| **test_patch** | ✅ Run tests after agent finishes | ❌ Never during solving |

**Critical**: Gold patch used ONLY for:
1. Drift detection (which files should be modified?)
2. Post-hoc evaluation (compare agent's patch vs gold)

---

## 九、Risk Management & Contingencies

### **Week 3 Checkpoint Decision Tree**

```
IF pattern_reuse_rate >= 30% AND drift_rate < 20%:
    ✅ Proceed with full plan (Q1 + Q2 + Q3)
    
ELIF pattern_reuse_rate >= 20% OR drift_rate < 25%:
    ⚠️ Partial success
    - Continue Q1 + Q2
    - Simplify Q3 (fixed rules instead of dynamic)
    
ELSE:
    ❌ Not meeting targets
    - Debug Q1 or Q2 (choose weaker)
    - Skip Q3 entirely
    - Focus on solid implementation of 2 components
```

### **Scope Adjustment Options**

| If Behind Schedule | Reduce Scope | Impact |
|-------------------|--------------|---------|
| **Week 3** | Skip Q3 dynamic selection | Still have Q1+Q2, 70% of novelty |
| **Week 4** | Use fixed abstraction rules | Q3 becomes simple ablation |
| **Week 5** | Reduce test set to 100 tasks | Less statistical power |
| **Week 6** | Skip user study | Already planned as future work |

---

## 十、Deliverables Summary

### **Required Deliverables**

| Deliverable | Format | Audience | Content |
|-------------|--------|----------|---------|
| **Working System** | Python code + Docker | Researchers | Runnable agent with all components |
| **Evaluation Results** | JSON + CSV | Reviewers | All metrics, per-task results |
| **Demo Video** | 5-min MP4 | General | Show system solving 3 tasks |
| **Paper Draft** | 4-6 pages PDF | Academic | ICLR/NeurIPS workshop format |
| **GitHub Repo** | Public repo | Community | Code, data splits, instructions |

### **Paper Structure**

```
1. Abstract (150 words)
   - Problem, approach, results

2. Introduction (1 page)
   - Motivation: Current agents drift, don't learn, use fixed abstraction
   - Solution: Goal tracking + Pattern learning + Dynamic abstraction
   - Contributions: Novel dynamic abstraction mechanism

3. Related Work (0.5 page)
   - Coding agents (AutoCodeRover, SWE-agent)
   - Memory systems (AgentFlow, ReAct)
   - Difference: We do dynamic abstraction

4. Method (1.5 pages)
   - Q1: Goal tracking architecture
   - Q2: Pattern extraction & reuse
   - Q3: Dynamic abstraction selection

5. Experiments (2 pages)
   - Dataset: SWE-bench Lite
   - Baselines: 3 systems
   - Metrics: Defined clearly
   - Results: Tables + Figures
   - Ablations: Component contributions
   - Analysis: Learning curves, error categorization

6. Conclusion (0.5 page)
   - Summary
   - Limitations: No user study yet
   - Future work: User adaptation
```

---

## 十一、Success Criteria by Grade

### **B Grade (Minimum Viable)**

```
Technical:
- ✅ Q1 implemented and working (drift < 20%)
- ✅ Q2 implemented and working (reuse ≥ 20%)
- ⚠️ Q3 basic version (multi-level storage)
- ✅ Evaluation on 100+ tasks
- ✅ Statistical significance shown

Paper Quality:
- Clear methods section
- Basic ablation study
- Some error analysis
```

### **A Grade (Strong Paper)**

```
Technical:
- ✅ Q1 excellent (drift < 15%, p < 0.01)
- ✅ Q2 excellent (reuse ≥ 30%, time savings ≥ 30%)
- ✅ Q3 dynamic selection working
- ✅ Evaluation on 200 tasks
- ✅ Comprehensive ablations
- ✅ Learning curves shown

Paper Quality:
- Publication-ready figures
- Thorough error analysis
- Comparison with SOTA
- Clear contribution
```

### **A+ / Publication Quality**

```
Technical:
- ✅ All metrics exceed targets
- ✅ Approach or beat AutoCodeRover (>20% resolve)
- ✅ Clear learning over time
- ✅ Novel insights from analysis

Paper Quality:
- Workshop-ready submission
- Open-source release
- Reproducible results
- Community impact potential
```

---

## 十二、Key Decisions Summary

| Decision Point | Options | Chosen | Rationale |
|---------------|---------|--------|-----------|
| **User Adaptation (Q3)** | Keep vs Adapt | Adapt to task context | SWE-bench has no users, still novel |
| **Evaluation Dataset** | SWE-bench vs Custom | SWE-bench | Standard benchmark, reproducible |
| **Pattern Extraction** | Rule-based vs LLM | LLM | Better quality, more flexible |
| **Drift Detection** | Real-time vs Post-hoc | Real-time | Can block drift immediately |
| **Abstraction Selection** | Static vs Dynamic | Dynamic (if time) | Core novelty, but contingent |
| **Test Set Size** | 100 vs 200 | 200 (if possible) | Better statistical power |

---

## 总结：核心要点

### **研究核心**
- 3个Questions，每个解决coding agents的一个fundamental problem
- Novel contribution是Q3的dynamic abstraction，但context需要从user调整到task

### **实现优先级**
- Week 1-2: Q1+Q2 foundation (P0)
- Week 3: Checkpoint - 决定是否做Q3
- Week 4-5: Q3 (if time) + Full evaluation
- Week 6: Polish

### **Evaluation关键**
- 3个baselines (weak/medium/strong)
- 统计显著性必须证明
- Ablation study证明每个component有用
- 200个tasks，95% confidence

### **成功标准**
- Conservative: Drift <20%, Reuse ≥20%, Resolve ≥25%
- Target: Drift <15%, Reuse ≥30%, Resolve ≥30%
- Stretch: Beat AutoCodeRover (>20%)

这个summary清楚吗？需要哪部分更详细？