"""
SWE-bench Evaluator Bridge
ç”Ÿæˆpredictions.jsonlå¹¶æä¾›å®˜æ–¹evaluatorä½¿ç”¨è¯´æ˜
"""

import json
from pathlib import Path
from typing import List, Dict, Any
from step1_load_data import SWEBenchTask


def prepare_predictions(
    tasks: List[SWEBenchTask],
    patches: List[str],
    output_file: Path
) -> None:
    """
    ç”Ÿæˆpredictions.jsonlä¾›SWE-benchå®˜æ–¹evaluatorä½¿ç”¨

    Args:
        tasks: SWE-benchä»»åŠ¡åˆ—è¡¨
        patches: å¯¹åº”çš„agentç”Ÿæˆçš„patchåˆ—è¡¨
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    if len(tasks) != len(patches):
        raise ValueError(f"Tasks ({len(tasks)}) and patches ({len(patches)}) must have same length")

    output_file.parent.mkdir(parents=True, exist_ok=True)

    with open(output_file, 'w') as f:
        for task, patch in zip(tasks, patches):
            prediction = {
                'instance_id': task.instance_id,
                'model_patch': patch,
                'model_name_or_path': 'q1-monitored-agent',  # å¯è‡ªå®šä¹‰
            }
            f.write(json.dumps(prediction) + '\n')

    print(f"âœ… Predictions saved to: {output_file}")
    print(f"   {len(tasks)} predictions written")


def print_evaluator_instructions(
    predictions_file: Path,
    swebench_file: Path,
    log_dir: Path
) -> None:
    """
    æ‰“å°å¦‚ä½•è¿è¡ŒSWE-benchå®˜æ–¹evaluatorçš„è¯´æ˜

    Args:
        predictions_file: predictions.jsonlè·¯å¾„
        swebench_file: verified.jsonlè·¯å¾„
        log_dir: æ—¥å¿—è¾“å‡ºç›®å½•
    """
    print("\n" + "=" * 80)
    print("ğŸ“‹ How to Run SWE-bench Official Evaluator")
    print("=" * 80)

    print("\n1. Install SWE-bench (if not already installed):")
    print("   ```bash")
    print("   git clone https://github.com/princeton-nlp/SWE-bench.git")
    print("   cd SWE-bench")
    print("   pip install -e .")
    print("   ```")

    print("\n2. Run evaluation:")
    print("   ```bash")
    print(f"   python -m swebench.harness.run_evaluation \\")
    print(f"       --predictions_path {predictions_file} \\")
    print(f"       --swe_bench_tasks {swebench_file} \\")
    print(f"       --log_dir {log_dir} \\")
    print(f"       --testbed /tmp/testbed")
    print("   ```")

    print("\n3. Wait for results (this may take a while):")
    print("   - Docker containers will be created for each task")
    print("   - Tests will be run automatically")
    print("   - Results saved to log_dir/")

    print("\n4. View results:")
    print(f"   ```bash")
    print(f"   cat {log_dir}/results.json")
    print("   ```")

    print("\n" + "=" * 80)
    print("ğŸ’¡ Expected output format:")
    print("=" * 80)
    print("""
{
    "instance_id": {
        "django__django-11119": true,
        "astropy__astropy-12907": false,
        ...
    },
    "resolved": {
        "count": 150,
        "percentage": 30.0
    }
}
""")

    print("\nâš ï¸  Important Notes:")
    print("   - Evaluation requires Docker")
    print("   - Each task takes ~2-5 minutes")
    print("   - 500 tasks â‰ˆ 16-40 hours total")
    print("   - Can run in parallel with --num_workers")


def load_evaluation_results(results_file: Path) -> Dict[str, Any]:
    """
    åŠ è½½SWE-bench evaluatorçš„ç»“æœ

    Args:
        results_file: å®˜æ–¹evaluatorè¾“å‡ºçš„results.json

    Returns:
        åŒ…å«resolvedçŠ¶æ€çš„å­—å…¸
    """
    if not results_file.exists():
        raise FileNotFoundError(
            f"Results file not found: {results_file}\n"
            f"Please run the official evaluator first (see print_evaluator_instructions)"
        )

    with open(results_file) as f:
        results = json.load(f)

    return results


def calculate_metrics(results: Dict[str, Any]) -> Dict[str, float]:
    """
    ä»evaluatorç»“æœè®¡ç®—metrics

    Args:
        results: å®˜æ–¹evaluatorçš„è¾“å‡º

    Returns:
        åŒ…å«resolve_rateç­‰æŒ‡æ ‡çš„å­—å…¸
    """
    if 'resolved' not in results:
        raise ValueError("Invalid results format: missing 'resolved' field")

    resolved_count = results['resolved']['count']
    total_count = len(results['instance_id'])

    resolve_rate = resolved_count / total_count if total_count > 0 else 0.0

    return {
        'resolve_rate': resolve_rate,
        'resolved_count': resolved_count,
        'total_count': total_count,
    }


# ===== ä½¿ç”¨ç¤ºä¾‹ =====
def main():
    """æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨evaluator bridge"""
    from step1_load_data import load_task

    print("=" * 80)
    print("Evaluator Bridge Demo")
    print("=" * 80)

    # æ¨¡æ‹Ÿï¼šåŠ è½½1ä¸ªä»»åŠ¡
    DATA_FILE = Path(__file__).parent.parent / "data" / "swebench" / "verified.jsonl"
    task = load_task(DATA_FILE, task_index=0)

    # æ¨¡æ‹Ÿï¼šagentç”Ÿæˆçš„patch
    mock_patch = """diff --git a/astropy/modeling/separable.py b/astropy/modeling/separable.py
--- a/astropy/modeling/separable.py
+++ b/astropy/modeling/separable.py
@@ -242,7 +242,7 @@ def _cstack(left, right):
         cright = _coord_matrix(right, 'right', noutp)
     else:
         cright = np.zeros((noutp, right.shape[1]))
-        cright[-right.shape[0]:, -right.shape[1]:] = 1
+        cright[-right.shape[0]:, -right.shape[1]:] = right

     return np.hstack([cleft, cright])
"""

    # 1. å‡†å¤‡predictions.jsonl
    print("\n1. Preparing predictions.jsonl...")
    output_file = Path("predictions.jsonl")
    prepare_predictions([task], [mock_patch], output_file)

    # 2. æ‰“å°evaluatorä½¿ç”¨è¯´æ˜
    print_evaluator_instructions(
        predictions_file=output_file,
        swebench_file=DATA_FILE,
        log_dir=Path("logs")
    )

    print("\n" + "=" * 80)
    print("âœ… Demo complete!")
    print("=" * 80)

    print("\nğŸ“ Next steps:")
    print("   1. Run the official evaluator command shown above")
    print("   2. Wait for evaluation to complete")
    print("   3. Load results with: load_evaluation_results('logs/results.json')")
    print("   4. Calculate metrics with: calculate_metrics(results)")


if __name__ == "__main__":
    main()
