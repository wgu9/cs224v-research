#!/usr/bin/env python3
"""
Process Long Conversation - å¤„ç†é•¿cursorå¯¹è¯

This script takes a long chat log file (e.g., from Cursor), splits it into
logical query-answer pairs, and uses an LLM to generate metadata and a
`goal.json` for each pair. The output is a structured session directory
in `data/1_sessions/` that is ready for Q1 analysis.

**LLM Usage**: âœ… YES - MULTIPLE LLM CALLS
  Step 1a: âŒ NO LLM - Regex-based cursor header extraction
  Step 1b: âœ… LLM CALL - Extract session metadata (1 call per session)
  Step 2:  âŒ NO LLM - Regex-based conversation splitting
  Step 3:  âœ… LLM CALLS - For each query-answer pair (N queries Ã— 2 calls):
           - Extract pair metadata (objective, task_type, etc.)
           - Generate goal.json (allowed_paths, required_tests, etc.)

**Total LLM Calls**: 1 + (N_queries Ã— 2)
  Example: 10 queries = 1 + 20 = 21 LLM calls

**Usage**: Run with runner.sh to ensure correct PYTHONPATH
  ./runner.sh python tools/process_long_conversation.py cursor.md
"""

import sys
import json
import pathlib
import re
import time
from typing import List, Tuple, Optional, Dict, Any, Literal

# å¯¼å…¥LLMå®¢æˆ·ç«¯
from tools.llm_client import get_llm_client


# ============================================
# Type Definitions (matching TypeScript)
# ============================================

# TaskTypeæšä¸¾ï¼ˆä¸types/cursor-chat/pair.tsä¿æŒä¸€è‡´ï¼‰
TaskType = Literal['code', 'doc', 'test', 'debug', 'refactor', 'config', 'qa', 'other']

VALID_TASK_TYPES = ['code', 'doc', 'test', 'debug', 'refactor', 'config', 'qa', 'other']


def validate_task_type(task_type: str) -> str:
    """
    éªŒè¯å¹¶ä¿®æ­£task_type

    Args:
        task_type: LLMè¿”å›çš„task_type

    Returns:
        æœ‰æ•ˆçš„task_typeï¼ˆå¦‚æœæ— æ•ˆåˆ™è¿”å›'other'ï¼‰
    """
    if task_type in VALID_TASK_TYPES:
        return task_type
    else:
        print(f"   âš ï¸  Invalid task_type '{task_type}', defaulting to 'other'")
        return 'other'


# ============================================
# LLM Prompts - Loaded from files
# ============================================

def load_prompt(filename: str) -> str:
    """
    ä»tools/prompts/ç›®å½•åŠ è½½promptæ–‡ä»¶

    Args:
        filename: promptæ–‡ä»¶åï¼ˆå¦‚ 'extract_session_metadata.txt'ï¼‰

    Returns:
        promptæ–‡æœ¬å†…å®¹
    """
    prompt_dir = pathlib.Path(__file__).parent / 'prompts'
    prompt_path = prompt_dir / filename

    if not prompt_path.exists():
        raise FileNotFoundError(f"Prompt file not found: {prompt_path}")

    return prompt_path.read_text(encoding='utf-8')


# åŠ è½½prompts
EXTRACT_SESSION_METADATA_PROMPT = load_prompt('extract_session_metadata.txt')
EXTRACT_PAIR_METADATA_PROMPT = load_prompt('extract_pair_metadata.txt')
GENERATE_GOAL_FROM_PAIR_PROMPT = load_prompt('generate_goal_from_pair.txt')


# ============================================
# Cursor Export Header Extraction
# ============================================

def extract_cursor_export_header(full_md: str) -> Dict[str, Any]:
    """
    æå–Cursorå¯¼å‡ºæ–‡ä»¶çš„å¤´éƒ¨ä¿¡æ¯

    Cursorå¯¼å‡ºçš„markdownæ ¼å¼ï¼š
    ç¬¬1è¡Œï¼š# <æ ‡é¢˜>
    ç¬¬2è¡Œï¼š_Exported on <date> at <time> <timezone> from Cursor (<version>)_

    Args:
        full_md: å®Œæ•´å¯¹è¯å†…å®¹

    Returns:
        åŒ…å«conversation_title, exported_datetime, cursor_versionçš„å­—å…¸
    """
    lines = full_md.split('\n')
    header_info = {}

    # æå–æ ‡é¢˜ï¼ˆç¬¬1è¡Œï¼‰
    if len(lines) > 0:
        first_line = lines[0].strip()
        if first_line.startswith('# '):
            header_info['conversation_title'] = first_line[2:].strip()

    # æå–å¯¼å‡ºä¿¡æ¯ï¼ˆç¬¬2è¡Œï¼‰
    if len(lines) > 1:
        second_line = lines[1].strip()
        # æ ¼å¼ï¼š_Exported on 10/25/2025 at 20:26:15 PDT from Cursor (1.7.53)_
        export_pattern = r'_Exported on (.+?) at (.+?) (.+?) from Cursor \((.+?)\)_'
        match = re.match(export_pattern, second_line)

        if match:
            date_str = match.group(1)      # "10/25/2025"
            time_str = match.group(2)      # "20:26:15"
            timezone = match.group(3)      # "PDT"
            version = match.group(4)       # "1.7.53"

            # è§£æä¸ºISO8601æ ¼å¼
            try:
                from datetime import datetime

                # è§£ææ—¥æœŸå’Œæ—¶é—´
                dt_str = f"{date_str} {time_str}"
                dt = datetime.strptime(dt_str, "%m/%d/%Y %H:%M:%S")

                # å¤„ç†æ—¶åŒºï¼ˆç®€åŒ–ï¼šPDT=-07:00, PST=-08:00, å…¶ä»–é»˜è®¤UTCï¼‰
                tz_offsets = {
                    'PDT': '-07:00',  # Pacific Daylight Time
                    'PST': '-08:00',  # Pacific Standard Time
                    'EDT': '-04:00',  # Eastern Daylight Time
                    'EST': '-05:00',  # Eastern Standard Time
                    'CDT': '-05:00',  # Central Daylight Time
                    'CST': '-06:00',  # Central Standard Time
                    'MDT': '-06:00',  # Mountain Daylight Time
                    'MST': '-07:00',  # Mountain Standard Time
                    'UTC': '+00:00',
                }
                tz_offset = tz_offsets.get(timezone, '+00:00')

                # æ ¼å¼åŒ–ä¸ºISO8601
                exported_datetime = dt.strftime("%Y-%m-%dT%H:%M:%S") + tz_offset
                header_info['exported_datetime'] = exported_datetime

            except Exception as e:
                # è§£æå¤±è´¥ï¼Œä¿å­˜åŸå§‹å­—ç¬¦ä¸²
                header_info['exported_datetime'] = f"{date_str} {time_str} {timezone}"

            header_info['cursor_version'] = version

    return header_info


# ============================================
# Conversation Splitting
# ============================================

def split_conversation(full_md: str) -> List[Tuple[int, str, str]]:
    """
    æ‹†åˆ†é•¿å¯¹è¯ä¸ºå¤šä¸ªquery-answer pairs

    Args:
        full_md: å®Œæ•´å¯¹è¯çš„markdownå†…å®¹

    Returns:
        List of (query_index, user_message, assistant_messages)
        query_indexä»0å¼€å§‹
    """
    pairs = []
    lines = full_md.split('\n')

    current_user = None
    current_assistant = []
    query_idx = 0

    i = 0
    while i < len(lines):
        line = lines[i]

        # æ£€æµ‹åˆ°æ–°çš„Useræ¶ˆæ¯
        if re.match(r'\*\*User\*\*', line, re.IGNORECASE):
            # ä¿å­˜å‰ä¸€ä¸ªpair
            if current_user is not None:
                pairs.append((
                    query_idx,
                    current_user.strip(),
                    '\n\n'.join(current_assistant)
                ))
                query_idx += 1

            # å¼€å§‹æ–°çš„pair
            current_user = ''
            current_assistant = []
            i += 1

            # æ”¶é›†useræ¶ˆæ¯ç›´åˆ°é‡åˆ°assistant
            while i < len(lines) and not re.match(
                r'\*\*(?:Cursor|Claude|Assistant)\*\*',
                lines[i],
                re.IGNORECASE
            ):
                current_user += lines[i] + '\n'
                i += 1
            continue

        # æ£€æµ‹åˆ°Assistantæ¶ˆæ¯
        if re.match(r'\*\*(?:Cursor|Claude|Assistant)\*\*', lines[i], re.IGNORECASE):
            i += 1
            assistant_msg = ''
            # æ”¶é›†assistantæ¶ˆæ¯ç›´åˆ°é‡åˆ°ä¸‹ä¸€ä¸ªUser
            while i < len(lines) and not re.match(r'\*\*User\*\*', lines[i], re.IGNORECASE):
                assistant_msg += lines[i] + '\n'
                i += 1
            current_assistant.append(assistant_msg.strip())
            continue

        i += 1

    # ä¿å­˜æœ€åä¸€ä¸ªpair
    if current_user is not None:
        pairs.append((query_idx, current_user.strip(), '\n\n'.join(current_assistant)))

    return pairs


# ============================================
# LLM Extraction Functions
# ============================================

def extract_session_metadata(full_md: str, llm_client) -> Dict[str, Any]:
    """
    ä»å®Œæ•´å¯¹è¯æå–session metadata

    Args:
        full_md: å®Œæ•´å¯¹è¯å†…å®¹
        llm_client: LLMå®¢æˆ·ç«¯å®ä¾‹

    Returns:
        Session metadataå­—å…¸
    """
    # å–å‰2000è¡Œ + å500è¡Œä½œä¸ºæ ·æœ¬
    lines = full_md.split('\n')
    sample = '\n'.join(lines[:2000] + ['\n...(omitted middle)...\n'] + lines[-500:])

    print("   ğŸ¤– Calling LLM to extract session metadata...")

    try:
        meta = llm_client.generate_json(
            system_prompt=EXTRACT_SESSION_METADATA_PROMPT,
            user_prompt=f"Analyze this conversation:\n\n{sample}",
            max_tokens=1000,
            temperature=0.1
        )

        # ç”Ÿæˆsession_idå¦‚æœä¸å­˜åœ¨æˆ–æ ¼å¼ä¸å¯¹
        if not meta.get('session_id') or not meta['session_id'].startswith('s_'):
            # ä½¿ç”¨å‹å¥½çš„æ—¶é—´æˆ³æ ¼å¼: YYYY-MM-DD-HH-MM-SS
            timestamp = time.strftime("%Y-%m-%d-%H-%M-%S", time.localtime())
            meta['session_id'] = f"s_{timestamp}_cursor"

        return meta

    except Exception as e:
        print(f"   âš ï¸  LLM extraction failed, using defaults: {e}")
        # è¿”å›é»˜è®¤å€¼ï¼Œä½¿ç”¨å‹å¥½çš„æ—¶é—´æˆ³æ ¼å¼
        timestamp = time.strftime("%Y-%m-%d-%H-%M-%S", time.localtime())
        return {
            "session_id": f"s_{timestamp}_cursor",
            "source": "cursor",
            "start_datetime": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
            "total_queries": len(re.findall(r'\*\*User\*\*', full_md, re.IGNORECASE)),
            "total_turns": len(re.findall(r'\*\*(?:User|Cursor|Assistant|Claude)\*\*', full_md, re.IGNORECASE)),
            "model": "auto",
            "max_mode": "No",
            "project_context_llm": "Unknown",
            "overall_objective_llm": "Unknown",
            "tags_llm": []
        }


def extract_pair_metadata(
    pair_content: str,
    query_index: int,
    session_id: str,
    llm_client
) -> Dict[str, Any]:
    """
    ä»å•ä¸ªQA pairæå–metadata

    Args:
        pair_content: pairçš„å®Œæ•´å†…å®¹ï¼ˆå¯èƒ½åŒ…å«å‰ä¸€ä¸ªpairçš„ä¸Šä¸‹æ–‡ï¼‰
        query_index: queryåºå·ï¼ˆ1-basedï¼‰
        session_id: session ID
        llm_client: LLMå®¢æˆ·ç«¯å®ä¾‹

    Returns:
        Pair metadataå­—å…¸
    """
    print(f"   ğŸ¤– Calling LLM to extract pair metadata for Q{query_index}...")

    try:
        # é™åˆ¶é•¿åº¦ï¼Œé¿å…è¶…è¿‡tokené™åˆ¶
        content_sample = pair_content[:4000]

        meta = llm_client.generate_json(
            system_prompt=EXTRACT_PAIR_METADATA_PROMPT,
            user_prompt=f"Analyze this query-answer pair:\n\n{content_sample}",
            max_tokens=1000,
            temperature=0.1
        )

        # è¡¥å……å­—æ®µ
        meta['query_index'] = query_index
        meta['session_id'] = session_id
        meta['pair_id'] = f"{session_id}_q{query_index:02d}"

        # éªŒè¯task_type_llm
        if 'task_type_llm' in meta:
            meta['task_type_llm'] = validate_task_type(meta['task_type_llm'])

        return meta

    except Exception as e:
        print(f"   âš ï¸  LLM extraction failed, using defaults: {e}")
        # è¿”å›é»˜è®¤å€¼
        return {
            "pair_id": f"{session_id}_q{query_index:02d}",
            "session_id": session_id,
            "query_index": query_index,
            "objective_llm": f"Query {query_index}",
            "task_type_llm": "code",
            "related_files_llm": [],
            "is_followup_llm": False,
            "has_context_dependency_llm": False
        }


def generate_goal_for_pair(
    pair_content: str,
    session_id: str,
    query_index: int,
    llm_client
) -> Dict[str, Any]:
    """
    ä¸ºå•ä¸ªpairç”Ÿæˆgoal.json

    Args:
        pair_content: pairçš„å†…å®¹ï¼ˆå¯èƒ½åŒ…å«ä¸Šä¸‹æ–‡ï¼‰
        session_id: session ID
        query_index: queryåºå·
        llm_client: LLMå®¢æˆ·ç«¯å®ä¾‹

    Returns:
        goal.jsonå­—å…¸
    """
    print(f"   ğŸ¯ Calling LLM to generate goal.json for Q{query_index}...")

    try:
        # é™åˆ¶é•¿åº¦
        content_sample = pair_content[:6000]

        goal = llm_client.generate_json(
            system_prompt=GENERATE_GOAL_FROM_PAIR_PROMPT,
            user_prompt=f"Generate goal.json for this query-answer pair:\n\n{content_sample}",
            max_tokens=1500,
            temperature=0.1
        )

        # è®¾ç½®run_id
        goal['run_id'] = f"{session_id}_q{query_index:02d}"

        # ç¡®ä¿å¿…éœ€å­—æ®µå­˜åœ¨
        if 'checkpoints' not in goal:
            goal['checkpoints'] = ["reproduce", "modify", "test", "regress"]

        return goal

    except Exception as e:
        print(f"   âš ï¸  Goal generation failed, using defaults: {e}")
        # è¿”å›é»˜è®¤goal
        return {
            "run_id": f"{session_id}_q{query_index:02d}",
            "objective": f"Query {query_index}",
            "allowed_paths": ["**"],
            "checkpoints": ["reproduce", "modify", "test", "regress"]
        }


# ============================================
# Utility Functions
# ============================================

def sanitize_filename(s: str) -> str:
    """æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤éæ³•å­—ç¬¦"""
    s = re.sub(r'[^\w\s-]', '', s)
    s = re.sub(r'[-\s]+', '_', s)
    return s[:50].lower()


# ============================================
# Main Processing Function
# ============================================

def process_long_conversation(
    full_md_path: str,
    output_dir: str = "data/1_sessions"
) -> str:
    """
    å¤„ç†é•¿å¯¹è¯çš„å®Œæ•´æµç¨‹

    Args:
        full_md_path: å®Œæ•´å¯¹è¯markdownæ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½• (é»˜è®¤: data/1_sessions)

    Returns:
        session_id
    """
    print(f"ğŸš€ Processing long conversation: {full_md_path}")
    print("="*80)

    # è¯»å–å®Œæ•´å¯¹è¯
    full_md = pathlib.Path(full_md_path).read_text(encoding='utf-8')
    print(f"ğŸ“„ Loaded conversation: {len(full_md)} characters, {len(full_md.splitlines())} lines")

    # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
    print("\nğŸ”§ Initializing LLM client...")
    try:
        llm_client = get_llm_client()
        print(f"âœ… LLM client initialized: {llm_client.base_url}")
    except Exception as e:
        print(f"âŒ Failed to initialize LLM client: {e}")
        print("ğŸ’¡ Make sure you have .env file with LLM_API_KEY and LLM_API_ENDPOINT")
        sys.exit(1)

    # Step 1a: æå–Cursorå¯¼å‡ºheaderä¿¡æ¯
    print("\nğŸ“„ Step 1a: Extracting Cursor export header...")
    header_info = extract_cursor_export_header(full_md)
    if header_info.get('conversation_title'):
        print(f"âœ… Conversation Title: {header_info['conversation_title']}")
    if header_info.get('exported_datetime'):
        print(f"   Exported: {header_info['exported_datetime']}")
    if header_info.get('cursor_version'):
        print(f"   Cursor Version: {header_info['cursor_version']}")

    # Step 1b: æå–session metadataï¼ˆé€šè¿‡LLMï¼‰
    print("\nğŸ“Š Step 1b: Extracting session metadata (via LLM)...")
    session_meta = extract_session_metadata(full_md, llm_client)

    # åˆå¹¶headerä¿¡æ¯åˆ°session metadataï¼ˆheaderä¼˜å…ˆï¼Œå› ä¸ºæ˜¯ç¡®å®šæ€§æå–ï¼‰
    # headerå­—æ®µï¼šconversation_title, exported_datetime, cursor_version
    # è¿™äº›å­—æ®µä¸ä¼šä¸LLMç”Ÿæˆçš„å­—æ®µå†²çªï¼ˆLLMå­—æ®µéƒ½æœ‰_llmåç¼€ï¼‰
    for key, value in header_info.items():
        if key in session_meta:
            print(f"   âš ï¸  Warning: Header field '{key}' overwrites LLM field")
        session_meta[key] = value

    session_id = session_meta['session_id']

    print(f"âœ… Session ID: {session_id}")
    print(f"   Model: {session_meta.get('model', 'auto')}")
    print(f"   Max Mode: {session_meta.get('max_mode', 'No')}")
    print(f"   Project: {session_meta.get('project_context_llm', 'N/A')}")
    print(f"   Objective: {session_meta.get('overall_objective_llm', 'N/A')}")

    # åˆ›å»ºç›®å½•ç»“æ„
    session_dir = pathlib.Path(output_dir) / session_id
    (session_dir / 'raw').mkdir(parents=True, exist_ok=True)
    (session_dir / 'pairs').mkdir(parents=True, exist_ok=True)

    # ä¿å­˜åŸå§‹å¯¹è¯
    (session_dir / 'raw' / 'full_conversation.md').write_text(full_md, encoding='utf-8')

    # Step 2: æ‹†åˆ†å¯¹è¯
    print("\nğŸ”ª Step 2: Splitting conversation into query-answer pairs...")
    pairs = split_conversation(full_md)
    print(f"âœ… Found {len(pairs)} query-answer pairs")

    # Step 3: å¤„ç†æ¯ä¸ªpair
    print(f"\nğŸ“ Step 3: Processing {len(pairs)} query-answer pairs...")
    pair_metas = []
    prev_pair_content = None

    for idx, (q_idx, user_msg, assistant_msg) in enumerate(pairs):
        print(f"\n{'='*60}")
        print(f"Processing Q{idx+1}/{len(pairs)} (index={q_idx})")
        print(f"{'='*60}")

        # æ„å»ºpairå†…å®¹
        pair_content = f"**User**\n{user_msg}\n\n**Assistant**\n{assistant_msg}"

        # å¦‚æœä¸æ˜¯ç¬¬ä¸€ä¸ªpairï¼Œé™„åŠ å‰ä¸€ä¸ªpairä½œä¸ºä¸Šä¸‹æ–‡
        if idx > 0 and prev_pair_content:
            pair_content_with_context = f"""## Previous Query-Answer:
{prev_pair_content[:500]}...

---

## Current Query-Answer:
{pair_content}
"""
        else:
            pair_content_with_context = pair_content

        # æå–pair metadata
        pair_meta = extract_pair_metadata(
            pair_content_with_context,
            idx + 1,  # 1-based index
            session_id,
            llm_client
        )

        print(f"   Objective: {pair_meta['objective_llm']}")
        print(f"   Task Type: {pair_meta['task_type_llm']}")
        print(f"   Is Followup: {pair_meta.get('is_followup_llm', False)}")
        print(f"   Has Context Dependency: {pair_meta.get('has_context_dependency_llm', False)}")

        # åˆ›å»ºqueryç›®å½•ï¼ˆèšåˆç»“æ„ï¼špairs/q01/ï¼‰
        query_dir = session_dir / 'pairs' / f"q{idx+1:02d}"
        query_dir.mkdir(parents=True, exist_ok=True)

        # ä¿å­˜chat.mdï¼ˆå›ºå®šåç§°ï¼‰
        chat_path = query_dir / 'chat.md'

        # æ ¹æ®æ˜¯å¦æœ‰ä¸Šä¸‹æ–‡ä¾èµ–ï¼Œå†³å®šä¿å­˜çš„å†…å®¹
        if pair_meta.get('has_context_dependency_llm') and prev_pair_content:
            final_content = f"""# Query {idx+1}: {pair_meta['objective_llm']}

## Context from Previous Query

{prev_pair_content[:500]}...

---

## Current Query-Answer

{pair_content}
"""
        else:
            final_content = f"""# Query {idx+1}: {pair_meta['objective_llm']}

{pair_content}
"""

        chat_path.write_text(final_content, encoding='utf-8')
        pair_meta['markdown_path'] = str(chat_path.relative_to(session_dir))

        # ç”Ÿæˆgoal.jsonï¼ˆä¿å­˜åœ¨åŒä¸€ç›®å½•ï¼‰
        # æ ¹æ®has_context_dependency_llmå†³å®šè¾“å…¥
        goal_input = pair_content_with_context if pair_meta.get('has_context_dependency_llm') else pair_content
        goal = generate_goal_for_pair(goal_input, session_id, idx + 1, llm_client)

        goal_path = query_dir / 'goal.json'
        goal_path.write_text(json.dumps(goal, indent=2, ensure_ascii=False), encoding='utf-8')
        pair_meta['goal_json_path'] = str(goal_path.relative_to(session_dir))

        pair_metas.append(pair_meta)
        prev_pair_content = pair_content

        print(f"   âœ… Saved query dir: pairs/q{idx+1:02d}/")
        print(f"      - chat.md")
        print(f"      - goal.json")

    # Step 4: ä¿å­˜session metadataå’Œpairsåˆ—è¡¨
    print(f"\nğŸ’¾ Step 4: Saving session metadata...")
    session_meta['total_queries'] = len(pairs)
    session_meta['total_turns'] = sum(1 for _ in re.finditer(r'\*\*(?:User|Cursor|Assistant|Claude)\*\*', full_md, re.IGNORECASE))

    (session_dir / 'session.json').write_text(
        json.dumps(session_meta, indent=2, ensure_ascii=False),
        encoding='utf-8'
    )
    print(f"   âœ… Saved session.json")

    (session_dir / 'pairs.json').write_text(
        json.dumps(pair_metas, indent=2, ensure_ascii=False),
        encoding='utf-8'
    )
    print(f"   âœ… Saved pairs.json")

    # Summary
    print(f"\n{'='*80}")
    print(f"âœ… SESSION PROCESSED SUCCESSFULLY")
    print(f"{'='*80}")
    print(f"ğŸ“ Output Directory: {session_dir}")
    print(f"ğŸ†” Session ID: {session_id}")
    print(f"ğŸ“Š Statistics:")
    print(f"   - Total Queries: {len(pairs)}")
    print(f"   - Total Turns: {session_meta['total_turns']}")
    print(f"   - Followup Queries: {sum(1 for p in pair_metas if p.get('is_followup_llm'))}")
    print(f"   - Context Dependent: {sum(1 for p in pair_metas if p.get('has_context_dependency_llm'))}")
    print(f"\nğŸ“ Task Types:")
    task_types = {}
    for p in pair_metas:
        t = p.get('task_type_llm', 'unknown')
        task_types[t] = task_types.get(t, 0) + 1
    for task_type, count in sorted(task_types.items()):
        print(f"   - {task_type}: {count}")

    print(f"\nğŸš€ Next Steps:")
    print(f"   1. Review session.json and pairs.json")
    print(f"   2. Run Q1 batch analysis:")
    print(f"      python tools/run_q1_batch.py {session_dir}")
    print(f"   ")
    print(f"   Or manually analyze each query:")
    print(f"      cd {session_dir}")
    print(f"      for query_dir in pairs/q*/; do")
    print(f'        query_id=$(basename "$query_dir")')
    print(f'        echo "Analyzing $query_id..."')
    print(f"        # Use goal.json and chat.md from $query_dir")
    print(f"      done")

    return session_id


# ============================================
# Command Line Interface
# ============================================

def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    if len(sys.argv) < 2:
        print("Usage: python process_long_conversation.py <full_conversation.md> [output_dir]")
        print("\nExample:")
        print("  python process_long_conversation.py spot-test/cursor_chat.md")
        print("  python process_long_conversation.py spot-test/cursor_chat.md data/my_sessions")
        print("\nRequires: .env file with LLM_API_KEY and LLM_API_ENDPOINT")
        sys.exit(1)

    full_md_path = sys.argv[1]
    output_dir = sys.argv[2] if len(sys.argv) > 2 else "data/1_sessions"

    if not pathlib.Path(full_md_path).exists():
        print(f"âŒ Error: File not found: {full_md_path}")
        sys.exit(1)

    try:
        session_id = process_long_conversation(full_md_path, output_dir)
        print(f"\nğŸ‰ Done! Session ID: {session_id}")
    except Exception as e:
        print(f"\nâŒ Error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
