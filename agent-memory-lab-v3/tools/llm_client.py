#!/usr/bin/env python3
"""
LLM Client - é€šç”¨çš„LLM APIå®¢æˆ·ç«¯

æ”¯æŒOpenAIå…¼å®¹çš„APIï¼ˆåŒ…æ‹¬è‡ªéƒ¨ç½²çš„æ¨¡å‹ï¼‰
ä».envæ–‡ä»¶è¯»å–é…ç½®ï¼šLLM_API_KEY å’Œ LLM_API_ENDPOINT
"""

import requests
import os
import json
from dotenv import load_dotenv
from typing import Optional, Dict, Any


class LLMClient:
    """
    LLMå®¢æˆ·ç«¯ï¼Œå°è£…APIè°ƒç”¨é€»è¾‘
    """

    def __init__(self, api_key: Optional[str] = None, base_url: Optional[str] = None):
        """
        åˆå§‹åŒ–LLMå®¢æˆ·ç«¯

        Args:
            api_key: APIå¯†é’¥ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
            base_url: APIç«¯ç‚¹ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
        """
        # åŠ è½½.envæ–‡ä»¶
        load_dotenv(".env")

        # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„å‚æ•°ï¼Œå¦åˆ™ä»ç¯å¢ƒå˜é‡è¯»å–
        self.api_key = api_key or os.getenv("LLM_API_KEY")
        self.base_url = base_url or os.getenv("LLM_API_ENDPOINT")

        if not self.api_key:
            raise ValueError("LLM_API_KEY not found. Set it in .env or pass it to LLMClient()")
        if not self.base_url:
            raise ValueError("LLM_API_ENDPOINT not found. Set it in .env or pass it to LLMClient()")

        # ç§»é™¤base_urlæœ«å°¾çš„æ–œæ 
        self.base_url = self.base_url.rstrip('/')

    def chat_completion(
        self,
        messages: list,
        model: str = "gpt-4.1",
        max_tokens: int = 2000,
        temperature: float = 0.1,
        timeout: int = 60
    ) -> Dict[str, Any]:
        """
        è°ƒç”¨chat completion API

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨ï¼Œæ ¼å¼ï¼š[{"role": "user", "content": "..."}]
            model: æ¨¡å‹åç§°
            max_tokens: æœ€å¤§tokenæ•°
            temperature: æ¸©åº¦å‚æ•°
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

        Returns:
            APIå“åº”çš„JSONå­—å…¸

        Raises:
            requests.RequestException: APIè°ƒç”¨å¤±è´¥
        """
        response = requests.post(
            f"{self.base_url}/chat/completions",
            headers={
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            },
            json={
                "model": model,
                "messages": messages,
                "max_tokens": max_tokens,
                "temperature": temperature
            },
            timeout=timeout
        )

        if response.status_code != 200:
            raise requests.RequestException(
                f"API call failed with status {response.status_code}: {response.text}"
            )

        return response.json()

    def generate_json(
        self,
        system_prompt: str,
        user_prompt: str,
        model: str = "gpt-4.1",
        max_tokens: int = 2000,
        temperature: float = 0.1
    ) -> dict:
        """
        ç”ŸæˆJSONæ ¼å¼çš„å“åº”

        Args:
            system_prompt: System prompt
            user_prompt: User prompt
            model: æ¨¡å‹åç§°
            max_tokens: æœ€å¤§tokenæ•°
            temperature: æ¸©åº¦å‚æ•°

        Returns:
            è§£æåçš„JSONå­—å…¸

        Raises:
            json.JSONDecodeError: å“åº”ä¸æ˜¯æœ‰æ•ˆçš„JSON
        """
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]

        response = self.chat_completion(
            messages=messages,
            model=model,
            max_tokens=max_tokens,
            temperature=temperature
        )

        # æå–content
        content = response['choices'][0]['message']['content'].strip()

        # ç§»é™¤å¯èƒ½çš„markdownä»£ç å—
        if content.startswith("```"):
            lines = content.split('\n')
            # ç§»é™¤ç¬¬ä¸€è¡Œå’Œæœ€åä¸€è¡Œï¼ˆä»£ç å—æ ‡è®°ï¼‰
            if len(lines) > 2:
                content = '\n'.join(lines[1:-1])

        # è§£æJSON
        return json.loads(content)

    def generate_text(
        self,
        system_prompt: str,
        user_prompt: str,
        model: str = "gpt-4.1",
        max_tokens: int = 2000,
        temperature: float = 0.7
    ) -> str:
        """
        ç”Ÿæˆçº¯æ–‡æœ¬å“åº”

        Args:
            system_prompt: System prompt
            user_prompt: User prompt
            model: æ¨¡å‹åç§°
            max_tokens: æœ€å¤§tokenæ•°
            temperature: æ¸©åº¦å‚æ•°

        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬å†…å®¹
        """
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]

        response = self.chat_completion(
            messages=messages,
            model=model,
            max_tokens=max_tokens,
            temperature=temperature
        )

        return response['choices'][0]['message']['content']


# ä¾¿æ·å‡½æ•°
def get_llm_client() -> LLMClient:
    """
    è·å–LLMå®¢æˆ·ç«¯å®ä¾‹ï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®ï¼‰

    Returns:
        LLMClientå®ä¾‹
    """
    return LLMClient()


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("ğŸ§ª Testing LLM Client...")

    try:
        client = get_llm_client()
        print(f"âœ… Client initialized")
        print(f"   Endpoint: {client.base_url}")

        # æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ
        print("\nğŸ“ Testing text generation...")
        text = client.generate_text(
            system_prompt="You are a helpful assistant.",
            user_prompt="Say hello in one sentence.",
            max_tokens=50
        )
        print(f"âœ… Response: {text}")

        # æµ‹è¯•JSONç”Ÿæˆ
        print("\nğŸ“Š Testing JSON generation...")
        json_response = client.generate_json(
            system_prompt="You are a JSON generator. Always respond with valid JSON.",
            user_prompt='Generate a JSON object with fields: name (string), age (number), hobbies (array of strings)',
            max_tokens=100
        )
        print(f"âœ… JSON Response: {json.dumps(json_response, indent=2)}")

    except Exception as e:
        print(f"âŒ Test failed: {e}")
