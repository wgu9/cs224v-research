# Adaptive Memory System for Coding Agents

**CS224V Project | 6 Weeks | Jeremy Gu**

---

## Part 1. Executive Summary

### Yucheng's Research Direction
**Oct 22 Meeting Consensus**: Agent memory systems that (1) learn across sessions, (2) improve from feedback, and (3) decontextualize specific experiences into transferable patterns. His key insight: **"abstraction level needs to be adjusted on the fly—nobody is doing that."**

### Key Research Questions

Current coding agents (AutoCodeRover, SWE-agent, Devin) fail at three critical tasks:

#### **Q1: How can agents maintain goal alignment during multi-step tasks?**
- **Problem**: Agents drift from objectives—attempting unnecessary refactoring, modifying wrong files, skipping required tests
- **Our Solution**: Four-Guard System monitors scope, plan adherence, test coverage, and evidence tracking
- **Sub-tasks**: 
  - Parse goals from problem statements into structured objectives
  - Real-time drift detection with weighted scoring
  - Action blocking when drift_score ≥ 0.8
- **Success Metric**: Drift rate < 15% (vs baseline 28%)

#### **Q2: How can agents extract and reuse patterns across sessions?**
- **Problem**: Each task solved from scratch—no learning from past successes
- **Our Solution**: Decontextualize solutions into generalizable patterns, store in searchable library, retrieve and apply to similar tasks
- **Sub-tasks**:
  - Extract patterns from successful sessions using LLM
  - Store patterns at multiple abstraction levels (hint/explanation/code)
  - Semantic retrieval based on task similarity
- **Success Metrics**: Pattern reuse ≥ 30%, Time savings ≥ 30%

#### **Q3: How should abstraction levels adapt dynamically?**
- **Problem**: One-size-fits-all responses don't match task/agent context
- **Our Solution**: Dynamic selection of detail level based on task complexity, pattern familiarity, and agent performance
- **Context Adaptation** (adjusted from original vision):
  - **Original**: User expertise (novice vs expert)
  - **SWE-bench Adaptation**: Task complexity + Pattern familiarity + Agent history
  - **Rationale**: SWE-bench is automated benchmark without interactive users
  - **Core Novelty Preserved**: Dynamic abstraction adjustment (Yucheng's "nobody is doing")
- **Sub-tasks**:
  - Extract context features (task complexity, pattern confidence, agent state)
  - Implement selection logic (rule-based or learned)
  - Measure efficiency gain: dynamic vs fixed abstraction
- **Success Metric**: Dynamic outperforms fixed in efficiency without sacrificing success rate

### Our Contribution

**Novel**: First coding agent system with dynamic abstraction adjustment based on context. While Yucheng emphasized user expertise, we adapt to task/agent context—both instances of context-aware abstraction selection that current systems lack.

**Why This Matters**: 
- Academic: Rigorous evaluation on SWE-bench (200 tasks) with statistical significance vs SOTA baselines
- Industrial: Proven time savings (30%+), reduced drift (15% vs 28%), quantifiable ROI

### Expected Results

**Overall Target**: Resolve rate ≥ 30% (vs AutoCodeRover 20%, SWE-agent 13%, Vanilla GPT-4 8%)

**Component Targets**:
- Q1: Drift rate < 15%, False positive < 30%
- Q2: Pattern reuse ≥ 30%, Time savings ≥ 30%
- Q3: Efficiency gain > 10% (dynamic vs fixed)

---

## Part 2. Alignment with Yucheng's Research Direction

### Yucheng's Three Core Requirements

#### **1. Cross-Session Memory** ✅ Fully Aligned
- **Requirement**: Store and retrieve knowledge across conversations
- **Our Implementation**: Pattern library stores decontextualized solutions from all successful tasks
- **Example Flow**:
  - Task 1: Fix null pointer in `payment.py` → Extract pattern "null_check_before_access"
  - Task 50: Issue mentions user object errors → Retrieve null check pattern → Apply to new context
- **Validation**: Pattern reuse rate ≥ 30% on 200 test tasks

#### **2. Feedback-Driven Learning** ⚠️ Adapted for SWE-bench
- **Original Requirement**: Learn from test outcomes AND human feedback
- **Our Adaptation**: 
  - **Test outcomes**: Full support—pass/fail from SWE-bench test suites
  - **Human feedback**: Not available in automated benchmark
  - **Alternative feedback**: Pattern success rate across tasks serves as implicit feedback
- **Justification**: SWE-bench provides objective, reproducible evaluation. User feedback requires user study (Phase 2, post-class)
- **Validation**: Pattern confidence updates based on success rate across usage

#### **3. Decontextualization → Recontextualization** ✅ Fully Aligned
- **Requirement**: Transform specific examples → general patterns → apply to new contexts
- **Our Implementation**:
  - **Decontextualize**: `payment.py line 45 null check` → `"Objects need null validation before attribute access"`
  - **Generalize**: Pattern applicable to payment, user, order, transaction domains
  - **Recontextualize**: When new task mentions user object, suggest null check pattern with code template
- **Validation**: Pattern transfers to different repos/domains (coverage ≥ 40% of test tasks)

### Yucheng's Key Insight: Dynamic Abstraction

**"Abstraction level needs to be adjusted on the fly—nobody is doing that."**

#### Original Vision vs Our Adaptation

| Dimension | Yucheng's Original Vision | Our SWE-bench Adaptation |
|-----------|--------------------------|--------------------------|
| **Adapt to** | User expertise (novice vs expert) | Task complexity + Agent experience |
| **Feedback source** | User reactions ("too detailed") | Test outcomes + Pattern success history |
| **Abstraction levels** | Hint / Explanation / Code | **Same** ✅ |
| **Selection mechanism** | User modeling over sessions | Context-based selection + Learning |
| **Learning** | From user feedback | From pattern usage outcomes |

#### Why This Adaptation Preserves Novelty

**Core Contribution Unchanged**: Dynamic adjustment of abstraction level based on context

- **What's the same**: Multiple levels, dynamic selection, learning optimal levels from outcomes
- **What's different**: Context is task/agent instead of user
- **Why it's still novel**: Nobody is doing dynamic abstraction in coding agents—whether for users or tasks

**Research Question Evolution**:
- Original: "How to adapt to user expertise?"
- Adapted: "How to adapt to task/agent context?"
- **Both address**: Context-aware abstraction vs one-size-fits-all

#### Future Work: User Study

Phase 2 (post-class) will validate original user-adaptation vision:
- Recruit novice/intermediate/expert developers
- Have them use system on real tasks
- Collect feedback on abstraction preferences
- Validate that expertise-based adaptation improves satisfaction

**This project establishes technical foundation**. User study extends contribution to full original vision.

---

## Part 3. System Design

### High-Level Architecture

```
GitHub Issue (SWE-bench)
    ↓
┌─────────────────────────────────────────┐
│ Goal Parser (LLM)                       │
│ Extract: objective, allowed_paths,      │
│          forbidden_actions, tests       │
└─────────────────────────────────────────┘
    ↓
┌─────────────────────────────────────────┐
│ Pattern Retrieval (Vector Search)       │
│ Query: Task description                 │
│ Return: Top-3 relevant patterns         │
└─────────────────────────────────────────┘
    ↓
┌─────────────────────────────────────────┐
│ Abstraction Selector (Q3)               │
│ Context: task complexity + familiarity  │
│ Output: Level 1/2/3                     │
└─────────────────────────────────────────┘
    ↓
┌─────────────────────────────────────────┐
│ Agent Execution Loop                    │
│   For each action:                      │
│   1. Generate action                    │
│   2. Check alignment (Four-Guard) ←─ Q1 │
│   3. If drift_score ≥ 0.8 → Block       │
│   4. Execute action                     │
│   5. Observe results                    │
└─────────────────────────────────────────┘
    ↓
┌─────────────────────────────────────────┐
│ Test Execution (SWE-bench harness)      │
│ Run: FAIL_TO_PASS tests                 │
│ Result: Pass / Fail                     │
└─────────────────────────────────────────┘
    ↓ (if Pass)
┌─────────────────────────────────────────┐
│ Pattern Extraction (LLM) ←────────── Q2 │
│ Input: Task + Solution + Tests          │
│ Output: Decontextualized pattern        │
│ Store: hint/explanation/code versions   │
└─────────────────────────────────────────┘
```

### Three Core Components

#### **1. Goal-Aware Tracking (Q1)** - Built on Existing Work

**Input**: GitHub issue problem statement  
**Output**: Structured goal + Real-time drift monitoring  
**Function**: Prevent agent from straying off-task

**Four-Guard System** (85-90% code already implemented):

| Guard | What It Checks | Weight | Detection Logic |
|-------|----------------|--------|----------------|
| **Scope Guard** | File modification scope | 0.4 | Is file in `allowed_paths`? Not in `forbidden_paths`? |
| **Plan Guard** | Phase/tool matching | 0.3 | Is tool allowed in current phase? (e.g., no edit during reproduce) |
| **Test Guard** | Test sufficiency | 0.2 | Did agent run all `required_tests`? |
| **Evidence Guard** | Modification traceability | 0.1 | Does code change have supporting evidence? |

**Drift Score Calculation**:
```
drift_score = 0.4×scope + 0.3×plan + 0.2×test + 0.1×evidence

Action Decision:
- drift_score < 0.5  → OK (proceed)
- 0.5 ≤ drift_score < 0.8 → WARN
- drift_score ≥ 0.8 → BLOCK (prevent action)
```

**Existing Assets**: 34 unit tests, chat2events extraction, events2guards scoring

#### **2. Cross-Session Pattern Learning (Q2)**

**Input**: Successful session (task + solution + test results)  
**Output**: Generalizable pattern stored at multiple levels  
**Function**: Extract reusable knowledge, accelerate future tasks

**Pattern Card Schema**:
```json
{
  "id": "null_check_before_access",
  "name": "Null Check Before Access",
  "problem_signature": "NullPointerError / AttributeError on None",
  "solution_approach": "Add null validation before object access",
  
  "level_1_hint": "Check for None before using object",
  "level_2_explanation": "When object might be None, add validation: if obj is None: raise/return",
  "level_3_code": "if user is None:\n    raise ValueError('User required')\nuser.process()",
  
  "times_used": 0,
  "success_rate": 0.0,
  "applicable_domains": ["payment", "user", "order"],
  "embedding": [0.1, 0.2, ...]
}
```

**Workflow**:
1. **Extraction**: LLM analyzes successful session → Identifies core pattern
2. **Decontextualization**: Remove specifics (payment.py → objects in general)
3. **Multi-level Generation**: Create hint/explanation/code for same pattern
4. **Storage**: Vector DB (embeddings) + Metadata (success stats)
5. **Retrieval**: Semantic search based on new task description
6. **Application**: Inject pattern at selected abstraction level

#### **3. Dynamic Abstraction Engine (Q3)** - Conditional on Week 3

**Input**: Pattern + Task context + Agent state  
**Output**: Abstraction level (1=hint, 2=explanation, 3=code)  
**Function**: Choose appropriate detail based on context

**Context Features**:
- **Task complexity**: Estimated from problem statement length, repo size, files mentioned
- **Pattern familiarity**: How many times agent has seen this pattern (0=first, 5+=familiar)
- **Agent performance**: Recent success rate on similar tasks

**Selection Logic** (simplified version):
```
IF task_complexity = hard OR pattern_familiarity = first_time:
    → Level 3 (detailed code)
ELIF task_complexity = simple AND pattern_familiarity = familiar:
    → Level 1 (concise hint)
ELSE:
    → Level 2 (explanation)
```

**Learning** (if time permits Week 4-5):
- Track: (pattern_id, level_used, outcome)
- Analyze: Which levels work best for which contexts
- Update: Adjust selection rules based on empirical success

---

## Part 4. Implementation Architecture

### Data: SWE-bench Lite

**Dataset**: 300 real GitHub issues with ground truth solutions  
**Split Strategy**:
- **Train**: 50 tasks (extract initial patterns, tune thresholds)
- **Val**: 50 tasks (Week 3 checkpoint validation)
- **Test**: 200 tasks (final evaluation)

**What SWE-bench Provides**:
- `problem_statement`: Natural language issue description
- `repo`: GitHub repository at specific commit
- `base_commit`: Starting code state
- `patch`: Ground truth solution (used ONLY for evaluation, not generation)
- `test_patch`: Test suite
- `FAIL_TO_PASS`: Tests that must pass for success

**Why SWE-bench**:
- ✅ Standard benchmark (reproducible results)
- ✅ Real-world complexity (not toy problems)
- ✅ Ground truth for objective evaluation
- ✅ Community recognition (Stanford, CMU use it)
- ✅ SOTA baselines available for comparison

### System Components

#### **Infrastructure (Week 0-1)**
```
Components:
├─ PostgreSQL: Sessions, actions, patterns metadata
├─ ChromaDB: Pattern embeddings for semantic search
├─ SWE-bench harness: Task loading + Docker test execution
└─ Cost tracking: Monitor API usage per task
```

#### **Q1: Goal Alignment Module (Week 1-2)**
```
Pipeline:
1. Goal Parser
   Input: problem_statement
   Process: LLM extraction → structured JSON
   Output: goal.json
   Model: GPT-4o (accuracy critical)
   Cost: ~$0.02/task

2. Action Logger
   Input: Agent execution trace
   Process: Record tool, file, phase, timestamp
   Output: events.jsonl
   Cost: Free (local logging)

3. Four-Guard Drift Detector
   Input: events.jsonl + goal.json
   Process: Rule-based scoring (no LLM)
   Output: guards.jsonl + drift_score per action
   Cost: Free (rules only)
   
4. Drift Blocker
   Input: drift_score
   Process: If ≥ 0.8 → prevent action execution
   Output: Blocked action + explanation
   Cost: Free
```

**Existing Code Reuse**: 85-90% from Q1 MVP (Four-Guard System, events schema)

#### **Q2: Pattern Learning Module (Week 2-3)**
```
Pipeline:
1. Success Identifier
   Input: Session results
   Filter: tests_passed AND drift_rate < 25%
   Output: High-quality session candidates
   
2. Pattern Extractor
   Input: Task + Solution trace + Tests
   Process: LLM summarization
   Output: Pattern JSON (problem/solution/levels)
   Model: GPT-4o (quality critical)
   Cost: ~$0.05/pattern

3. Pattern Store
   Input: Pattern JSON
   Process: Generate embedding, store metadata
   Output: Vector DB entry + SQL record
   Cost: ~$0.0001/pattern (embedding)

4. Semantic Retrieval
   Input: New task description
   Process: Embed query, cosine similarity search
   Output: Top-3 relevant patterns
   Cost: ~$0.0001/query

5. Pattern Application
   Input: Pattern + Abstraction level
   Process: Inject hint/explanation/code into agent context
   Output: Enhanced prompt for code generation
```

#### **Q3: Dynamic Abstraction Module (Week 4 - Conditional)**
```
Pipeline:
1. Context Feature Extractor
   Input: Task + Pattern + Agent state
   Process: Estimate complexity, check familiarity
   Output: Context vector
   Cost: Free (heuristics)

2. Abstraction Selector
   Input: Context vector
   Process: Rule-based decision (or learned if time)
   Output: Level 1/2/3
   Cost: Free

3. Usage Tracker (optional)
   Input: (pattern_id, level, outcome)
   Process: Record for analysis
   Output: Statistics for learning
```

**Conditional Execution**: Only if Week 3 checkpoint passes (pattern reuse ≥ 20%)

#### **Agent Execution Loop (Week 2-4)**
```
For each SWE-bench task:
1. Parse goal (Q1)
2. Retrieve patterns (Q2)
3. Select abstraction level (Q3 if enabled)
4. Generate solution with pattern guidance
   Model: Qwen-32B local (code gen) or GPT-4o
   Cost: $0 (local) or ~$0.20/task (API)
5. Check each action for drift (Q1)
6. Execute if drift_score < 0.8
7. Run tests (SWE-bench harness)
8. If pass: Extract pattern (Q2)
```

### Technology Stack

| Component | Technology | Reason |
|-----------|-----------|--------|
| **Language** | Python 3.10+ | SWE-bench compatibility |
| **LLM API** | OpenAI (GPT-4o, 4o-mini) | Goal parsing, pattern extraction |
| **Local LLM** | Qwen-32B (Ollama) | High-volume code generation (cost savings) |
| **Vector DB** | ChromaDB | Lightweight, embeddings-first |
| **Relational DB** | PostgreSQL | Session/pattern metadata |
| **Embeddings** | text-embedding-3-small | OpenAI, cost-effective |
| **SWE-bench** | Docker + pytest | Standard harness |
| **Version Control** | Git | Track code, reproducibility |

### Cost Management Strategy

**Problem**: All-API approach costs ~$280 for 300 tasks (exceeds budget)

**Solution**: Hybrid API + Local

| Task Type | Model | Frequency | Cost/Task | Total (300 tasks) |
|-----------|-------|-----------|-----------|-------------------|
| **Goal parsing** | GPT-4o API | 1x | $0.02 | $6 |
| **Pattern extraction** | GPT-4o API | 0.3x (only successes) | $0.05 | $4.50 |
| **Code generation** | Qwen-32B Local | 3x | $0 | $0 |
| **Drift checking** | Qwen-32B Local | 10x | $0 | $0 |
| **Embeddings** | text-embed-3-small | 5x | $0.0001 | $0.15 |
| **Total** | | | | **~$15-20** ✅ |

**Rationale**:
- Critical tasks (goal parsing, pattern extraction) → API (accuracy matters)
- High-volume tasks (code gen, drift check) → Local (Qwen-32B on your 5090)
- Savings: 75% cost reduction vs all-API

**Hardware**: RTX 5090 (32GB VRAM) can run Qwen-32B-Q4 at ~25 tokens/s

---

## Part 5. Evaluation & Demonstration

### Evaluation Framework

#### **Primary Metrics** (P0 - Must Achieve)

| Metric | Target | Measurement | Baseline |
|--------|--------|-------------|----------|
| **Resolve Rate** | ≥ 30% | Tests pass on test set (200 tasks) | AutoCodeRover: 20%<br>SWE-agent: 13%<br>GPT-4: 8% |
| **Drift Rate** | < 15% | % actions flagged by Four-Guard | Baseline (no tracking): 28% |
| **Pattern Reuse Rate** | ≥ 30% | % tasks where pattern retrieved & used | No-memory baseline: 0% |
| **Time Savings** | ≥ 30% | (time_without_pattern - time_with) / time_without | - |

#### **Secondary Metrics** (P1 - Strong Results)

| Metric | Target | Purpose |
|--------|--------|---------|
| **False Positive Rate** | < 30% | Ensure drift detection not overly strict |
| **Pattern Coverage** | ≥ 40% | % test tasks with relevant pattern available |
| **Actions per Task** | < 15 | Efficiency (fewer actions = more focused) |
| **Q3 Efficiency Gain** | > 10% | Dynamic vs fixed abstraction |

#### **Tertiary Metrics** (P2 - Nice to Have)

| Metric | Purpose |
|--------|---------|
| **Learning Curve Slope** | Prove improvement over time |
| **Pattern Quality Score** | Human rating of top-10 patterns |
| **Drift Recovery Rate** | % times agent recovers after drift |

### Statistical Rigor

#### **Required Tests** (for publication quality)

1. **T-tests**: Your system vs each baseline (p < 0.05 for significance)
2. **Effect Size**: Cohen's d > 0.5 (medium effect)
3. **Confidence Intervals**: Bootstrap 95% CI for resolve rate
4. **Ablation Study**: Prove each component (Q1/Q2/Q3) contributes

#### **Ablation Design**

| System Variant | Components Active | Purpose |
|----------------|-------------------|---------|
| Full System | Q1 + Q2 + Q3 | Best performance |
| Q1 + Q2 | Goal + Pattern, no abstraction | Isolate Q3 contribution |
| Q1 Only | Goal tracking only | Isolate Q1 contribution |
| Q2 Only | Pattern learning only | Isolate Q2 contribution |
| Baseline | None | Vanilla GPT-4 |

Run each variant on 100 tasks, compare resolve rates.

### Error Analysis

**Manual Analysis** (50 failed tasks):
- Categorize: Inherent difficulty / Goal parsing error / Pattern mismatch / Drift false positive / Other
- Identify: Common failure patterns
- Suggest: Improvements for future work

### Demonstration Plan

#### **Live Demo Components** (5 minutes)

**Setup**: CLI tool running on sample SWE-bench tasks

**Flow**:
1. **Goal Tracking** (1 min)
   - Show: Parse goal from GitHub issue
   - Show: Agent attempts to modify wrong file → Drift detector blocks
   - Explain: Four-Guard scoring

2. **Pattern Learning** (1.5 min)
   - Show: Agent solves null pointer bug
   - Show: System extracts "null_check_before_access" pattern
   - Show: Pattern stored at 3 levels (hint/explanation/code)

3. **Pattern Reuse** (1.5 min)
   - Show: New task mentions user object error
   - Show: System retrieves null check pattern
   - Show: Agent applies pattern → Quick success
   - Metrics: Time saved 40% vs solving from scratch

4. **Dynamic Abstraction** (1 min)
   - Show: Same pattern applied to two different tasks
   - Task A (complex): Gets Level 3 (full code)
   - Task B (simple): Gets Level 1 (hint only)
   - Explain: Context-based selection

**Demo Assets**:
- Pre-recorded backup video (in case live fails)
- CLI with colored output for clarity
- Slides showing architecture diagram

#### **Supplementary Materials**

- **GitHub Repo**: Code, README, reproduction instructions
- **Results Dashboard**: Metrics visualization (Jupyter notebook)
- **Paper Draft**: 4-6 pages, workshop format (ICLR/NeurIPS)

### Success Criteria

| Level | Criteria |
|-------|----------|
| **Minimum (B grade)** | - Resolve ≥ 20% (100 tasks)<br>- Pattern reuse ≥ 15%<br>- Drift detection working<br>- Demo shows all components |
| **Strong (A grade)** | - Resolve ≥ 30% (200 tasks)<br>- Pattern reuse ≥ 30%<br>- Drift < 15%<br>- Statistical significance (p<0.05)<br>- Complete ablations |
| **Exceptional (A+)** | - Beat AutoCodeRover (>20%)<br>- All metrics hit targets<br>- Publication-ready paper<br>- Open-source release |

---

## Part 6. Timeline & Milestones

### Phase-by-Phase Plan

#### **Week 0: Preparation** (Before Week 1)
- [ ] **Alignment with Yucheng** `P0 Critical`
  - Email explaining Q3 context shift (user → task/agent)
  - Get explicit approval before proceeding
- [ ] **Environment Setup**
  - Load SWE-bench Lite, test on 5 tasks
  - Setup cost tracking
  - Test Qwen-32B quality vs GPT-4o
- [ ] **Decision**: API-only vs Hybrid (choose hybrid if Qwen ≥ 80% quality)

#### **Week 1-2: Foundation** (Q1 + Q2 Core)

**Week 1 Focus**: Goal Tracking
- [ ] Goal Parser implementation (GPT-4o)
- [ ] Action Logger (from agent trace)
- [ ] Four-Guard System adaptation to SWE-bench
- [ ] Test on 10 tasks

**Week 2 Focus**: Pattern Learning
- [ ] Pattern Extractor (LLM-based)
- [ ] Pattern Store (ChromaDB + embeddings)
- [ ] Semantic Retrieval
- [ ] Test on 20-30 tasks

**Week 2 Milestone**:
- ✅ Goal parsing accuracy > 80% (manual check 10 examples)
- ✅ Drift detection working (can identify obvious drift)
- ✅ Pattern extraction quality (manual review 10 patterns)

#### **Week 3: Checkpoint** ⚠️⚠️⚠️ **CRITICAL DECISION POINT**

**Activities**:
- [ ] Run complete Q1+Q2 system on 50 tasks
- [ ] Measure: Pattern reuse rate, Drift rate, Resolve rate
- [ ] Compare: With baseline (vanilla GPT-4)

**Success Criteria** (Week 3 Targets - Conservative):
```
✅ Pattern reuse ≥ 20%
✅ Drift rate < 25%
✅ Resolve rate ≥ 12% (vs baseline 8%)
✅ False positive < 30%
```

**Go/No-Go Decision**:

| Outcome | Action |
|---------|--------|
| **All targets met** | ✅ Proceed with Q3 (dynamic abstraction) |
| **3/4 targets met** | ⚠️ Continue Q1+Q2, simplified Q3 (fixed rules only) |
| **<3 targets met** | ❌ Debug Q2 (pattern learning), skip Q3 |

**Why This Matters**: If pattern learning doesn't work, Q3 (dynamic abstraction) is meaningless. Week 3 validates core functionality before investing in advanced features.

#### **Week 4: Advanced Features** (Conditional)

**If Week 3 ✅ Success**:
- [ ] Q3 Implementation:
  - Context feature extraction
  - Abstraction selector (rule-based)
  - Fixed-level baselines
  - Dynamic vs fixed comparison
- [ ] Q1 Validation:
  - False positive analysis
  - Weight tuning if needed

**If Week 3 ⚠️ Partial**:
- [ ] Focus: Debug Q2 pattern quality
- [ ] Simplified Q3: Fixed abstraction rules (no learning)
- [ ] Stretch: Multi-level storage only

**If Week 3 ❌ Fail**:
- [ ] Pivot: Q1-only project (drift detection as core)
- [ ] Or: Reduce task difficulty (easier subset of SWE-bench)

**Week 4 Milestone**:
- ✅ Q3 implemented (if checkpoint passed)
- ✅ Initial A/B testing (dynamic vs fixed)

#### **Week 5: Evaluation Week**

**Monday-Tuesday**: Compute-Intensive
- [ ] Run full system on 200 test tasks (~8-10 hours)
- [ ] Run all baselines (Vanilla GPT-4, Static RAG)
- [ ] Run ablations (Q1 only, Q2 only, Q1+Q2, Full)

**Wednesday**: Statistical Analysis
- [ ] T-tests: Your system vs baselines
- [ ] Effect sizes (Cohen's d)
- [ ] Confidence intervals (bootstrap)
- [ ] Generate statistical report

**Thursday**: Error Analysis
- [ ] Manual review: 50 failed tasks
- [ ] Categorize failure types
- [ ] Identify improvement opportunities

**Friday**: Figures & Tables
- [ ] Main results table
- [ ] Ablation table
- [ ] Learning curves
- [ ] Error distribution chart

**Week 5 Deliverable**: Complete evaluation package with publication-quality results

#### **Week 6: Polish & Deliverables**

**Monday-Tuesday**: Demo
- [ ] 5-min demo video
- [ ] CLI tool polished
- [ ] Pre-recorded backup
- [ ] Practice presentation

**Wednesday-Thursday**: Paper
- [ ] Draft: Abstract, Intro, Method, Results
- [ ] Figures integrated
- [ ] Discussion + Future Work
- [ ] Proofread

**Friday**: Final Polish
- [ ] GitHub repo cleanup
- [ ] README with reproduction steps
- [ ] Code comments
- [ ] Final deliverables check

**Week 6 Deliverable**: Complete project package ready for submission

### Risk Management

| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|-----------|
| **Week 3 checkpoint fails** | Medium | High | Prepared contingency: Q1-only or simplified Q3 |
| **Resolve rate < 15%** | Medium | High | Option: Reduce task difficulty (easier subset) |
| **Q3 context shift rejected by Yucheng** | Low | Critical | **Week 0 email for early alignment** |
| **API cost exceeds budget** | High | Medium | Hybrid API+Local strategy (already planned) |
| **Pattern quality insufficient** | Low | Medium | Human-in-loop validation for first 10 patterns |
| **Compute time too long** | Low | Low | Use faster model or reduce test set to 150 |

### Weekly Time Allocation

| Week | Hours | Focus |
|------|-------|-------|
| 0 | 10h | Setup + Yucheng alignment |
| 1 | 20h | Q1 implementation |
| 2 | 25h | Q2 implementation |
| 3 | 25h | Checkpoint evaluation + Decision |
| 4 | 20h | Q3 (if GO) or Q2 refinement |
| 5 | 30h | Full evaluation (compute + analysis) |
| 6 | 20h | Demo + Paper + Polish |
| **Total** | **150h** | ~25h/week average |

### Dependencies & Critical Path

```
Critical Path (must complete in order):
Week 0: Yucheng approval
    ↓
Week 1-2: Q1 (Goal) + Q2 (Pattern) implementation
    ↓
Week 3: Checkpoint evaluation
    ↓ (if pass)
Week 4: Q3 implementation
    ↓
Week 5: Full evaluation
    ↓
Week 6: Deliverables

Can Parallelize:
- Week 1-2: Q1 and Q2 initial development
- Week 5: Baselines + Ablations can run concurrently
```

### Contingency Plans

#### **If Week 3 Checkpoint Fails**

**Scenario A: Pattern reuse < 20%**
- Debug: Check pattern extraction quality (manual review)
- Debug: Check retrieval threshold (too strict?)
- Adjust: Lower reuse target to 15% or choose easier tasks
- Action: Allocate Week 4 to fixing Q2, skip Q3

**Scenario B: Drift rate > 25%**
- Debug: Analyze false positives
- Adjust: Relax thresholds or lower weights
- Action: Keep Q1 functional but not central claim

**Scenario C: Resolve rate < 10%**
- Major pivot: Focus on drift detection only (Q1-only project)
- Or: Switch to easier task subset
- Or: Lower final target to 20% (still above baseline 8%)

#### **If Yucheng Rejects Q3 Adaptation**

**Option 1**: Add small user study component (Week 6)
- 5 developers × 10 tasks each
- Collect feedback on abstraction preferences
- Preliminary validation of user-adaptation

**Option 2**: Reframe Q3 entirely
- Focus only on multi-level storage (Q2 extension)
- Skip dynamic selection
- Claim: "Foundation for future user adaptation"

**Option 3**: Accept Q1+Q2 only scope
- Still 2 novel contributions
- Still publishable (workshop)
- Q3 becomes explicit future work

---

## Appendices

### A. Comparison with Related Work

| System | Drift Detection | Pattern Learning | Dynamic Abstraction | SWE-bench Evaluation |
|--------|----------------|------------------|--------------------|--------------------|
| **AutoCodeRover** | ❌ No | ⚠️ Retrieval only | ❌ No | ✅ 19.6% |
| **SWE-agent** | ❌ No | ❌ No | ❌ No | ✅ 12.8% |
| **Devin** | ⚠️ Implicit | ⚠️ Proprietary | ❌ No | ✅ 13.9% |
| **Reflexion** | ❌ No | ⚠️ Self-reflection | ❌ No | ❌ Not tested |
| **Voyager** | ❌ No | ✅ Skill library | ❌ No | ❌ Not tested (Minecraft) |
| **Our System** | ✅ Four-Guard | ✅ Decontextualization | ✅ Context-aware | ✅ Target: 30% |

**Key Differentiators**:
1. **Only system with real-time drift blocking** (not just detection)
2. **Only system with dynamic abstraction** (Yucheng's gap)
3. **Comprehensive evaluation on SWE-bench** (vs SOTA baselines)

### B. Cost Breakdown Detail

| Component | Model | Input Tokens | Output Tokens | Cost/Call | Calls (300 tasks) | Total |
|-----------|-------|--------------|---------------|-----------|-------------------|-------|
| Goal Parser | GPT-4o | 1000 | 500 | $0.02 | 300 | $6.00 |
| Pattern Extract | GPT-4o | 2000 | 800 | $0.05 | 90 | $4.50 |
| Code Gen (API) | GPT-4o | 1500 | 1000 | $0.20 | 0 | $0.00 |
| Code Gen (Local) | Qwen-32B | - | - | $0.00 | 900 | $0.00 |
| Drift Check | Qwen-32B | - | - | $0.00 | 3000 | $0.00 |
| Embeddings | text-embed-3 | 200 | - | $0.0001 | 1500 | $0.15 |
| **Total** | | | | | | **$10.65** |

**Buffer**: 50% overhead for dev/debug → Final estimate: **$15-20** ✅ Within budget

### C. Data Splits Detail

**SWE-bench Lite Task Selection**:

| Phase | # Tasks | Difficulty | Purpose |
|-------|---------|-----------|---------|
| **Train** | 50 | Mixed (25 simple, 20 medium, 5 hard) | Extract initial patterns, tune weights |
| **Val** | 50 | Mixed | Week 3 checkpoint validation |
| **Test** | 200 | Mixed (66 simple, 100 medium, 34 hard) | Final evaluation |

**Difficulty Criteria**:
- Simple: 1-2 files, < 50 lines changed, clear bug fix
- Medium: 3-5 files, 50-200 lines, moderate complexity
- Hard: 5+ files, > 200 lines, or architectural changes

### D. Q3 Context Shift Justification

**Why This Adaptation Is Still Novel**:

1. **Core mechanism unchanged**: Dynamic selection based on context features
2. **Learning unchanged**: System improves from outcomes
3. **Multi-level unchanged**: Hint/Explanation/Code storage
4. **Gap addressed**: "Nobody is doing dynamic abstraction" applies to task-based too
5. **Research contribution**: How to select abstraction level dynamically (method is novel)

**Comparison with Static Systems**:
- AutoCodeRover: Always returns full code (Level 3)
- SWE-agent: Always returns command + explanation (Level 2)
- Our system: Adapts level 1/2/3 based on context

**This is still "nobody is doing"** ✅
