# 面向编程代理的自适应记忆系统（中文翻译）
CS224V 项目提案 | Jeremy Gu（原文中文全译 + 校对备注）

---

## 动机（Motivation）

延续我们 10 月 22 日关于“具备（1）跨会话学习，（2）从反馈中改进，（3）将经验去上下文化、可迁移化”的代理记忆系统讨论。你的核心洞见是：需要“按需动态调整抽象层级（abstraction level）——几乎没有人在做这件事”。

---

## 三个研究问题（Three Research Questions）

当前的编码代理（如 Claude Code、Cursor、GitHub Copilot）在三项关键能力上存在缺失：

### Q1：如何在多步任务中保持目标对齐（goal alignment）？

问题：代理容易偏离最初目标——对 10 个真实编码会话的初步分析显示，代理常修改超出预期范围的文件，或跳过验证步骤。

解决思路：多维度对齐检查
- 作用域（Scope）：文件修改是否落在预期范围？
- 计划（Plan）：当前阶段使用的工具是否匹配？
- 测试（Test）：是否执行了必要的验证步骤？
- 证据（Evidence）：修改是否有支撑性理由和证据？

机制：跟踪对齐违规；在置信度不足时阻断动作（或给出警示/回滚建议）。

在 SWE‑bench 上的评估：
- 度量：修改操作中，落入金标准补丁（gold patch）作用域的比例
- 基线：未受监控的代理表现
- 目标：在建立基线（第 3 周）后确定

---

### Q2：如何从多次会话中抽取并复用模式（patterns）？

问题：每个任务都“从零开始”——跨会话没有知识累积。

解决思路：模式抽取与检索

流程：
1. 抽取（Extract）：LLM 分析成功任务 → 识别可复用模式
2. 去上下文化（Decontextualize）：将具体解法转化为一般方法
3. 多层级存储（Store）：
   - 层级 1：高层思路（例：“先做输入校验”）
   - 层级 2：概念性解释 + 关键步骤
   - 层级 3：可落地的代码模板
4. 检索（Retrieve）：新任务与问题签名匹配时语义检索
5. 应用（Apply）：将相关模式注入代理上下文

在 SWE‑bench 上的评估：
- 模式复用率（Pattern Reuse Rate）：检索并使用模式的任务占比
- 使用模式的成功率（Success with Pattern）：使用模式 vs 未使用模式的解决率（Resolve）对比
- 覆盖度（Coverage）：测试集中有可用相关模式的任务占比
- 目标：证明“有模式可用”与“更高成功率”正相关

---

### Q3：抽象层级应如何动态自适应？

问题：静态响应无法匹配多样需求——有的场景需要详细步骤，有的只需简洁提示。

解决思路：基于上下文的抽象层级选择

可用上下文信号（均可由 SWE‑bench 执行过程观测得到）：
- 任务指标：问题描述长度、提及文件数量
- 模式因素：该模式类型是首次出现还是重复出现
- 执行状态：历史尝试次数、近期动作成功率

选择方法：
- 起步策略：基于规则的启发式
- 若时间允许（第 4 周）：基于结果学习“何时选择何种层级”

在 SWE‑bench 上的评估：
- 主要指标：动态选择 vs 固定层级 的解决率对比
- 次要分析：不同任务类型下，何种层级更有效
- 目标：动态 ≥ 固定（任何提升即可验证方法有效）

创新点：契合你的洞见——“动态抽象”基本空白。已有系统（如 AutoCodeRover、SWE‑agent）多为固定检索深度或固定模板。

---

## 数据与评测（Data & Evaluation）

数据集：SWE‑bench Lite（300 个真实 GitHub issues）

SWE‑bench 提供：
- 来自流行 Python 项目的真实缺陷报告
- 金标准修复（仅用于评估）
- 测试套件（客观的通过/失败判定）

SWE‑bench 不提供：
- 交互式用户反馈（无真实用户在环）
- 显式任务难度标签（需我们估计）
- 时间度量（自动化执行场景下）

我们的划分：
- 训练（50）：抽取初始模式、建立基线
- 验证（50）：第 3 周检查点——决定是否推进 Q3
- 测试（200）：最终评估

---

## 指标（Metrics）

主要指标（Primary Metric）：解决率（Resolve Rate）
- 定义：所有必需测试均通过的任务比例
- 当前 SOTA（示例参考）：AutoCodeRover ≈19–22%（Lite），SWE‑agent ≈12.5%（Full）
- 我们的目标：≥20%（对齐 SOTA），进取目标：≥25%

Q1 指标：
- 作用域对齐：编辑操作中，落在 gold patch 作用域内的比例
- 基线：第 1 周以“未受监控的代理”运行建立
- 目标：较基线改善（具体数值待定）

Q2 指标：
- 模式复用率：检索到模式的任务占比
- 使用模式的成功率：使用模式 vs 未使用模式 的解决率差异
- 目标：达到统计显著（t 检验，p<0.05）

Q3 指标：
- 动态 vs 固定：解决率对比
- 层级分布：各抽象层级的使用频率分析
- 目标：动态 ≥ 最优的固定层级策略

统计要求：
- 显著性检验（t 检验，p<0.05）
- 效应量（Cohen's d）

---

## 关键概念（Core Concepts）

记忆（Memory）
- 定义：系统将以往任务执行中获得的知识进行存储、检索与应用，以提升未来任务表现的能力。
- 在本项目中的含义：跨会话模式存储——从成功解法中抽取模式，去上下文化，使其在未来相似任务中可复用。
- 对比：当前代理几乎无长期记忆——每次任务都从零开始。

目标对齐（Goal Alignment）
- 定义：代理的行为与预期任务范围与计划的一致程度。
- 度量：通过四个加权维度合成 drift 分：
  - 作用域守卫（Scope，0.4）：文件修改是否在预期边界？
  - 计划守卫（Plan，0.3）：工具使用是否符合当前阶段？
  - 测试守卫（Test，0.2）：是否执行必要的验证步骤？
  - 证据守卫（Evidence，0.1）：修改是否有足够证据支撑？
- 漂移分计算：`drift_score = 0.4×scope + 0.3×plan + 0.2×test + 0.1×evidence`
- 动作决策：
  - `drift_score < 0.5` → OK（继续）
  - `0.5 ≤ drift_score < 0.8` → WARN（警示但允许）
  - `drift_score ≥ 0.8` → BLOCK（阻断执行）
- 漂移示例：
  - 任务：“修复登录校验以接受带 ‘+’ 的邮箱”
  - 预期范围：`login_validator.py`（1 文件，约 5 行变更）
  - 代理实际：修改 12 个文件、大幅重构认证系统、改动数据库结构、跳过测试
  - 结果：检测到目标漂移，阻断动作

多步任务（Multi-step Task）
- 定义：需要跨阶段顺序执行的任务（如 理解 → 复现 → 实现 → 验证 → 测试）。
- 在 SWE‑bench 中：每个 issue 平均需要 5–15 个代理动作，涉及读文件、编辑、运行测试、调试等。
- 为什么重要：单步任务不易出现目标漂移；多步任务更容易“跑偏”。

作用域（Scope，任务范围）
- 定义：与任务相关的文件、函数、测试用例集合。
- 在 SWE‑bench 中：由问题描述推断，并可用 gold patch 验证（仅评测时使用）。
- 组成：
  - `allowed_paths`：问题中提及或很可能相关的文件/目录
  - `forbidden_paths`：系统文件、配置、不相关模块
  - `required_tests`：判定成功必须通过的测试
- 示例：
  - 任务：“修复支付校验中的空指针错误”
  - 范围：`payment_processor.py, payment_validator.py, test_payment.py`
  - 非范围：`user_auth.py`, `database_config.py`
- 度量：编辑操作落在范围内的比例（亦可同时看文件级精确与目录类匹配两口径）

阶段（Phase，任务执行阶段）
- 定义：多步任务中的当前阶段。
- 标准阶段（借鉴调试方法学）：
  1. 理解（Understand）：读文件、搜索代码、分析问题
     - 允许工具：`read_file`、`grep`、`search`
     - 禁止工具：`edit_file`、`submit`
  2. 复现（Reproduce）：运行测试、观察失败
     - 允许工具：`run_test`、`bash`
     - 禁止工具：`edit_file`（未确认问题前不改动）
  3. 实现（Implement）：修改代码、提交变更
     - 允许工具：`edit_file`、`insert`、`replace`
     - 前置要求：需先运行过失败测试
  4. 验证（Verify）：运行测试、检查回归
     - 允许工具：`run_test`
     - 前置要求：至少进行过一次代码编辑
- 转移逻辑：阶段可部分重叠，但需遵守偏序（未理解不可直接实现）。
- 实践：阶段由动作历史推断，而非显式申明。

动作（Action）
- 定义：代理执行的原子操作。
- 类型（参考 SWE‑bench 工具集）：
  - 读取：`read_file(path)`、`search(pattern)`、`grep(query)`
  - 写入：`edit_file(path, old, new)`、`insert(path, line, content)`
  - 执行：`run_test(test_name)`、`bash(command)`
  - 控制：`submit(solution)`、`rollback()`、`checkpoint()`
- 元数据（每个动作记录）：
  - 时间戳、当前阶段（推断）、影响的文件、成功/失败结果
- 约束：每任务最多 100 次动作（SWE‑bench 常见实践），单动作 5 分钟超时（视脚手架而定）。
- 意义：Q1 漂移检测与 Q2 效率度量的基本单位。

模式（Pattern）
- 定义：从成功任务中抽取的可复用“问题—解法”对。
- 组成：
  - 问题签名：缺陷/问题类型（如“空指针异常”）
  - 解法路径：一般策略（如“访问前先做空值校验”）
  - 代码模板：可选，层级 3 提供可直接套用的实现示例
- 示例：当对象可能为 None 时，在访问属性前进行空值检查。

去上下文化（Decontextualize）
- 定义：将依赖具体上下文的解法转化为通用、可迁移的模式。
- 流程：去除具体变量名、文件路径、项目细节 → 保留核心逻辑与步骤。
- 例：
  - 具体：在 `payment_processor.py:45` 中，先判断 `customer.account is None`，为真则返回；随后再访问 `customer.account.debit(amount)`。
  - 去上下文化：在访问对象属性前，先校验对象不为 None，以避免 AttributeError。

抽取（Extract）
- 定义：用 LLM 从成功的任务轨迹中识别关键模式的自动化过程。
- 输入：完整任务轨迹（问题描述、采取的动作、最终解法、测试结果）
- 输出：结构化的模式（含问题签名、解法路径与多层抽象）
- 触发：仅在任务成功（全部测试通过）时执行。

抽象层级（Abstraction Level）
- 定义：呈现模式时的细节程度，从高层提示到具体实现。
- 三层：
  - 层级 1（Hint）：一句话提示（例：“访问前检查 None”）
  - 层级 2（Explanation）：概念性方案 + 关键步骤
  - 层级 3（Code）：完整实现模板与示例
- 选择依据：任务复杂度、模式熟悉度、代理近期表现（Q3 研究点）。

上下文（Context，用于抽象层级选择）
- 定义：关于当前任务与代理状态的可观测信号，用于决定采用何种抽象层级。
- 三类信号：
  1. 任务指标：问题描述长度、提及文件数量、仓库规模
  2. 模式因素：该模式类型的累计出现次数（0=首次，5+=熟悉）
  3. 执行状态：本任务的历史尝试次数、近期动作成功率
- 关键区别：均可由 SWE‑bench 执行日志观测得到，无需用户画像。
- 示例：
  - 高复杂 + 首次 + 近期成功率低 → 层级 3
  - 低复杂 + 熟悉 + 近期成功率高 → 层级 1

---

## 数据结构（Data Structures）

SWE‑bench 数据格式
- 定义：来自 12 个流行 Python 仓库的 2,294 个真实 GitHub issues。
- 关键字段：
  - `instance_id`：唯一标识（如“django__django-12453”）
  - `problem_statement`：GitHub issue 描述（用户报告的缺陷）
  - `repo`：仓库名与 URL
  - `base_commit`：修复前的基线提交哈希
  - `patch`：金标准修复（仅评估时使用，不暴露给代理）
  - `test_patch`：触发缺陷的测试文件
  - `PASS_TO_PASS` / `FAIL_TO_PASS`：定义成功的测试集合
- 提供价值：真实复杂度、客观测试评估、可复现环境
- 不足之处：无交互用户、无显式难度标签、无真实时间记录
- 适配原因：
  1) 客观评测（测试通过/失败）；2) 可复现（容器化环境）；3) 真实场景；4) 社区对比基线；5) 规模足够（Lite 的 300 任务）。

我们的数据划分
- 训练集（50）：用于抽取初始模式、调优守卫权重
- 验证集（50）：第 3 周检查点，决定是否推进 Q3
- 测试集（200）：最终评估

---

## 其他指标定义（节选，中文化）

模式复用率（Q2 主指标之一）
- 定义：检索到并（满足使用判定）使用模式的任务占比。
- 区分：检索到 ≠ 实际遵循。实际使用可通过事件与模式步骤/测试模板一致性来判定。

“使用模式的成功率”（Q2 主指标）
- 定义：有可用模式 vs 无可用模式 的解决率差异。
- 公式：`Resolve_rate(with_pattern) - Resolve_rate(without_pattern)`
- 统计：双样本 t 检验（p < 0.05）

模式覆盖度（Q2 次指标）
- 定义：测试集中，至少存在一条相似度阈值以上相关模式的任务占比（不要求实际使用）。
- 目标：≥40%

动作效率（Q2 次指标）
- 定义：平均动作数差异（有模式 vs 无模式），作为时间节省的代理指标。
- 目标：达到统计显著（t 检验，p<0.05）

动态 vs 固定效率（Q3 主指标）
- 定义：动态抽象选择 vs 三种固定层级（总是 L1/L2/L3）的解决率差异。
- 目标：动态 ≥ 最佳固定层级

层级分布（Q3 分析指标）
- 定义：在检索到模式的前提下，各抽象层级被选择的频率。
- 期望（假设）：L1 20–30%，L2 40–50%，L3 20–30%（随复杂度/熟悉度而变）

---

## 假设（Assumptions）

1. 模式可迁移：从训练任务抽取的模式对测试任务仍具参考价值（以语义相似度阈值≥0.7 控制）。
2. LLM 抽取质量：GPT‑4o 等可较可靠地识别可复用模式（第 2 周人工抽样 20 条校验）。
3. 守卫权重可调：四守卫默认 0.4/0.3/0.2/0.1 可在第 1–2 周用训练数据调参。
4. 上下文信号充分：问题长度、文件数、仓库规模等可合理近似任务复杂度。
5. 动作上限：绝大多数任务可在 50–100 个动作内完成（常见 SWE‑bench 实践）。
6. 测试可靠：SWE‑bench 测试套件可准确反映任务成功与否。
7. 代理指标有效：动作数与 token 用量可作为时间节省的合理代理。
8. 模式多样性：50 个训练任务可产出 10–20 类常见模式（空值检查、类型不匹配、边界条件、并发等），以抽样验证。
9. 检索阈值：余弦相似度 ≥0.7 合理（过低易引入不相关，过高易错过有益模式），在验证集上调优。
10. 单模式注入：同一任务仅注入 top‑1 模式（避免上下文过载）；多模式为未来工作。
11. 阶段推断：可用启发式从动作历史可靠推断当前阶段（第 1 周人工抽样 20 条校验）。

---

## 与现有工作的对比（Comparison with Existing Work）

| 系统 | 漂移监控（Drift Monitoring） | 跨会话记忆（Cross‑Session Memory） | 动态抽象（Dynamic Abstraction） |
|------|------------------------------|------------------------------------|-------------------------------|
| AutoCodeRover | ❌ | 仓内/静态检索 | ❌ |
| SWE‑agent | ❌ | ❌ | ❌ |
| Devin（专有） | 未知 | 未知 | ❌ |
| 我们（提案） | ✅ 多维守卫 | ✅ 去上下文化的模式卡 | ✅ 基于上下文的两档视图 |

差异化：据我们所知，在 SWE‑bench 类问题设置下，我们是首个显式统一三能（Q1 守卫、Q2 模式、Q3 动态抽象）的研究系统（以 2025‑10 为时间点的公开材料为准）。

---

## 预期贡献（Expected Contributions）

1. 机制：用于编码代理的多维度对齐监控（四守卫与漂移计分）
2. 框架：去上下文化、多层级存储的模式卡体系
3. 探索：基于上下文的抽象层级选择（“动态抽象”的首个系统化探索）
4. 评估：在标准基准上的严谨对比实验与报告

---

## 已知问题 / 限制（Open Questions / Limitations）

起始即知：
- 守卫权重最佳值：第 1–2 周调参验证
- 模式质量：依赖 LLM 抽取准确度（第 2 周人工校验）
- 抽象选择：先规则，后学习（视时间而定）
- 时间节省：基准自动化下无法测真实“人时”，以动作数/token 用量等代理

第 3 周待定：
- Q3 可行性取决于 Q2 结果
- 基线建立后再定精确目标数

未来工作：
- 用户研究：验证“按用户经验自适应抽象层级”的原始假设
- 多语言支持（当前以 Python 为主）

---

## 附：记号与格式（Notation）

- 粗体：被定义的关键术语
- 斜体：强调或引用概念
- 等宽：变量名、路径、函数名
- 百分比：以 100 为分母（如 20% = 20/100）

---

## 校对备注（明显不一致/需澄清处）

1) 解决率（Resolve）SOTA 数字与切分（split）
- 文中示例给出 AutoCodeRover ≈19.6%、SWE‑agent ≈12.8%。仓内其他笔记显示更稳妥口径为：ACR 19–22%（Lite），SWE‑agent ≈12.5%（Full）。建议统一为“ACR 19–22%（Lite）；SWE‑agent ≈12.5%（Full）”，并在正式文档加脚注注明时间与来源（论文/榜单）。

2) “BLOCK（阻断）”与当前实现的“rollback/建议动作”
- 文中 Q1 决策描述含“BLOCK”。当前实现产物多为建议动作（ok/warn/rollback），不执行真实阻断。建议在文档中对齐术语：BLOCK = rollback（建议回滚/阻断提交流程），或明确“仅记录建议，不强制执行”。

3) 阶段命名与代码实现的一致性
- 文中阶段为 Understand/Reproduce/Implement/Verify；而你们在 Q1 实现中常用 reproduce/modify/test/regress 四检查点。建议在 README/计划中补上“术语映射”，避免歧义。

4) 动作上限与超时
- 文中给出“每任务最多 100 动作、单动作 5 分钟”。这是不少脚手架的常见实践，但非 SWE‑bench 官方统一标准。建议标注为“依赖具体脚手架，数值可变”。

5) Plan/Evidence 在 SWE‑bench（无对话）下的可评估性
- 无对话时，Plan/Evidence 难以在 Chat 线评估。建议在评测口径中标注“NA/不可评估”或“合成最小证据”，避免将“缺数据”计为失败。

6) Q2“使用模式”的操作化判定
- 文中已有“复用率”“使用模式的成功率”指标，但“何谓使用”需操作化：建议按事件与模式步骤/测试模板一致性、或“守卫改善”来判断，并区分 retrieved_only / used_like / used_strict 三档。

7) 统计检验口径
- 仅使用 t 检验可能不足，建议同时给出 bootstrap 置信区间或 Mann‑Whitney U；多指标时做 FDR 控制。

（以上校对基于当前仓库实现与调研笔记；待联网后可进一步补充外部论文/榜单引用细节。）

