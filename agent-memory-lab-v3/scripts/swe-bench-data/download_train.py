#!/usr/bin/env python3
# è¯¥è„šæœ¬ä¸“é—¨ç”¨äºä¸‹è½½ SWE-bench è®­ç»ƒé›†ï¼ˆçº¦ 2,294 ä¸ªä»»åŠ¡ï¼‰ï¼Œç”¨äº Q2 çš„æ¨¡å¼æå–ä»»åŠ¡ã€‚
# ç”±äºæ•°æ®é‡è¾ƒå¤§ï¼Œå•ç‹¬æä¾›ä¸‹è½½è„šæœ¬ä¾¿äºæŒ‰éœ€ä¸‹è½½ã€‚
# è®­ç»ƒé›†ä¸»è¦ç”¨äºå­¦ä¹ å¸¸è§çš„ä»£ç ä¿®å¤æ¨¡å¼å’Œè§„å¾‹ã€‚
"""
Download SWE-bench train split for Q2 pattern extraction.
This is a separate script because it's larger (~2,294 tasks).
"""

import json
from pathlib import Path
from datasets import load_dataset

DATA_DIR = Path(__file__).parent.parent.parent / "data" / "swebench"
DATA_DIR.mkdir(parents=True, exist_ok=True)

def main():
    print("=" * 60)
    print("Downloading SWE-bench Train Split")
    print("=" * 60)
    print("\nThis will download ~2,294 tasks for Q2 pattern extraction.")
    print("Size: ~20-30MB, ETA: 1-2 minutes\n")

    # Load from HuggingFace
    print("ğŸ“¥ Loading from HuggingFace...")
    dataset = load_dataset("princeton-nlp/SWE-bench", split="train")

    # Save to JSONL
    output_file = DATA_DIR / "train.jsonl"
    print(f"ğŸ’¾ Saving to {output_file}...")

    with open(output_file, "w") as f:
        for i, item in enumerate(dataset):
            f.write(json.dumps(item) + "\n")
            if (i + 1) % 500 == 0:
                print(f"   Saved {i + 1}/{len(dataset)} tasks...")

    size_mb = output_file.stat().st_size / 1024 / 1024
    print(f"\nâœ… Complete!")
    print(f"   Tasks: {len(dataset)}")
    print(f"   File: {output_file}")
    print(f"   Size: {size_mb:.2f} MB")

    # Show example
    print(f"\nğŸ“‹ First task:")
    print(f"   Instance: {dataset[0]['instance_id']}")
    print(f"   Repo: {dataset[0]['repo']}")

if __name__ == "__main__":
    main()
